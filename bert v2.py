# -*- coding: utf-8 -*-
"""FEWSHOTmoderbert.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LRaONUDmacOYYuhp_KMkOSdlxqRCu6Wf
"""

pip install captum

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import requests
import json
import time
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_curve, auc, confusion_matrix, matthews_corrcoef,
    classification_report, precision_recall_curve
)
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectFromModel, RFECV, mutual_info_classif, SelectKBest
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
# Use AutoModel classes for ModernBERT
from transformers import (
    AutoTokenizer,
    AutoModel,
    AutoModelForSequenceClassification,
    get_linear_schedule_with_warmup,
    TrainingArguments,
    Trainer
)
from tqdm import tqdm
from captum.attr import IntegratedGradients
import warnings
warnings.filterwarnings('ignore')

# Set random seeds for reproducibility
seed = 42
np.random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

# Check for GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# APIVoid integration for few-shot learning
class APIVoidClient:
    def __init__(self, api_key):
        self.api_key = api_key
        self.base_url = "https://api.apivoid.com/v2/url-reputation"
        self.cache = {}  # Simple cache to avoid redundant API calls

    def get_url_reputation(self, url):
        """Get reputation data for a URL from APIVoid"""
        # Check cache first
        if url in self.cache:
            print(f"Using cached data for {url}")
            return self.cache[url]

        # Prepare the request
        headers = {
            "Content-Type": "application/json",
            "X-API-Key": self.api_key
        }
        data = {"url": url}

        try:
            response = requests.post(self.base_url, headers=headers, json=data)
            response.raise_for_status()  # Raise an exception for HTTP errors

            # Process response
            result = response.json()

            # Cache the result
            self.cache[url] = result

            # Wait to respect API rate limits
            time.sleep(0.5)

            return result

        except Exception as e:
            print(f"Error getting reputation for {url}: {e}")
            return None

    def extract_features(self, url):
        """Extract features from APIVoid response for a given URL"""
        reputation_data = self.get_url_reputation(url)

        if not reputation_data:
            # Return default features if API call failed
            return {
                "is_blacklisted": 0,
                "security_checks_failed": 0,
                "risk_score": 0,
                "external_redirect": 0,
                "suspicious_url_pattern": 0,
                "suspicious_tld": 0,
                "valid_ssl": 1,
                "domain_age_days": 365  # Default to 1 year
            }

        # Extract relevant features from APIVoid response
        features = {}

        # Domain blacklist information
        if "domain_blacklist" in reputation_data:
            features["is_blacklisted"] = int(reputation_data["domain_blacklist"]["detections"] > 0)
            features["blacklist_detections"] = reputation_data["domain_blacklist"]["detections"]
        else:
            features["is_blacklisted"] = 0
            features["blacklist_detections"] = 0

        # Risk score
        if "risk_score" in reputation_data and "result" in reputation_data["risk_score"]:
            features["risk_score"] = reputation_data["risk_score"]["result"]
        else:
            features["risk_score"] = 0

        # Security checks
        if "security_checks" in reputation_data:
            security_checks = reputation_data["security_checks"]

            # Count failed security checks
            failed_checks = sum(1 for key, value in security_checks.items()
                              if key.startswith("is_") and value == True)
            features["security_checks_failed"] = failed_checks

            # Specific security flags
            features["external_redirect"] = int(security_checks.get("is_external_redirect", False))
            features["suspicious_url_pattern"] = int(security_checks.get("is_suspicious_url_pattern", False))
            features["suspicious_tld"] = int(security_checks.get("is_most_abused_tld", False))
            features["valid_ssl"] = int(security_checks.get("is_valid_https", True))

            # Domain age
            if "is_domain_recent" in security_checks:
                if security_checks["is_domain_recent"] == "yes":
                    features["domain_age_days"] = 30  # ~1 month
                elif security_checks["is_domain_very_recent"] == "yes":
                    features["domain_age_days"] = 7   # ~1 week
                else:
                    features["domain_age_days"] = 365  # ~1 year
            else:
                features["domain_age_days"] = 365
        else:
            features["security_checks_failed"] = 0
            features["external_redirect"] = 0
            features["suspicious_url_pattern"] = 0
            features["suspicious_tld"] = 0
            features["valid_ssl"] = 1
            features["domain_age_days"] = 365

        return features

# 1. Data Loading and Preprocessing
print("Loading and preprocessing data...")

# Load the dataset
df = pd.read_csv('/content/drive/MyDrive/phishing/dataset_full.csv')

print(f"Dataset shape: {df.shape}")

# Check for missing values
print("\nChecking for missing values:")
print(df.isnull().sum().sum())

# Check class distribution
print("\nClass distribution:")
print(df['phishing'].value_counts(normalize=True))

# Extract features and target
features = df.drop(['phishing', 'url'] if 'url' in df.columns else ['phishing'], axis=1)
urls = df['url'].values if 'url' in df.columns else None
labels = df['phishing'].values

# If URL column doesn't exist, create a placeholder
if 'url' not in df.columns:
    print("Warning: URL column not found in the dataset. Adding a placeholder column.")
    # Generate placeholder URLs based on existing features
    urls = []
    for i, row in df.iterrows():
        if row['phishing'] == 1:  # Phishing
            urls.append(f"http://phishing-example-{i}.com/page")
        else:  # Legitimate
            urls.append(f"https://legitimate-example-{i}.com/page")
    df['url'] = urls
    urls = df['url'].values

# Feature selection will be abbreviated here to focus on ModernBERT integration
# Use a simple SelectFromModel for demonstration
print("\n2. Performing Feature Selection...")
rf_selector = SelectFromModel(
    RandomForestClassifier(n_estimators=100, random_state=seed),
    threshold='median'
)
rf_selector.fit(features, labels)
selected_features = features.columns[rf_selector.get_support()]
print(f"Selected {len(selected_features)} features")

# Use only the selected features
features = features[selected_features]
print(f"Reduced feature set shape: {features.shape}")

# Normalize the engineered features
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features).astype(np.float32)

# Split the data
X_train, X_test, urls_train, urls_test, y_train, y_test = train_test_split(
    features_scaled, urls, labels, test_size=0.2, random_state=seed, stratify=labels
)

print(f"Training set size: {len(X_train)}, Test set size: {len(X_test)}")

# 3. Initialize APIVoid client for few-shot learning
print("\n3. Initializing APIVoid client for few-shot learning...")
api_key = "eqJbPed.NbraD2fXYpn9oEuzPkRcG-45LeuigPB9.BOPVnF1uqyBHkZru3.0LzkG"
apivoid_client = APIVoidClient(api_key)

# Sample a small subset of URLs to get APIVoid features (to save API credits)
sample_size = min(20, len(urls_train))  # Adjust based on API limits
sampled_indices = np.random.choice(len(urls_train), size=sample_size, replace=False)

# 3.1 Get APIVoid features for the sampled URLs
print(f"\nGetting APIVoid features for {sample_size} sampled URLs...")
apivoid_features_train = []
for idx in tqdm(sampled_indices, desc="Processing APIVoid features"):
    url = urls_train[idx]
    apivoid_features = apivoid_client.extract_features(url)
    apivoid_features["label"] = y_train[idx]  # Add the label
    apivoid_features_train.append(apivoid_features)

# Create a DataFrame with APIVoid features
apivoid_df = pd.DataFrame(apivoid_features_train)
print(f"APIVoid features DataFrame shape: {apivoid_df.shape}")

# 4. ModernBERT Tokenization for URLs
print("\n4. Tokenizing URLs with ModernBERT...")

# Use the AutoTokenizer for ModernBERT
model_id = "answerdotai/ModernBERT-base"
tokenizer = AutoTokenizer.from_pretrained(model_id)

def preprocess_url(url):
    """Preprocess URL to make it more suitable for tokenization"""
    # Replace special characters with spaces around them to help tokenization
    for char in ['/', '.', '-', '=', '?', '&', '_', ':', '@']:
        url = url.replace(char, f' {char} ')
    # Additional preprocessing specific to URLs
    url = url.replace('http', 'http ')
    url = url.replace('https', 'https ')
    url = url.replace('www', 'www ')
    return url

# Create a function to prepare URL batches
def prepare_url_batch(urls, labels):
    processed_urls = [preprocess_url(url) for url in urls]
    encoded = tokenizer(
        processed_urls,
        padding='max_length',
        truncation=True,
        max_length=128,
        return_tensors='pt'
    )

    return {
        'input_ids': encoded['input_ids'],
        'attention_mask': encoded['attention_mask'],
        'labels': torch.tensor(labels, dtype=torch.long)
    }

# 5. Create a PyTorch Dataset for ModernBERT with engineered features
class PhishingDataset(Dataset):
    def __init__(self, urls, engineered_features, labels, tokenizer):
        self.urls = urls
        self.engineered_features = torch.tensor(engineered_features, dtype=torch.float32)
        self.labels = torch.tensor(labels, dtype=torch.long)
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        url = self.urls[idx]
        processed_url = preprocess_url(url)

        encoded = self.tokenizer.encode_plus(
            processed_url,
            add_special_tokens=True,
            max_length=128,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoded['input_ids'].squeeze(),
            'attention_mask': encoded['attention_mask'].squeeze(),
            'engineered_features': self.engineered_features[idx],
            'labels': self.labels[idx],
            'url': url
        }

# Create datasets
train_dataset = PhishingDataset(
    urls_train,
    X_train,
    y_train,
    tokenizer
)

test_dataset = PhishingDataset(
    urls_test,
    X_test,
    y_test,
    tokenizer
)

# Create data loaders
batch_size = 32  # Adjust based on memory constraints

train_dataloader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True
)

test_dataloader = DataLoader(
    test_dataset,
    batch_size=batch_size
)

print(f"Created dataloaders with batch size: {batch_size}")

# 6. Define the Enhanced Hybrid Model with ModernBERT and few-shot learning
class HybridModernBERTModel(nn.Module):
    def __init__(self, model_id='answerdotai/ModernBERT-base', num_engineered_features=None, num_labels=2):
        super(HybridModernBERTModel, self).__init__()

        # Initialize ModernBERT using AutoModel
        self.modernbert = AutoModel.from_pretrained(model_id)

        # Optional: Freeze some layers to reduce training time and prevent overfitting
        print("ModernBERT architecture:", dir(self.modernbert))

        # Dynamically determine the model structure and freeze appropriate layers
        try:
            if hasattr(self.modernbert, 'encoder') and hasattr(self.modernbert.encoder, 'layer'):
                modules = [self.modernbert.embeddings, *self.modernbert.encoder.layer[:6]]
            elif hasattr(self.modernbert, 'encoder'):
                modules = [self.modernbert.embeddings, self.modernbert.encoder]
            elif hasattr(self.modernbert, 'layer'):
                modules = [self.modernbert.embeddings, *self.modernbert.layer[:6]]
            else:
                # Simpler approach - freeze based on parameter names
                num_to_freeze = 6  # Number of layers to freeze
                modules = []
                print(f"Using parameter name-based freezing for first {num_to_freeze} layers")
                for name, param in self.modernbert.named_parameters():
                    for i in range(num_to_freeze):
                        if f"layer.{i}." in name or f"layers.{i}." in name:
                            param.requires_grad = False
                            break

            for module in modules:
                for param in module.parameters():
                    param.requires_grad = False

        except Exception as e:
            print(f"Error freezing layers: {e}")
            print("Skipping layer freezing - all layers will be fine-tuned")

        # Get actual feature dimension
        if num_engineered_features is None:
            num_engineered_features = X_train.shape[1]



        # Add extra dimension for APIVoid features
        num_apivoid_features = 5  # Number of APIVoid features we're using
        total_engineered_features = num_engineered_features + num_apivoid_features

        # Dropout layers with different rates
        self.dropout1 = nn.Dropout(0.3)
        self.dropout2 = nn.Dropout(0.5)  # Higher dropout for later layers

        # ModernBERT output size
        modernbert_hidden_size = self.modernbert.config.hidden_size  # Usually 768

        # Attention mechanism for engineered features
        self.feature_attention = nn.Sequential(
            nn.Linear(total_engineered_features, total_engineered_features),
            nn.Tanh(),
            nn.Linear(total_engineered_features, 1, bias=False),
            nn.Softmax(dim=1)
        )

        # Process engineered features separately before combining
        self.feature_encoder = nn.Sequential(
            nn.Linear(total_engineered_features, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.3)
        )

        # Process ModernBERT embeddings separately
        self.bert_encoder = nn.Sequential(
            nn.Linear(modernbert_hidden_size, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.3)
        )

        # Combined size after separate processing
        combined_size = 256 + 128  # ModernBERT encoder output + feature encoder output

        # Fully connected layers with residual connections
        self.fc1 = nn.Linear(combined_size, 256)
        self.bn1 = nn.BatchNorm1d(256)

        self.fc2 = nn.Linear(256, 128)
        self.bn2 = nn.BatchNorm1d(128)

        self.fc3 = nn.Linear(128, 64)
        self.bn3 = nn.BatchNorm1d(64)

        # Output layer
        self.classifier = nn.Linear(64, num_labels)  # 2 output classes

        # Activation functions
        self.relu = nn.ReLU()
        self.gelu = nn.GELU()  # Sometimes performs better than ReLU

        # Few-shot learning integration
        self.few_shot_head = nn.Linear(64, num_labels)

    def forward(self, input_ids, attention_mask, engineered_features=None, apivoid_features=None):
        # Get ModernBERT embeddings
        outputs = self.modernbert(
            input_ids=input_ids,
            attention_mask=attention_mask
        )

        # Extract the [CLS] token embedding which represents the entire sequence
        pooled_output = outputs.last_hidden_state[:, 0, :]

        # Apply dropout to ModernBERT output
        pooled_output = self.dropout1(pooled_output)

        # Process ModernBERT output
        bert_encoded = self.bert_encoder(pooled_output)

        # Combine engineered features with APIVoid features if available
        if engineered_features is not None:
            if apivoid_features is not None:
                combined_features = torch.cat((engineered_features, apivoid_features), dim=1)
            else:
                # If APIVoid features are not available, pad with zeros
                batch_size = engineered_features.size(0)
                apivoid_padding = torch.zeros(batch_size, 5, device=engineered_features.device)
                combined_features = torch.cat((engineered_features, apivoid_padding), dim=1)

            # Apply attention to engineered features
            feature_attention = self.feature_attention(combined_features)

            # Process engineered features
            feature_encoded = self.feature_encoder(combined_features)

            # Concatenate ModernBERT output with engineered features
            combined = torch.cat((bert_encoded, feature_encoded), dim=1)
        else:
            # If no engineered features, just use the ModernBERT output
            combined = bert_encoded

        # First fully connected block
        x = self.fc1(combined)
        x = self.bn1(x)
        x = self.gelu(x)
        x = self.dropout2(x)

        # Second fully connected block
        x = self.fc2(x)
        x = self.bn2(x)
        x = self.gelu(x)
        x = self.dropout2(x)

        # Third fully connected block
        x = self.fc3(x)
        x = self.bn3(x)
        x = self.gelu(x)
        pre_classification = self.dropout2(x)

        # Main classification head
        logits = self.classifier(pre_classification)

        # Few-shot classification head
        few_shot_logits = self.few_shot_head(pre_classification)

        return {'logits': logits, 'few_shot_logits': few_shot_logits, 'pre_classification': pre_classification}

# 7. Initialize the model
print("\n7. Initializing ModernBERT model...")
id2label = {0: "legitimate", 1: "phishing"}
label2id = {"legitimate": 0, "phishing": 1}

model = HybridModernBERTModel(
    model_id="answerdotai/ModernBERT-base",
    num_engineered_features=X_train.shape[1],
    num_labels=2
)
model = model.to(device)

print(f"Model initialized with {sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable parameters")

# 8. Training loop with few-shot learning
# Hyperparameters
epochs = 3
learning_rate = 1e-5
weight_decay = 0.01

# Check class imbalance for weighted loss
class_weights = None
class_counts = np.bincount(y_train)
if class_counts[0] / class_counts[1] > 1.5 or class_counts[1] / class_counts[0] > 1.5:
    print("\nDetected class imbalance. Using weighted loss function.")
    weight = torch.tensor([1.0, class_counts[0] / class_counts[1]], dtype=torch.float32).to(device)
    criterion = nn.CrossEntropyLoss(weight=weight)
else:
    criterion = nn.CrossEntropyLoss()

# Optimizer with weight decay
optimizer = torch.optim.AdamW(
    [
        {"params": model.modernbert.parameters(), "lr": learning_rate / 10},
        {"params": [p for n, p in model.named_parameters() if "modernbert" not in n], "lr": learning_rate}
    ],
    lr=learning_rate,
    weight_decay=weight_decay
)

# Learning rate scheduler
total_steps = len(train_dataloader) * epochs
warmup_steps = int(0.1 * total_steps)  # 10% of total steps for warmup
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=warmup_steps,
    num_training_steps=total_steps
)

# Custom training function with few-shot learning
def train_epoch(model, dataloader, optimizer, scheduler, criterion, device, apivoid_client=None):
    model.train()
    total_loss = 0
    correct_predictions = 0
    total_predictions = 0

    progress_bar = tqdm(dataloader, desc="Training")

    for batch in progress_bar:
        # Move batch to device
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        engineered_features = batch['engineered_features'].to(device)
        labels = batch['labels'].to(device)

        # Get APIVoid features for few-shot learning (optional)
        apivoid_features = None
        if apivoid_client is not None and 'url' in batch:
            # Sample a small percentage of URLs in each batch for APIVoid enrichment
            if np.random.random() < 0.1:  # 10% chance
                urls = batch['url']
                # Get features for just one URL to save API calls
                sample_idx = np.random.randint(0, len(urls))
                url = urls[sample_idx]
                apivoid_dict = apivoid_client.extract_features(url)

                # Convert to tensor
                apivoid_values = [
                    apivoid_dict.get('is_blacklisted', 0),
                    apivoid_dict.get('security_checks_failed', 0),
                    apivoid_dict.get('risk_score', 0),
                    apivoid_dict.get('suspicious_url_pattern', 0),
                    apivoid_dict.get('valid_ssl', 1)
                ]

                # Create a tensor with the same shape as the batch but filled with the same values
                apivoid_features = torch.tensor([apivoid_values] * len(urls), dtype=torch.float32).to(device)

        # Clear gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = model(input_ids, attention_mask, engineered_features, apivoid_features)
        logits = outputs['logits']
        few_shot_logits = outputs['few_shot_logits']

        # Calculate loss
        loss = criterion(logits, labels)
        if apivoid_features is not None:
            few_shot_loss = criterion(few_shot_logits, labels)
            loss = 0.8 * loss + 0.2 * few_shot_loss  # 80% main task, 20% few-shot task

        # Backward pass
        loss.backward()

        # Clip gradients
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        # Update weights
        optimizer.step()

        # Update learning rate
        scheduler.step()

        # Update metrics
        total_loss += loss.item()

        _, preds = torch.max(logits, dim=1)
        correct_predictions += (preds == labels).sum().item()
        total_predictions += labels.size(0)

        # Update progress bar
        progress_bar.set_postfix({
            'loss': f"{loss.item():.4f}",
            'acc': f"{correct_predictions/total_predictions:.4f}"
        })

    epoch_loss = total_loss / len(dataloader)
    epoch_acc = correct_predictions / total_predictions

    return epoch_loss, epoch_acc

# Evaluation function
def evaluate(model, dataloader, criterion, device):
    model.eval()
    total_loss = 0
    correct_predictions = 0
    total_predictions = 0

    all_preds = []
    all_labels = []
    all_probs = []

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            engineered_features = batch['engineered_features'].to(device)
            labels = batch['labels'].to(device)

            # Forward pass
            outputs = model(input_ids, attention_mask, engineered_features)
            logits = outputs['logits']

            # Calculate loss
            loss = criterion(logits, labels)

            # Get predictions and probabilities
            probs = F.softmax(logits, dim=1)
            _, preds = torch.max(logits, dim=1)

            # Update metrics
            total_loss += loss.item()
            correct_predictions += (preds == labels).sum().item()
            total_predictions += labels.size(0)

            # Store predictions and labels for metrics calculation
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs[:, 1].cpu().numpy())  # Probability of class 1 (phishing)

    epoch_loss = total_loss / len(dataloader)
    epoch_acc = correct_predictions / total_predictions

    return epoch_loss, epoch_acc, all_preds, all_labels, all_probs

# Training loop with early stopping
print("\n8. Training the model...")
patience = 3
best_val_loss = float('inf')
early_stopping_counter = 0
best_val_acc = 0

# For tracking metrics
train_losses = []
train_accs = []
val_losses = []
val_accs = []

# Training loop
for epoch in range(epochs):
    print(f"\nEpoch {epoch+1}/{epochs}")

    # Train with few-shot learning from APIVoid
    train_loss, train_acc = train_epoch(
        model, train_dataloader, optimizer, scheduler, criterion, device, apivoid_client
    )
    train_losses.append(train_loss)
    train_accs.append(train_acc)

    print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")

    # Evaluate
    val_loss, val_acc, val_preds, val_labels, val_probs = evaluate(
        model, test_dataloader, criterion, device
    )
    val_losses.append(val_loss)
    val_accs.append(val_acc)

    print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")

    # Save best model
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save(model.state_dict(), 'best_phishing_model_modernbert.pt')
        print(f"New best model saved with validation accuracy: {val_acc:.4f}")
        early_stopping_counter = 0
    else:
        early_stopping_counter += 1
        print(f"No improvement in validation accuracy for {early_stopping_counter} epochs")

        if early_stopping_counter >= patience:
            print(f"Early stopping triggered after {epoch+1} epochs")
            break

print("\nTraining complete!")

# 9. Final Evaluation and Metrics
print("\n9. Performing final evaluation...")

# Load the best model
model.load_state_dict(torch.load('best_phishing_model_modernbert.pt'))
model.eval()

# Get predictions on test set
_, _, y_pred, y_true, y_probs = evaluate(model, test_dataloader, criterion, device)

# Calculate metrics
acc = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)
mcc = matthews_corrcoef(y_true, y_pred)

print("\nTest Metrics:")
print(f"Accuracy: {acc:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"Matthews Correlation Coefficient: {mcc:.4f}")

# Print classification report
print("\nClassification Report:")
print(classification_report(y_true, y_pred, target_names=['Legitimate', 'Phishing']))

# 10. Visualizations and Deployment
# Confusion Matrix
plt.figure(figsize=(10, 8))
cm = confusion_matrix(y_true, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Legitimate', 'Phishing'],
            yticklabels=['Legitimate', 'Phishing'])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.savefig('confusion_matrix_modernbert.png')
plt.close()

# ROC Curve
plt.figure(figsize=(10, 8))
fpr, tpr, _ = roc_curve(y_true, y_probs)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.savefig('roc_curve_modernbert.png')
plt.close()

# 11. Create a production-ready prediction function
def predict_url_phishing(url, model, tokenizer, scaler, apivoid_client=None):
    """
    Predict whether a URL is phishing using the trained model

    Args:
        url (str): The URL to classify
        model: Trained ModernBERT model
        tokenizer: ModernBERT tokenizer
        scaler: Feature scaler for engineered features
        apivoid_client: Optional APIVoid client for additional features

    Returns:
        dict: Prediction results
    """
    model.eval()

    # 1. Process URL text for ModernBERT
    processed_url = preprocess_url(url)
    encoded = tokenizer.encode_plus(
        processed_url,
        add_special_tokens=True,
        max_length=128,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='pt'
    )

    input_ids = encoded['input_ids'].to(device)
    attention_mask = encoded['attention_mask'].to(device)

    # 2. Extract engineered features from URL
    # In a real-world scenario, you would extract these features from the URL
    # For demonstration, we'll use a placeholder with zeros
    # You should replace this with actual feature extraction code
    engineered_features = np.zeros((1, len(selected_features)), dtype=np.float32)

    # Scale the features
    engineered_features = scaler.transform(engineered_features)
    engineered_features_tensor = torch.tensor(engineered_features, dtype=torch.float32).to(device)

    # 3. Get APIVoid features if available
    apivoid_features_tensor = None
    if apivoid_client is not None:
        apivoid_dict = apivoid_client.extract_features(url)

        # Extract the features we want
        apivoid_values = [
            apivoid_dict.get('is_blacklisted', 0),
            apivoid_dict.get('security_checks_failed', 0),
            apivoid_dict.get('risk_score', 0),
            apivoid_dict.get('suspicious_url_pattern', 0),
            apivoid_dict.get('valid_ssl', 1)
        ]

        apivoid_features = np.array([apivoid_values], dtype=np.float32)
        apivoid_features_tensor = torch.tensor(apivoid_features, dtype=torch.float32).to(device)

    # 4. Make prediction
    with torch.no_grad():
        outputs = model(input_ids, attention_mask, engineered_features_tensor, apivoid_features_tensor)
        logits = outputs['logits']
        probs = F.softmax(logits, dim=1)
        _, pred = torch.max(logits, dim=1)

    # 5. Return results
    result = {
        'url': url,
        'is_phishing': bool(pred.item()),
        'phishing_probability': float(probs[0, 1].item()),
        'confidence': float(probs[0, pred.item()].item())
    }

    return result

# Example of using the prediction function
def demonstrate_prediction():
    # Create a simple example
    test_url = "http://suspicious-bank.secure-login.com/verify/account.php"

    # Load the model, tokenizer, and scaler (these should be loaded from saved files)
    # For demonstration we're using the variables from above

    # Make a prediction
    result = predict_url_phishing(test_url, model, tokenizer, scaler, apivoid_client)

    print(f"\nPrediction for URL: {test_url}")
    print(f"Is Phishing: {result['is_phishing']}")
    print(f"Phishing Probability: {result['phishing_probability']:.4f}")
    print(f"Confidence: {result['confidence']:.4f}")

    return result

# 12. Export model for production deployment
print("\n12. Preparing for production deployment...")

# Create a directory for model artifacts
import os
import pickle
import json

os.makedirs('modernbert_phishing_model', exist_ok=True)

# Save the tokenizer
tokenizer.save_pretrained('modernbert_phishing_model/tokenizer')

# Save the model configuration
model_config = {
    'model_id': 'answerdotai/ModernBERT-base',
    'num_labels': 2,
    'id2label': id2label,
    'label2id': label2id,
    'selected_features': list(selected_features),
    'performance': {
        'accuracy': acc,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'mcc': mcc
    }
}

with open('modernbert_phishing_model/model_config.json', 'w') as f:
    json.dump(model_config, f, indent=2)

# Save the scaler
with open('modernbert_phishing_model/scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)

# Save the model weights
torch.save(model.state_dict(), 'modernbert_phishing_model/model.pt')

# Create a sample script for loading and using the model
deployment_script = """
import torch
import torch.nn.functional as F
import numpy as np
import pickle
import json
from transformers import AutoTokenizer, AutoModel

# Load model configuration
with open('modernbert_phishing_model/model_config.json', 'r') as f:
    model_config = json.load(f)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained('modernbert_phishing_model/tokenizer')

# Load scaler
with open('modernbert_phishing_model/scaler.pkl', 'rb') as f:
    scaler = pickle.load(f)

# Define model class
class HybridModernBERTModel(torch.nn.Module):
    def __init__(self, model_id, num_engineered_features, num_labels):
        super(HybridModernBERTModel, self).__init__()

        # Initialize ModernBERT using AutoModel
        self.modernbert = AutoModel.from_pretrained(model_id)

        # Get ModernBERT output size
        modernbert_hidden_size = self.modernbert.config.hidden_size

        # Add extra dimension for APIVoid features
        num_apivoid_features = 5
        total_engineered_features = num_engineered_features + num_apivoid_features

        # Rest of the model architecture
        self.dropout1 = torch.nn.Dropout(0.3)
        self.dropout2 = torch.nn.Dropout(0.5)

        self.feature_attention = torch.nn.Sequential(
            torch.nn.Linear(total_engineered_features, total_engineered_features),
            torch.nn.Tanh(),
            torch.nn.Linear(total_engineered_features, 1, bias=False),
            torch.nn.Softmax(dim=1)
        )

        self.feature_encoder = torch.nn.Sequential(
            torch.nn.Linear(total_engineered_features, 128),
            torch.nn.BatchNorm1d(128),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3)
        )

        self.bert_encoder = torch.nn.Sequential(
            torch.nn.Linear(modernbert_hidden_size, 256),
            torch.nn.BatchNorm1d(256),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.3)
        )

        combined_size = 256 + 128

        self.fc1 = torch.nn.Linear(combined_size, 256)
        self.bn1 = torch.nn.BatchNorm1d(256)

        self.fc2 = torch.nn.Linear(256, 128)
        self.bn2 = torch.nn.BatchNorm1d(128)

        self.fc3 = torch.nn.Linear(128, 64)
        self.bn3 = torch.nn.BatchNorm1d(64)

        self.classifier = torch.nn.Linear(64, num_labels)
        self.few_shot_head = torch.nn.Linear(64, num_labels)

        self.gelu = torch.nn.GELU()

    def forward(self, input_ids, attention_mask, engineered_features=None, apivoid_features=None):
        # Get ModernBERT embeddings
        outputs = self.modernbert(
            input_ids=input_ids,
            attention_mask=attention_mask
        )

        # Extract the [CLS] token embedding
        pooled_output = outputs.last_hidden_state[:, 0, :]

        # Apply dropout to ModernBERT output
        pooled_output = self.dropout1(pooled_output)

        # Process ModernBERT output
        bert_encoded = self.bert_encoder(pooled_output)

        # Handle engineered features
        if engineered_features is not None:
            if apivoid_features is not None:
                combined_features = torch.cat((engineered_features, apivoid_features), dim=1)
            else:
                batch_size = engineered_features.size(0)
                apivoid_padding = torch.zeros(batch_size, 5, device=engineered_features.device)
                combined_features = torch.cat((engineered_features, apivoid_padding), dim=1)

            # Process engineered features
            feature_encoded = self.feature_encoder(combined_features)

            # Concatenate ModernBERT output with engineered features
            combined = torch.cat((bert_encoded, feature_encoded), dim=1)
        else:
            combined = bert_encoded

        # Fully connected layers
        x = self.fc1(combined)
        x = self.bn1(x)
        x = self.gelu(x)
        x = self.dropout2(x)

        x = self.fc2(x)
        x = self.bn2(x)
        x = self.gelu(x)
        x = self.dropout2(x)

        x = self.fc3(x)
        x = self.bn3(x)
        x = self.gelu(x)
        pre_classification = self.dropout2(x)

        # Main classification head
        logits = self.classifier(pre_classification)

        return {'logits': logits, 'pre_classification': pre_classification}

# Initialize model
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = HybridModernBERTModel(
    model_id=model_config['model_id'],
    num_engineered_features=len(model_config['selected_features']),
    num_labels=model_config['num_labels']
)

# Load weights
model.load_state_dict(torch.load('modernbert_phishing_model/model.pt', map_location=device))
model.to(device)
model.eval()

# Preprocessing function
def preprocess_url(url):
    # Replace special characters with spaces around them
    for char in ['/', '.', '-', '=', '?', '&', '_', ':', '@']:
        url = url.replace(char, f' {char} ')
    # Additional preprocessing specific to URLs
    url = url.replace('http', 'http ')
    url = url.replace('https', 'https ')
    url = url.replace('www', 'www ')
    return url

# Prediction function
def predict_phishing(url):
    # 1. Process URL text for ModernBERT
    processed_url = preprocess_url(url)
    encoded = tokenizer.encode_plus(
        processed_url,
        add_special_tokens=True,
        max_length=128,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='pt'
    )

    input_ids = encoded['input_ids'].to(device)
    attention_mask = encoded['attention_mask'].to(device)

    # 2. Extract engineered features from URL
    # In a real-world scenario, you would have code here to extract these features
    # For now we'll use placeholder zeros
    engineered_features = np.zeros((1, len(model_config['selected_features'])), dtype=np.float32)

    # Scale the features
    engineered_features = scaler.transform(engineered_features)
    engineered_features_tensor = torch.tensor(engineered_features, dtype=torch.float32).to(device)

    # 3. Make prediction
    with torch.no_grad():
        outputs = model(input_ids, attention_mask, engineered_features_tensor)
        logits = outputs['logits']
        probs = F.softmax(logits, dim=1)
        _, pred = torch.max(logits, dim=1)

    # 4. Return results
    result = {
        'url': url,
        'is_phishing': bool(pred.item()),
        'phishing_probability': float(probs[0, 1].item()),
        'confidence': float(probs[0, pred.item()].item()),
        'class': model_config['id2label'][str(pred.item())]
    }

    return result

# Example usage
if __name__ == '__main__':
    test_url = "http://example-bank.com/login/verify.php"
    result = predict_phishing(test_url)
    print(f"URL: {result['url']}")
    print(f"Prediction: {result['class']}")
    print(f"Phishing Probability: {result['phishing_probability']:.4f}")
    print(f"Confidence: {result['confidence']:.4f}")
"""

with open('modernbert_phishing_model/deploy.py', 'w') as f:
    f.write(deployment_script)

print("\nDeployment artifacts saved to 'modernbert_phishing_model' directory")

# 13. Run a demonstration prediction
print("\n13. Running demonstration prediction...")
demo_result = demonstrate_prediction()

# 14. Final summary
print("\n14. Project Summary")
print("=" * 60)
print("Enhanced Phishing Detection Model with ModernBERT and Few-Shot Learning")
print("=" * 60)
print(f"Dataset size: {df.shape[0]} samples")
print(f"Original features: {df.shape[1] - 2}")  # Subtract phishing and URL columns
print(f"Selected features: {len(selected_features)}")
print("\nModel Architecture:")
print("- ModernBERT base model with fine-tuning")
print("- Engineered features with attention mechanism")
print("- Few-shot learning with APIVoid integration")
print("\nPerformance:")
print(f"Accuracy: {acc:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"MCC: {mcc:.4f}")
print("\nModel Deployment:")
print("- Saved model and artifacts in 'modernbert_phishing_model' directory")
print("- Created deployment script 'deploy.py'")
print("- Ready for production use")
print("=" * 60)

print("\nPhishing detection model with ModernBERT successfully trained and deployed!")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import requests
import json
import time
import random
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_curve, auc, confusion_matrix, matthews_corrcoef,
    classification_report
)
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer,
    AutoModel,
    get_linear_schedule_with_warmup
)
from tqdm import tqdm
import urllib.parse
import warnings
warnings.filterwarnings('ignore')

# Set random seeds for reproducibility
seed = 42
np.random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

# Check for GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# OpenPhish integration for few-shot learning
class OpenPhishClient:
    def __init__(self, cache_size=100):
        self.feed_url = "https://raw.githubusercontent.com/openphish/public_feed/refs/heads/main/feed.txt"
        self.phishing_urls = []
        self.cache = {}  # Cache for features
        self.cache_size = cache_size
        self.load_urls()

    def load_urls(self):
        """Load phishing URLs from OpenPhish feed"""
        try:
            response = requests.get(self.feed_url)
            if response.status_code == 200:
                self.phishing_urls = response.text.strip().split('\n')
                print(f"Loaded {len(self.phishing_urls)} phishing URLs from OpenPhish")
            else:
                print(f"Failed to load OpenPhish feed: {response.status_code}")
        except Exception as e:
            print(f"Error loading OpenPhish feed: {e}")

    def get_random_phishing_url(self):
        """Get a random phishing URL from the loaded list"""
        if not self.phishing_urls:
            return None
        return random.choice(self.phishing_urls)

    def extract_features(self, url):
        """Extract features from a URL for few-shot learning"""
        # Check cache first
        if url in self.cache:
            return self.cache[url]

        # Parse the URL
        parsed_url = urllib.parse.urlparse(url)

        # Extract features
        features = {
            # Domain-based features
            "domain_length": len(parsed_url.netloc),
            "has_www": 1 if parsed_url.netloc.startswith('www.') else 0,
            "has_subdomain": 1 if parsed_url.netloc.count('.') > 1 else 0,
            "is_ip": 1 if all(c.isdigit() or c == '.' for c in parsed_url.netloc) else 0,

            # Path-based features
            "path_length": len(parsed_url.path),
            "path_depth": parsed_url.path.count('/'),
            "has_suspicious_path": 1 if any(word in parsed_url.path.lower() for word in ['login', 'signin', 'account', 'password', 'secure', 'update']) else 0,

            # Query-based features
            "has_query": 1 if len(parsed_url.query) > 0 else 0,
            "query_length": len(parsed_url.query),
            "query_param_count": parsed_url.query.count('&') + 1 if parsed_url.query else 0,

            # Security features
            "has_https": 1 if parsed_url.scheme == 'https' else 0,

            # Special characters
            "url_special_chars": sum(1 for c in url if not c.isalnum() and c not in '.-/:?=&'),
            "domain_special_chars": sum(1 for c in parsed_url.netloc if not c.isalnum() and c not in '.-')
        }

        # Cache the result if not full
        if len(self.cache) < self.cache_size:
            self.cache[url] = features

        return features

    def get_few_shot_batch(self, batch_size, include_legitimate=True):
        """Generate a batch of few-shot examples with features"""
        if not self.phishing_urls:
            return None

        # Select random phishing URLs
        num_phishing = batch_size if not include_legitimate else batch_size // 2
        num_legitimate = 0 if not include_legitimate else batch_size - num_phishing

        phishing_sample = random.sample(self.phishing_urls, min(num_phishing, len(self.phishing_urls)))

        # Generate some legitimate URLs (simplified examples)
        legitimate_domains = ['google.com', 'microsoft.com', 'amazon.com', 'facebook.com',
                              'apple.com', 'github.com', 'youtube.com', 'wikipedia.org',
                              'linkedin.com', 'twitter.com', 'instagram.com', 'reddit.com']

        legitimate_paths = ['', '/', '/index.html', '/about', '/contact', '/news',
                           '/products', '/services', '/blog', '/faq', '/help']

        legitimate_sample = []
        for _ in range(num_legitimate):
            domain = random.choice(legitimate_domains)
            path = random.choice(legitimate_paths)
            legitimate_sample.append(f"https://{domain}{path}")

        # Combine samples
        urls = phishing_sample + legitimate_sample
        labels = [1] * len(phishing_sample) + [0] * len(legitimate_sample)

        # Extract features
        features = [self.extract_features(url) for url in urls]

        return urls, features, labels

# 1. Data Loading and Preprocessing
print("Loading and preprocessing data...")

# Load the dataset
df = pd.read_csv('/content/drive/MyDrive/phishing/dataset_full.csv')

print(f"Dataset shape: {df.shape}")

# Check for missing values
print("\nChecking for missing values:")
print(df.isnull().sum().sum())

# Check class distribution
print("\nClass distribution:")
print(df['phishing'].value_counts(normalize=True))

# Extract features and target
features = df.drop(['phishing', 'url'] if 'url' in df.columns else ['phishing'], axis=1)
urls = df['url'].values if 'url' in df.columns else None
labels = df['phishing'].values

# If URL column doesn't exist, create a placeholder
if 'url' not in df.columns:
    print("Warning: URL column not found in the dataset. Adding a placeholder column.")
    # Generate placeholder URLs based on existing features
    urls = []
    for i, row in df.iterrows():
        if row['phishing'] == 1:  # Phishing
            urls.append(f"http://phishing-example-{i}.com/page")
        else:  # Legitimate
            urls.append(f"https://legitimate-example-{i}.com/page")
    df['url'] = urls
    urls = df['url'].values

# 2. Feature Selection
print("\n2. Performing Feature Selection...")
rf_selector = SelectFromModel(
    RandomForestClassifier(n_estimators=100, random_state=seed),
    threshold='median'
)
rf_selector.fit(features, labels)
selected_features = features.columns[rf_selector.get_support()]
print(f"Selected {len(selected_features)} features")

# Use only the selected features
features = features[selected_features]
print(f"Reduced feature set shape: {features.shape}")

# Normalize the engineered features
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features).astype(np.float32)

# Split the data
X_train, X_test, urls_train, urls_test, y_train, y_test = train_test_split(
    features_scaled, urls, labels, test_size=0.2, random_state=seed, stratify=labels
)

print(f"Training set size: {len(X_train)}, Test set size: {len(X_test)}")

# 3. Initialize OpenPhish client for few-shot learning
print("\n3. Initializing OpenPhish client for few-shot learning...")
openphish_client = OpenPhishClient()

# 3.1 Get a sample of phishing URLs to understand their structure
print("\n3.1 Examining sample phishing URLs from OpenPhish...")
sample_urls = openphish_client.phishing_urls[:5] if len(openphish_client.phishing_urls) >= 5 else openphish_client.phishing_urls
for i, url in enumerate(sample_urls):
    print(f"Sample {i+1}: {url}")

# 3.2 Get sample features for few-shot learning
print("\n3.2 Extracting features from sample phishing URLs...")
sample_features = [openphish_client.extract_features(url) for url in sample_urls]
# Display the first one
if sample_features:
    print("Example features extracted from phishing URL:")
    for key, value in sample_features[0].items():
        print(f"  {key}: {value}")

# 4. ModernBERT Tokenization for URLs
print("\n4. Tokenizing URLs with ModernBERT...")

# Use the AutoTokenizer for ModernBERT
model_id = "answerdotai/ModernBERT-base"
tokenizer = AutoTokenizer.from_pretrained(model_id)

def preprocess_url(url):
    """Preprocess URL to make it more suitable for tokenization"""
    # Replace special characters with spaces around them to help tokenization
    for char in ['/', '.', '-', '=', '?', '&', '_', ':', '@']:
        url = url.replace(char, f' {char} ')
    # Additional preprocessing specific to URLs
    url = url.replace('http', 'http ')
    url = url.replace('https', 'https ')
    url = url.replace('www', 'www ')
    return url

# 5. Create a PyTorch Dataset for ModernBERT with engineered features
class PhishingDataset(Dataset):
    def __init__(self, urls, engineered_features, labels, tokenizer):
        self.urls = urls
        self.engineered_features = torch.tensor(engineered_features, dtype=torch.float32)
        self.labels = torch.tensor(labels, dtype=torch.long)
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        url = self.urls[idx]
        processed_url = preprocess_url(url)

        encoded = self.tokenizer.encode_plus(
            processed_url,
            add_special_tokens=True,
            max_length=128,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoded['input_ids'].squeeze(),
            'attention_mask': encoded['attention_mask'].squeeze(),
            'engineered_features': self.engineered_features[idx],
            'labels': self.labels[idx],
            'url': url
        }

# Create datasets
train_dataset = PhishingDataset(
    urls_train,
    X_train,
    y_train,
    tokenizer
)

test_dataset = PhishingDataset(
    urls_test,
    X_test,
    y_test,
    tokenizer
)

# Create data loaders
batch_size = 32  # Adjust based on memory constraints

train_dataloader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True
)

test_dataloader = DataLoader(
    test_dataset,
    batch_size=batch_size
)

print(f"Created dataloaders with batch size: {batch_size}")

# 6. Define the Enhanced Hybrid Model with ModernBERT and few-shot learning
class HybridModernBERTModel(nn.Module):
    def __init__(self, model_id='answerdotai/ModernBERT-base', num_engineered_features=None, num_labels=2):
        super(HybridModernBERTModel, self).__init__()

        # Initialize ModernBERT using AutoModel
        self.modernbert = AutoModel.from_pretrained(model_id)

        # Optional: Freeze some layers to reduce training time and prevent overfitting
        print("ModernBERT architecture:", dir(self.modernbert))

        # Dynamically determine the model structure and freeze appropriate layers
        try:
            if hasattr(self.modernbert, 'encoder') and hasattr(self.modernbert.encoder, 'layer'):
                modules = [self.modernbert.embeddings, *self.modernbert.encoder.layer[:6]]
            elif hasattr(self.modernbert, 'encoder'):
                modules = [self.modernbert.embeddings, self.modernbert.encoder]
            elif hasattr(self.modernbert, 'layer'):
                modules = [self.modernbert.embeddings, *self.modernbert.layer[:6]]
            else:
                # Simpler approach - freeze based on parameter names
                num_to_freeze = 6  # Number of layers to freeze
                modules = []
                print(f"Using parameter name-based freezing for first {num_to_freeze} layers")
                for name, param in self.modernbert.named_parameters():
                    for i in range(num_to_freeze):
                        if f"layer.{i}." in name or f"layers.{i}." in name:
                            param.requires_grad = False
                            break

            for module in modules:
                for param in module.parameters():
                    param.requires_grad = False

        except Exception as e:
            print(f"Error freezing layers: {e}")
            print("Skipping layer freezing - all layers will be fine-tuned")

        # Get actual feature dimension
        if num_engineered_features is None:
            num_engineered_features = X_train.shape[1]

        # Add extra dimension for few-shot features (typically from OpenPhish URLs)
        num_fewshot_features = 13  # Number of features we extract from URLs
        total_engineered_features = num_engineered_features + num_fewshot_features

        # Dropout layers with different rates
        self.dropout1 = nn.Dropout(0.3)
        self.dropout2 = nn.Dropout(0.5)  # Higher dropout for later layers

        # ModernBERT output size
        modernbert_hidden_size = self.modernbert.config.hidden_size  # Usually 768

        # Attention mechanism for engineered features
        self.feature_attention = nn.Sequential(
            nn.Linear(total_engineered_features, total_engineered_features),
            nn.Tanh(),
            nn.Linear(total_engineered_features, 1, bias=False),
            nn.Softmax(dim=1)
        )

        # Process engineered features separately before combining
        self.feature_encoder = nn.Sequential(
            nn.Linear(total_engineered_features, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.3)
        )

        # Process ModernBERT embeddings separately
        self.bert_encoder = nn.Sequential(
            nn.Linear(modernbert_hidden_size, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.3)
        )

        # Combined size after separate processing
        combined_size = 256 + 128  # ModernBERT encoder output + feature encoder output

        # Fully connected layers with residual connections
        self.fc1 = nn.Linear(combined_size, 256)
        self.bn1 = nn.BatchNorm1d(256)

        self.fc2 = nn.Linear(256, 128)
        self.bn2 = nn.BatchNorm1d(128)

        self.fc3 = nn.Linear(128, 64)
        self.bn3 = nn.BatchNorm1d(64)

        # Output layer
        self.classifier = nn.Linear(64, num_labels)  # 2 output classes

        # Activation functions
        self.relu = nn.ReLU()
        self.gelu = nn.GELU()  # Sometimes performs better than ReLU

        # Few-shot learning integration
        self.few_shot_head = nn.Linear(64, num_labels)

    def forward(self, input_ids, attention_mask, engineered_features=None, fewshot_features=None):
        # Get ModernBERT embeddings
        outputs = self.modernbert(
            input_ids=input_ids,
            attention_mask=attention_mask
        )

        # Extract the [CLS] token embedding which represents the entire sequence
        pooled_output = outputs.last_hidden_state[:, 0, :]

        # Apply dropout to ModernBERT output
        pooled_output = self.dropout1(pooled_output)

        # Process ModernBERT output
        bert_encoded = self.bert_encoder(pooled_output)

        # Combine engineered features with few-shot features if available
        if engineered_features is not None:
            if fewshot_features is not None:
                combined_features = torch.cat((engineered_features, fewshot_features), dim=1)
            else:
                # If few-shot features are not available, pad with zeros
                batch_size = engineered_features.size(0)
                fewshot_padding = torch.zeros(batch_size, 13, device=engineered_features.device)
                combined_features = torch.cat((engineered_features, fewshot_padding), dim=1)

            # Apply attention to engineered features
            feature_attention = self.feature_attention(combined_features)

            # Process engineered features
            feature_encoded = self.feature_encoder(combined_features)

            # Concatenate ModernBERT output with engineered features
            combined = torch.cat((bert_encoded, feature_encoded), dim=1)
        else:
            # If no engineered features, just use the ModernBERT output
            combined = bert_encoded

        # First fully connected block
        x = self.fc1(combined)
        x = self.bn1(x)
        x = self.gelu(x)
        x = self.dropout2(x)

        # Second fully connected block
        x = self.fc2(x)
        x = self.bn2(x)
        x = self.gelu(x)
        x = self.dropout2(x)

        # Third fully connected block
        x = self.fc3(x)
        x = self.bn3(x)
        x = self.gelu(x)
        pre_classification = self.dropout2(x)

        # Main classification head
        logits = self.classifier(pre_classification)

        # Few-shot classification head
        few_shot_logits = self.few_shot_head(pre_classification)

        return {'logits': logits, 'few_shot_logits': few_shot_logits, 'pre_classification': pre_classification}

# 7. Initialize the model
print("\n7. Initializing ModernBERT model...")
id2label = {0: "legitimate", 1: "phishing"}
label2id = {"legitimate": 0, "phishing": 1}

model = HybridModernBERTModel(
    model_id="answerdotai/ModernBERT-base",
    num_engineered_features=X_train.shape[1],
    num_labels=2
)
model = model.to(device)

print(f"Model initialized with {sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable parameters")

# 8. Training loop with few-shot learning from OpenPhish
# Hyperparameters
epochs = 3
learning_rate = 1e-5
weight_decay = 0.01

# Check class imbalance for weighted loss
class_weights = None
class_counts = np.bincount(y_train)
if class_counts[0] / class_counts[1] > 1.5 or class_counts[1] / class_counts[0] > 1.5:
    print("\nDetected class imbalance. Using weighted loss function.")
    weight = torch.tensor([1.0, class_counts[0] / class_counts[1]], dtype=torch.float32).to(device)
    criterion = nn.CrossEntropyLoss(weight=weight)
else:
    criterion = nn.CrossEntropyLoss()

# Optimizer with weight decay
optimizer = torch.optim.AdamW(
    [
        {"params": model.modernbert.parameters(), "lr": learning_rate / 10},
        {"params": [p for n, p in model.named_parameters() if "modernbert" not in n], "lr": learning_rate}
    ],
    lr=learning_rate,
    weight_decay=weight_decay
)

# Learning rate scheduler
total_steps = len(train_dataloader) * epochs
warmup_steps = int(0.1 * total_steps)  # 10% of total steps for warmup
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=warmup_steps,
    num_training_steps=total_steps
)

# Helper function to convert OpenPhish URL features to tensor
def url_features_to_tensor(url_features_list, device):
    """Convert a list of URL feature dictionaries to a tensor"""
    # Create a list of lists containing feature values in a consistent order
    feature_names = list(url_features_list[0].keys())
    feature_values = []

    for features in url_features_list:
        values = [features[name] for name in feature_names]
        feature_values.append(values)

    # Convert to tensor
    tensor = torch.tensor(feature_values, dtype=torch.float32).to(device)
    return tensor

# Custom training function with few-shot learning from OpenPhish
def train_epoch(model, dataloader, optimizer, scheduler, criterion, device, openphish_client=None):
    model.train()
    total_loss = 0
    correct_predictions = 0
    total_predictions = 0

    progress_bar = tqdm(dataloader, desc="Training")

    for batch in progress_bar:
        # Move batch to device
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        engineered_features = batch['engineered_features'].to(device)
        labels = batch['labels'].to(device)

        # Get OpenPhish features for few-shot learning (optional)
        fewshot_features = None
        if openphish_client is not None:
            # Sample a small percentage of batches for OpenPhish enrichment
            if np.random.random() < 0.1:  # 10% chance
                # Get a few-shot batch from OpenPhish
                few_shot_size = min(8, batch_size)  # Use a smaller few-shot batch size
                fs_urls, fs_features, fs_labels = openphish_client.get_few_shot_batch(few_shot_size)

                if fs_urls and fs_features:
                    # Convert features to tensor
                    fewshot_features = url_features_to_tensor(fs_features, device)

                    # If the few-shot batch is smaller than the main batch, repeat to match size
                    if fewshot_features.shape[0] < input_ids.shape[0]:
                        repeats = (input_ids.shape[0] // fewshot_features.shape[0]) + 1
                        fewshot_features = fewshot_features.repeat(repeats, 1)[:input_ids.shape[0]]

        # Clear gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = model(input_ids, attention_mask, engineered_features, fewshot_features)
        logits = outputs['logits']
        few_shot_logits = outputs['few_shot_logits']

        # Calculate loss
        loss = criterion(logits, labels)
        if fewshot_features is not None:
            few_shot_loss = criterion(few_shot_logits, labels)
            loss = 0.8 * loss + 0.2 * few_shot_loss  # 80% main task, 20% few-shot task

        # Backward pass
        loss.backward()

        # Clip gradients
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        # Update weights
        optimizer.step()

        # Update learning rate
        scheduler.step()

        # Update metrics
        total_loss += loss.item()

        _, preds = torch.max(logits, dim=1)
        correct_predictions += (preds == labels).sum().item()
        total_predictions += labels.size(0)

        # Update progress bar
        progress_bar.set_postfix({
            'loss': f"{loss.item():.4f}",
            'acc': f"{correct_predictions/total_predictions:.4f}"
        })

    epoch_loss = total_loss / len(dataloader)
    epoch_acc = correct_predictions / total_predictions

    return epoch_loss, epoch_acc

# Evaluation function
def evaluate(model, dataloader, criterion, device):
    model.eval()
    total_loss = 0
    correct_predictions = 0
    total_predictions = 0

    all_preds = []
    all_labels = []
    all_probs = []

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            engineered_features = batch['engineered_features'].to(device)
            labels = batch['labels'].to(device)

            # Forward pass
            outputs = model(input_ids, attention_mask, engineered_features)
            logits = outputs['logits']

            # Calculate loss
            loss = criterion(logits, labels)

            # Get predictions and probabilities
            probs = F.softmax(logits, dim=1)
            _, preds = torch.max(logits, dim=1)

            # Update metrics
            total_loss += loss.item()
            correct_predictions += (preds == labels).sum().item()
            total_predictions += labels.size(0)

            # Store predictions and labels for metrics calculation
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs[:, 1].cpu().numpy())  # Probability of class 1 (phishing)

    epoch_loss = total_loss / len(dataloader)
    epoch_acc = correct_predictions / total_predictions

    return epoch_loss, epoch_acc, all_preds, all_labels, all_probs

# Training loop with early stopping
print("\n8. Training the model with OpenPhish few-shot learning...")
patience = 3
best_val_loss = float('inf')
early_stopping_counter = 0
best_val_acc = 0

# For tracking metrics
train_losses = []
train_accs = []
val_losses = []
val_accs = []

# Training loop
for epoch in range(epochs):
    print(f"\nEpoch {epoch+1}/{epochs}")

    # Train with few-shot learning from OpenPhish
    train_loss, train_acc = train_epoch(
        model, train_dataloader, optimizer, scheduler, criterion, device, openphish_client
    )
    train_losses.append(train_loss)
    train_accs.append(train_acc)

    print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")

    # Evaluate
    val_loss, val_acc, val_preds, val_labels, val_probs = evaluate(
        model, test_dataloader, criterion, device
    )
    val_losses.append(val_loss)
    val_accs.append(val_acc)

    print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")

    # Save best model
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save(model.state_dict(), 'best_phishing_model_modernbert.pt')
        print(f"New best model saved with validation accuracy: {val_acc:.4f}")
        early_stopping_counter = 0
    else:
        early_stopping_counter += 1
        print(f"No improvement in validation accuracy for {early_stopping_counter} epochs")

        if early_stopping_counter >= patience:
            print(f"Early stopping triggered after {epoch+1} epochs")
            break

print("\nTraining complete!")

# 9. Final Evaluation and Metrics
print("\n9. Performing final evaluation...")

# Load the best model
model.load_state_dict(torch.load('best_phishing_model_modernbert.pt'))
model.eval()

# Get predictions on test set
_, _, y_pred, y_true, y_probs = evaluate(model, test_dataloader, criterion, device)

# Calculate metrics
acc = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)
mcc = matthews_corrcoef(y_true, y_pred)

print("\nTest Metrics:")
print(f"Accuracy: {acc:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"Matthews Correlation Coefficient: {mcc:.4f}")

# Print classification report
print("\nClassification Report:")
print(classification_report(y_true, y_pred, target_names=['Legitimate', 'Phishing']))

# 10. Visualizations and Deployment
# Confusion Matrix
plt.figure(figsize=(10, 8))
cm = confusion_matrix(y_true, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Legitimate', 'Phishing'],
            yticklabels=['Legitimate', 'Phishing'])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.savefig('confusion_matrix_modernbert.png')
plt.close()

# ROC Curve
# ROC Curve
plt.figure(figsize=(10, 8))
fpr, tpr, _ = roc_curve(y_true, y_probs)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.savefig('roc_curve_modernbert.png')
plt.close()

# Load your best model that has already been trained for 2 epochs
print("Loading best model from previous training...")
model.load_state_dict(torch.load('best_phishing_model_modernbert.pt'))
model = model.to(device)

# Fix the forward method in-place to handle batch size mismatches
def fixed_forward(self, input_ids, attention_mask, engineered_features=None, fewshot_features=None):
    # Get ModernBERT embeddings
    outputs = self.modernbert(
        input_ids=input_ids,
        attention_mask=attention_mask
    )

    # Extract the [CLS] token embedding which represents the entire sequence
    pooled_output = outputs.last_hidden_state[:, 0, :]

    # Apply dropout to ModernBERT output
    pooled_output = self.dropout1(pooled_output)

    # Process ModernBERT output
    bert_encoded = self.bert_encoder(pooled_output)

    # Combine engineered features with few-shot features if available
    if engineered_features is not None:
        batch_size = engineered_features.size(0)

        if fewshot_features is not None:
            # Ensure the batch sizes match before concatenating
            if fewshot_features.size(0) != batch_size:
                if fewshot_features.size(0) > batch_size:
                    # Truncate if too large
                    fewshot_features = fewshot_features[:batch_size]
                else:
                    # Repeat if too small
                    repeats = int(np.ceil(batch_size / fewshot_features.size(0)))
                    fewshot_features = fewshot_features.repeat(repeats, 1)[:batch_size]

            combined_features = torch.cat((engineered_features, fewshot_features), dim=1)
        else:
            # If few-shot features are not available, pad with zeros
            fewshot_padding = torch.zeros(batch_size, 13, device=engineered_features.device)
            combined_features = torch.cat((engineered_features, fewshot_padding), dim=1)

        # Apply attention to engineered features
        feature_attention = self.feature_attention(combined_features)

        # Process engineered features
        feature_encoded = self.feature_encoder(combined_features)

        # Concatenate ModernBERT output with engineered features
        combined = torch.cat((bert_encoded, feature_encoded), dim=1)
    else:
        # If no engineered features, just use the ModernBERT output
        combined = bert_encoded

    # First fully connected block
    x = self.fc1(combined)
    x = self.bn1(x)
    x = self.gelu(x)
    x = self.dropout2(x)

    # Second fully connected block
    x = self.fc2(x)
    x = self.bn2(x)
    x = self.gelu(x)
    x = self.dropout2(x)

    # Third fully connected block
    x = self.fc3(x)
    x = self.bn3(x)
    x = self.gelu(x)
    pre_classification = self.dropout2(x)

    # Main classification head
    logits = self.classifier(pre_classification)

    # Few-shot classification head
    few_shot_logits = self.few_shot_head(pre_classification)

    return {'logits': logits, 'few_shot_logits': few_shot_logits, 'pre_classification': pre_classification}

# Replace the model's forward method with our fixed version
import types
model.forward = types.MethodType(fixed_forward, model)

# Fix the few-shot batch generation to always return the requested batch size
def fixed_get_few_shot_batch(self, batch_size, include_legitimate=True):
    """Generate a batch of few-shot examples with features"""
    if not self.phishing_urls:
        return None, None, None

    # Select random phishing URLs
    num_phishing = batch_size if not include_legitimate else batch_size // 2
    num_legitimate = 0 if not include_legitimate else batch_size - num_phishing

    # Ensure we don't try to sample more than we have
    num_phishing = min(num_phishing, len(self.phishing_urls))

    # Sample phishing URLs
    phishing_sample = random.sample(self.phishing_urls, num_phishing)

    # Generate some legitimate URLs (simplified examples)
    legitimate_domains = ['google.com', 'microsoft.com', 'amazon.com', 'facebook.com',
                         'apple.com', 'github.com', 'youtube.com', 'wikipedia.org',
                         'linkedin.com', 'twitter.com', 'instagram.com', 'reddit.com']

    legitimate_paths = ['', '/', '/index.html', '/about', '/contact', '/news',
                       '/products', '/services', '/blog', '/faq', '/help']

    legitimate_sample = []
    for _ in range(num_legitimate):
        domain = random.choice(legitimate_domains)
        path = random.choice(legitimate_paths)
        legitimate_sample.append(f"https://{domain}{path}")

    # Combine samples
    urls = phishing_sample + legitimate_sample
    labels = [1] * len(phishing_sample) + [0] * len(legitimate_sample)

    # Extract features
    features = [self.extract_features(url) for url in urls]

    # Ensure we are returning the exact batch size requested
    if len(urls) < batch_size:
        # If we don't have enough, duplicate some samples
        indices = list(range(len(urls)))
        while len(urls) < batch_size:
            idx = random.choice(indices)
            urls.append(urls[idx])
            features.append(features[idx])
            labels.append(labels[idx])

    return urls, features, labels

# Replace the client's method with our fixed version
openphish_client.get_few_shot_batch = types.MethodType(fixed_get_few_shot_batch, openphish_client)

# Define a fixed train_epoch function that properly handles batch size matching
def fixed_train_epoch(model, dataloader, optimizer, scheduler, criterion, device, openphish_client=None):
    model.train()
    total_loss = 0
    correct_predictions = 0
    total_predictions = 0

    progress_bar = tqdm(dataloader, desc="Training")

    for batch in progress_bar:
        # Move batch to device
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        engineered_features = batch['engineered_features'].to(device)
        labels = batch['labels'].to(device)

        # Get OpenPhish features for few-shot learning (optional)
        fewshot_features = None
        if openphish_client is not None:
            # Sample a small percentage of batches for OpenPhish enrichment
            if np.random.random() < 0.1:  # 10% chance
                # Get a few-shot batch from OpenPhish matching the current batch size
                batch_size = input_ids.size(0)
                fs_urls, fs_features, fs_labels = openphish_client.get_few_shot_batch(batch_size)

                if fs_urls and fs_features:
                    # Convert features to tensor
                    fewshot_features = url_features_to_tensor(fs_features, device)

                    # Double-check the batch size matches
                    if fewshot_features.size(0) != batch_size:
                        print(f"Warning: batch size mismatch - expected {batch_size}, got {fewshot_features.size(0)}")
                        # Fix it by truncating or repeating
                        if fewshot_features.size(0) > batch_size:
                            fewshot_features = fewshot_features[:batch_size]
                        else:
                            repeats = int(np.ceil(batch_size / fewshot_features.size(0)))
                            fewshot_features = fewshot_features.repeat(repeats, 1)[:batch_size]

        # Clear gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = model(input_ids, attention_mask, engineered_features, fewshot_features)
        logits = outputs['logits']
        few_shot_logits = outputs['few_shot_logits']

        # Calculate loss
        loss = criterion(logits, labels)
        if fewshot_features is not None:
            few_shot_loss = criterion(few_shot_logits, labels)
            loss = 0.8 * loss + 0.2 * few_shot_loss  # 80% main task, 20% few-shot task

        # Backward pass
        loss.backward()

        # Clip gradients
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        # Update weights
        optimizer.step()

        # Update learning rate
        scheduler.step()

        # Update metrics
        total_loss += loss.item()

        _, preds = torch.max(logits, dim=1)
        correct_predictions += (preds == labels).sum().item()
        total_predictions += labels.size(0)

        # Update progress bar
        progress_bar.set_postfix({
            'loss': f"{loss.item():.4f}",
            'acc': f"{correct_predictions/total_predictions:.4f}"
        })

    epoch_loss = total_loss / len(dataloader)
    epoch_acc = correct_predictions / total_predictions

    return epoch_loss, epoch_acc

# Continue training from epoch 3 (using the fixed functions)
print("\nContinuing training from epoch 3/3 with fixes applied...")

# Training loop continues with our fixed functions
for epoch in range(2, epochs):  # Starting from epoch 2 (3rd epoch)
    print(f"\nEpoch {epoch+1}/{epochs}")

    # Train with few-shot learning from OpenPhish
    train_loss, train_acc = fixed_train_epoch(
        model, train_dataloader, optimizer, scheduler, criterion, device, openphish_client
    )
    train_losses.append(train_loss)
    train_accs.append(train_acc)

    print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")

    # Evaluate
    val_loss, val_acc, val_preds, val_labels, val_probs = evaluate(
        model, test_dataloader, criterion, device
    )
    val_losses.append(val_loss)
    val_accs.append(val_acc)

    print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")

    # Save best model
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save(model.state_dict(), 'best_phishing_model_modernbert.pt')
        print(f"New best model saved with validation accuracy: {val_acc:.4f}")
        early_stopping_counter = 0
    else:
        early_stopping_counter += 1
        print(f"No improvement in validation accuracy for {early_stopping_counter} epochs")

        if early_stopping_counter >= patience:
            print(f"Early stopping triggered after {epoch+1} epochs")
            break

print("\nTraining complete!")

# Continue with the evaluation and other steps
print("\n9. Performing final evaluation...")

# Load the best model
model.load_state_dict(torch.load('best_phishing_model_modernbert.pt'))
model.eval()

# Get predictions on test set
_, _, y_pred, y_true, y_probs = evaluate(model, test_dataloader, criterion, device)

# Calculate metrics
acc = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)
mcc = matthews_corrcoef(y_true, y_pred)

print("\nTest Metrics:")
print(f"Accuracy: {acc:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"Matthews Correlation Coefficient: {mcc:.4f}")