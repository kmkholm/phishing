# -*- coding: utf-8 -*-
"""fewshot_advance_.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16-AjFt4GatNIR99NBZhKPDpkdLVkdpP7
"""

pip install mofapy2

pip install eli5

pip install lime

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, Subset
import numpy as np
from tqdm import tqdm
from transformers import BertTokenizer, BertModel, BertConfig, get_linear_schedule_with_warmup
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import random
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import pickle
import os
import json

# ================================================================================
# FEW-SHOT LEARNING EXTENSION FOR PHISHING DETECTION
# ================================================================================

class FewShotPhishingDataset(Dataset):
    """
    Dataset for few-shot learning with phishing URLs

    This dataset handles creating episodes for training and evaluation in the
    few-shot learning paradigm.
    """
    def __init__(self, input_ids, attention_masks, engineered_features, labels, urls=None):
        """
        Initialize the few-shot dataset

        Args:
            input_ids: Tensor of tokenized URL input IDs
            attention_masks: Tensor of attention masks
            engineered_features: Tensor of engineered features
            labels: Tensor of labels (0 for legitimate, 1 for phishing)
            urls: Optional list of raw URLs for reference/debugging
        """
        self.input_ids = input_ids
        self.attention_masks = attention_masks
        self.engineered_features = engineered_features
        self.labels = labels
        self.urls = urls

        # Get indices for each class
        self.phishing_indices = torch.where(labels == 1)[0]
        self.legitimate_indices = torch.where(labels == 0)[0]

        print(f"FewShotPhishingDataset initialized with {len(self.phishing_indices)} phishing and "
              f"{len(self.legitimate_indices)} legitimate examples")

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
            'input_ids': self.input_ids[idx],
            'attention_mask': self.attention_masks[idx],
            'engineered_features': self.engineered_features[idx],
            'labels': self.labels[idx],
            'url': self.urls[idx] if self.urls is not None else None
        }

    def create_episode(self, n_way=2, n_shot=5, n_query=15, device='cuda'):
        """
        Create an episode for few-shot learning

        Args:
            n_way: Number of classes (typically 2 for phishing detection)
            n_shot: Number of support examples per class
            n_query: Number of query examples per class
            device: Device to place tensors on

        Returns:
            Dictionary containing support and query sets
        """
        # For binary classification in phishing detection:
        # - n_way is fixed at 2 (phishing vs legitimate)
        # - We need to ensure balanced representation

        episode = {}

        # Sample support and query examples for legitimate class
        legitimate_indices = self.legitimate_indices[torch.randperm(len(self.legitimate_indices))]
        legitimate_support_indices = legitimate_indices[:n_shot]
        legitimate_query_indices = legitimate_indices[n_shot:n_shot+n_query]

        # Sample support and query examples for phishing class
        phishing_indices = self.phishing_indices[torch.randperm(len(self.phishing_indices))]
        phishing_support_indices = phishing_indices[:n_shot]
        phishing_query_indices = phishing_indices[n_shot:n_shot+n_query]

        # Combine support sets
        support_indices = torch.cat([legitimate_support_indices, phishing_support_indices])
        support_labels = torch.cat([
            torch.zeros(n_shot, dtype=torch.long),
            torch.ones(n_shot, dtype=torch.long)
        ])

        # Create support set
        episode['support_input_ids'] = self.input_ids[support_indices].to(device)
        episode['support_attention_mask'] = self.attention_masks[support_indices].to(device)
        episode['support_engineered_features'] = self.engineered_features[support_indices].to(device)
        episode['support_labels'] = support_labels.to(device)

        # Combine query sets
        query_indices = torch.cat([legitimate_query_indices, phishing_query_indices])
        query_labels = torch.cat([
            torch.zeros(n_query, dtype=torch.long),
            torch.ones(n_query, dtype=torch.long)
        ])

        # Create query set
        episode['query_input_ids'] = self.input_ids[query_indices].to(device)
        episode['query_attention_mask'] = self.attention_masks[query_indices].to(device)
        episode['query_engineered_features'] = self.engineered_features[query_indices].to(device)
        episode['query_labels'] = query_labels.to(device)

        # Save indices for reference
        episode['support_indices'] = support_indices
        episode['query_indices'] = query_indices

        return episode

    def create_campaign_episode(self, campaign_indices, n_shot=5, n_query=15, device='cuda'):
        """
        Create an episode for a specific phishing campaign

        Args:
            campaign_indices: Indices of examples from the target campaign
            n_shot: Number of support examples per class
            n_query: Number of query examples per class
            device: Device to place tensors on

        Returns:
            Dictionary containing support and query sets
        """
        episode = {}

        # For campaign-specific episode:
        # - Support set contains examples from the specific campaign (positive) and random legitimate (negative)
        # - Query set contains different examples from same distributions

        # Ensure we have enough examples
        assert len(campaign_indices) >= n_shot + n_query, \
            f"Not enough campaign examples ({len(campaign_indices)}) for {n_shot} shot and {n_query} query"

        # Shuffle campaign indices
        campaign_indices = campaign_indices[torch.randperm(len(campaign_indices))]

        # Split into support and query
        campaign_support_indices = campaign_indices[:n_shot]
        campaign_query_indices = campaign_indices[n_shot:n_shot+n_query]

        # Get legitimate examples (random)
        legitimate_indices = self.legitimate_indices[torch.randperm(len(self.legitimate_indices))]
        legitimate_support_indices = legitimate_indices[:n_shot]
        legitimate_query_indices = legitimate_indices[n_shot:n_shot+n_query]

        # Combine support sets
        support_indices = torch.cat([legitimate_support_indices, campaign_support_indices])
        support_labels = torch.cat([
            torch.zeros(n_shot, dtype=torch.long),
            torch.ones(n_shot, dtype=torch.long)
        ])

        # Create support set
        episode['support_input_ids'] = self.input_ids[support_indices].to(device)
        episode['support_attention_mask'] = self.attention_masks[support_indices].to(device)
        episode['support_engineered_features'] = self.engineered_features[support_indices].to(device)
        episode['support_labels'] = support_labels.to(device)

        # Combine query sets
        query_indices = torch.cat([legitimate_query_indices, campaign_query_indices])
        query_labels = torch.cat([
            torch.zeros(n_query, dtype=torch.long),
            torch.ones(n_query, dtype=torch.long)
        ])

        # Create query set
        episode['query_input_ids'] = self.input_ids[query_indices].to(device)
        episode['query_attention_mask'] = self.attention_masks[query_indices].to(device)
        episode['query_engineered_features'] = self.engineered_features[query_indices].to(device)
        episode['query_labels'] = query_labels.to(device)

        # Save indices for reference
        episode['support_indices'] = support_indices
        episode['query_indices'] = query_indices

        return episode

# Prototypical Network model for few-shot learning
import torch
import torch.nn as nn

class PrototypicalNetwork(nn.Module):
    def __init__(self, feature_extractor, embedding_dim=256):
        super(PrototypicalNetwork, self).__init__()

        # Store the feature extractor
        self.feature_extractor = feature_extractor

        # Get BERT's hidden size
        bert_dim = self.feature_extractor.bert.config.hidden_size

        # Find the last linear layer in feature_encoder
        feature_encoder_layers = list(self.feature_extractor.feature_encoder.children())
        feature_dim = None
        for layer in reversed(feature_encoder_layers):
            if isinstance(layer, nn.Linear):
                feature_dim = layer.out_features
                break
        if feature_dim is None:
            raise ValueError("No nn.Linear layer found in feature_encoder")

        # Compute the total feature output dimension
        feature_output_dim = bert_dim + feature_dim

        # Define the embedding projection
        self.embedding_projection = nn.Sequential(
            nn.Linear(feature_output_dim, 512),
            nn.LayerNorm(512),
            nn.ReLU(),
            nn.Linear(512, embedding_dim),
            nn.LayerNorm(embedding_dim)
        )

        # Initialize feature influence parameter
        self.feature_influence = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.distance_metric = self._euclidean_distance

    def _euclidean_distance(self, x, y):
        # Placeholder for distance metric implementation
        return torch.cdist(x, y)

    def _get_combined_embeddings(self, input_ids, attention_mask, engineered_features):
        """Get embeddings from the feature extractor"""
        # Forward pass through feature extractor
        _ = self.feature_extractor(input_ids, attention_mask, engineered_features)

        # Get intermediate outputs
        outputs = self.feature_extractor.get_intermediate_outputs()

        # Weight the importance of different feature types
        normalized_influence = F.softmax(self.feature_influence, dim=0)

        # Combine BERT and engineered feature representations with learned weights
        bert_output = outputs['bert_output']
        feature_output = outputs['feature_output']

        # Apply influence weighting
        weighted_combined = torch.cat([
            bert_output * normalized_influence[0],
            feature_output * normalized_influence[1]
        ], dim=1)

        # Get combined embedding through projection
        embeddings = self.embedding_projection(weighted_combined)

        return embeddings

    def _euclidean_distance(self, x, y):
        """Compute Euclidean distance between x and y"""
        n = x.size(0)
        m = y.size(0)
        d = x.size(1)

        assert d == y.size(1)

        x = x.unsqueeze(1).expand(n, m, d)
        y = y.unsqueeze(0).expand(n, m, d)

        return torch.sqrt(torch.pow(x - y, 2).sum(2) + 1e-6)

    def forward(self, support_input_ids, support_attention_mask, support_engineered_features,
                support_labels, query_input_ids, query_attention_mask, query_engineered_features,
                n_way=2, temperature=1.0):
        """
        Forward pass for prototypical networks

        Args:
            support_input_ids: Tensor of support set input IDs
            support_attention_mask: Tensor of support set attention masks
            support_engineered_features: Tensor of support set engineered features
            support_labels: Tensor of support set labels
            query_input_ids: Tensor of query set input IDs
            query_attention_mask: Tensor of query set attention masks
            query_engineered_features: Tensor of query set engineered features
            n_way: Number of classes
            temperature: Temperature parameter for softmax

        Returns:
            logits: Tensor of shape [n_query, n_way] containing classification logits
        """
        # Get embeddings for support examples
        support_embeddings = self._get_combined_embeddings(
            support_input_ids, support_attention_mask, support_engineered_features
        )

        # Get embeddings for query examples
        query_embeddings = self._get_combined_embeddings(
            query_input_ids, query_attention_mask, query_engineered_features
        )

        # Compute prototypes (class centroids)
        prototypes = torch.zeros(n_way, support_embeddings.size(1),
                                device=support_embeddings.device)

        for i in range(n_way):
            # Get indices of examples from class i
            mask = support_labels == i
            if mask.sum() > 0:  # Check if we have examples for this class
                # Compute prototype as mean of support examples for this class
                prototypes[i] = support_embeddings[mask].mean(0)

        # Compute distances from query examples to prototypes
        distances = self.distance_metric(query_embeddings, prototypes)

        # Convert distances to logits with temperature scaling
        logits = -distances / temperature

        return logits

    def compute_loss(self, logits, target_labels):
        """Compute cross-entropy loss for the prototypical network"""
        return F.cross_entropy(logits, target_labels)

# Campaign Detection and Clustering
class PhishingCampaignManager:
    """
    Manages the detection, storage, and tracking of phishing campaigns

    This class:
    1. Detects new phishing campaigns through clustering
    2. Stores examples from each campaign
    3. Evaluates prototypical network performance on each campaign
    """
    def __init__(self, embedding_model, eps=0.5, min_samples=5, distance_threshold=0.7):
        """
        Initialize the campaign manager

        Args:
            embedding_model: Model used to generate embeddings
            eps: Maximum distance between samples for DBSCAN clustering
            min_samples: Minimum samples in a cluster to form a core point
            distance_threshold: Threshold for considering campaigns as the same
        """
        self.embedding_model = embedding_model
        self.eps = eps
        self.min_samples = min_samples
        self.distance_threshold = distance_threshold

        # Storage for campaign information
        self.campaigns = {}
        self.campaign_counter = 0

        # For DBSCAN clustering
        from sklearn.cluster import DBSCAN
        self.clusterer = DBSCAN(eps=eps, min_samples=min_samples, metric='euclidean')

        # Performance tracking
        self.campaign_metrics = {}

    def detect_campaigns(self, input_ids, attention_mask, engineered_features, urls=None):
        """
        Detect phishing campaigns by clustering embeddings

        Args:
            input_ids: Tensor of input IDs
            attention_mask: Tensor of attention masks
            engineered_features: Tensor of engineered features
            urls: Optional list of URLs for reference

        Returns:
            campaign_labels: Array of campaign labels for each example
        """
        print("Detecting phishing campaigns...")

        # Generate embeddings
        embeddings = self._generate_embeddings(input_ids, attention_mask, engineered_features)

        # Perform clustering
        campaign_labels = self.clusterer.fit_predict(embeddings)

        # Count campaigns
        unique_campaigns = np.unique(campaign_labels)
        num_campaigns = len(unique_campaigns[unique_campaigns >= 0])

        print(f"Detected {num_campaigns} potential campaigns")
        print(f"Examples without a campaign: {(campaign_labels == -1).sum()}")

        # Visualize campaign distribution
        self._visualize_campaign_distribution(campaign_labels)

        return campaign_labels

    def register_campaigns(self, campaign_labels, input_ids, attention_mask,
                          engineered_features, labels, urls=None):
        """
        Register detected campaigns

        Args:
            campaign_labels: Array of campaign labels from detect_campaigns
            input_ids: Tensor of input IDs
            attention_mask: Tensor of attention masks
            engineered_features: Tensor of engineered features
            labels: Tensor of labels
            urls: Optional list of URLs

        Returns:
            campaign_ids: Dictionary mapping cluster labels to campaign IDs
        """
        # Generate embeddings for all examples
        embeddings = self._generate_embeddings(input_ids, attention_mask, engineered_features)

        # Map cluster labels to campaign IDs
        campaign_ids = {}

        # Only process valid clusters (label >= 0)
        for cluster_label in np.unique(campaign_labels):
            if cluster_label < 0:
                continue

            # Get examples in this cluster
            cluster_mask = campaign_labels == cluster_label
            cluster_indices = np.where(cluster_mask)[0]

            # Only register if this cluster contains phishing examples
            if labels[cluster_indices].sum() > 0:
                # Check if this cluster is similar to an existing campaign
                is_existing = False
                for existing_id, campaign_data in self.campaigns.items():
                    similarity = self._calculate_campaign_similarity(
                        embeddings[cluster_mask],
                        campaign_data['embeddings']
                    )

                    if similarity > self.distance_threshold:
                        # This is part of an existing campaign
                        campaign_ids[cluster_label] = existing_id

                        # Add new examples to the existing campaign
                        self._add_examples_to_campaign(
                            existing_id,
                            cluster_indices,
                            input_ids,
                            attention_mask,
                            engineered_features,
                            labels,
                            embeddings,
                            urls
                        )

                        is_existing = True
                        break

                if not is_existing:
                    # Create a new campaign
                    campaign_id = f"campaign_{self.campaign_counter}"
                    self.campaign_counter += 1

                    # Register the campaign
                    self.campaigns[campaign_id] = {
                        'embeddings': embeddings[cluster_indices],
                        'input_ids': input_ids[cluster_indices],
                        'attention_mask': attention_mask[cluster_indices],
                        'engineered_features': engineered_features[cluster_indices],
                        'labels': labels[cluster_indices],
                        'indices': cluster_indices,
                        'urls': [urls[i] for i in cluster_indices] if urls is not None else None,
                        'creation_time': pd.Timestamp.now(),
                        'cluster_label': cluster_label,
                        'size': len(cluster_indices)
                    }

                    campaign_ids[cluster_label] = campaign_id
                    print(f"Registered new campaign {campaign_id} with {len(cluster_indices)} examples")

        return campaign_ids

    def get_campaign_examples(self, campaign_id, n_examples=10):
        """Get examples from a specific campaign"""
        if campaign_id in self.campaigns:
            campaign = self.campaigns[campaign_id]
            indices = torch.randperm(len(campaign['labels']))[:n_examples]

            return {
                'input_ids': campaign['input_ids'][indices],
                'attention_mask': campaign['attention_mask'][indices],
                'engineered_features': campaign['engineered_features'][indices],
                'labels': campaign['labels'][indices],
                'urls': [campaign['urls'][i] for i in indices] if campaign['urls'] is not None else None
            }
        else:
            return None

    def evaluate_campaign_detection(self, proto_model, dataset, campaign_ids, n_shot=5, n_query=15, device='cuda'):
        """
        Evaluate how well the prototypical network detects specific campaigns

        Args:
            proto_model: Prototypical network model
            dataset: FewShotPhishingDataset
            campaign_ids: List of campaign IDs to evaluate
            n_shot: Number of support examples
            n_query: Number of query examples
            device: Device to run evaluation on

        Returns:
            metrics: Dictionary of metrics for each campaign
        """
        metrics = {}
        proto_model.eval()

        for campaign_id in campaign_ids:
            if campaign_id not in self.campaigns:
                continue

            # Get campaign data
            campaign = self.campaigns[campaign_id]
            campaign_indices = torch.tensor(campaign['indices'])

            # Create an episode for this campaign
            episode = dataset.create_campaign_episode(
                campaign_indices, n_shot=n_shot, n_query=n_query, device=device
            )

            # Evaluate on this episode
            with torch.no_grad():
                logits = proto_model(
                    episode['support_input_ids'],
                    episode['support_attention_mask'],
                    episode['support_engineered_features'],
                    episode['support_labels'],
                    episode['query_input_ids'],
                    episode['query_attention_mask'],
                    episode['query_engineered_features']
                )

                # Get predictions
                _, preds = torch.max(logits, dim=1)

                # Calculate metrics
                accuracy = accuracy_score(
                    episode['query_labels'].cpu().numpy(),
                    preds.cpu().numpy()
                )

                precision = precision_score(
                    episode['query_labels'].cpu().numpy(),
                    preds.cpu().numpy()
                )

                recall = recall_score(
                    episode['query_labels'].cpu().numpy(),
                    preds.cpu().numpy()
                )

                f1 = f1_score(
                    episode['query_labels'].cpu().numpy(),
                    preds.cpu().numpy()
                )

                # Store metrics
                self.campaign_metrics[campaign_id] = {
                    'accuracy': accuracy,
                    'precision': precision,
                    'recall': recall,
                    'f1': f1,
                    'n_shot': n_shot,
                    'n_query': n_query,
                    'evaluation_time': pd.Timestamp.now()
                }

                # Add to return metrics
                metrics[campaign_id] = self.campaign_metrics[campaign_id]

        return metrics

    def _generate_embeddings(self, input_ids, attention_mask, engineered_features):
        """Generate embeddings for examples using the embedding model"""
        self.embedding_model.eval()
        embeddings = []

        # Process in batches to avoid OOM
        batch_size = 32
        num_examples = input_ids.size(0)

        with torch.no_grad():
            for start_idx in range(0, num_examples, batch_size):
                end_idx = min(start_idx + batch_size, num_examples)

                batch_input_ids = input_ids[start_idx:end_idx].to(next(self.embedding_model.parameters()).device)
                batch_attention_mask = attention_mask[start_idx:end_idx].to(next(self.embedding_model.parameters()).device)
                batch_engineered_features = engineered_features[start_idx:end_idx].to(next(self.embedding_model.parameters()).device)

                # Generate embeddings
                batch_embeddings = self.embedding_model._get_combined_embeddings(
                    batch_input_ids, batch_attention_mask, batch_engineered_features
                )

                embeddings.append(batch_embeddings.cpu().numpy())

        return np.vstack(embeddings)

    def _calculate_campaign_similarity(self, new_embeddings, existing_embeddings):
        """Calculate similarity between a new cluster and existing campaign"""
        # Calculate average distance between centroids
        new_centroid = new_embeddings.mean(axis=0)
        existing_centroid = existing_embeddings.mean(axis=0)

        # Normalized cosine similarity
        similarity = np.dot(new_centroid, existing_centroid) / (
            np.linalg.norm(new_centroid) * np.linalg.norm(existing_centroid)
        )

        return similarity

    def _add_examples_to_campaign(self, campaign_id, indices, input_ids, attention_mask,
                                 engineered_features, labels, embeddings, urls):
        """Add new examples to an existing campaign"""
        campaign = self.campaigns[campaign_id]

        # Convert indices to numpy for safety
        indices = np.array(indices)

        # Add new examples
        campaign['embeddings'] = np.vstack([campaign['embeddings'], embeddings[indices]])
        campaign['input_ids'] = torch.cat([campaign['input_ids'], input_ids[indices]])
        campaign['attention_mask'] = torch.cat([campaign['attention_mask'], attention_mask[indices]])
        campaign['engineered_features'] = torch.cat([campaign['engineered_features'], engineered_features[indices]])
        campaign['labels'] = torch.cat([campaign['labels'], labels[indices]])

        # Add indices
        campaign['indices'] = np.concatenate([campaign['indices'], indices])

        # Add URLs if available
        if urls is not None and campaign['urls'] is not None:
            campaign['urls'].extend([urls[i] for i in indices])

        # Update size
        campaign['size'] = len(campaign['indices'])

        print(f"Added {len(indices)} examples to campaign {campaign_id}, new size: {campaign['size']}")

    def _visualize_campaign_distribution(self, campaign_labels):
        """Visualize distribution of examples across campaigns"""
        # Count examples in each campaign
        counter = {}
        for label in campaign_labels:
            if label >= 0:  # Skip noise points
                if label not in counter:
                    counter[label] = 0
                counter[label] += 1

        # Convert to list of (campaign, count) tuples
        campaign_counts = [(f"Campaign {k}", v) for k, v in counter.items()]
        campaign_counts.sort(key=lambda x: x[1], reverse=True)

        # Create dataframe for plotting
        df = pd.DataFrame(campaign_counts, columns=['Campaign', 'Count'])

        # Plot
        plt.figure(figsize=(12, 6))
        sns.barplot(x='Campaign', y='Count', data=df.head(20))  # Show top 20 campaigns
        plt.title(f'Examples per Campaign (showing top 20 of {len(counter)} campaigns)')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig('campaign_distribution.png')
        plt.close()

# ================================================================================
# TRAINING AND EVALUATION FUNCTIONS
# ================================================================================

def train_prototypical_network(proto_model, dataset, optimizer, scheduler=None,
                              n_epochs=3, n_episodes=100, n_way=2, n_shot=5, n_query=15,
                              device='cuda', eval_interval=10):
    """
    Train a prototypical network with episodic learning

    Args:
        proto_model: Prototypical network model
        dataset: FewShotPhishingDataset
        optimizer: Optimizer
        scheduler: Learning rate scheduler (optional)
        n_epochs: Number of epochs
        n_episodes: Number of episodes per epoch
        n_way: Number of classes per episode
        n_shot: Number of support examples per class
        n_query: Number of query examples per class
        device: Device to train on
        eval_interval: Number of episodes between evaluations

    Returns:
        metrics: Dictionary of training metrics
    """
    # Initialize metrics tracking
    metrics = {
        'train_loss': [],
        'train_accuracy': [],
        'eval_accuracy': [],
        'eval_precision': [],
        'eval_recall': [],
        'eval_f1': []
    }

    # Training loop
    for epoch in range(n_epochs):
        proto_model.train()
        epoch_loss = 0
        epoch_acc = 0

        # Create episodes for training
        for episode_idx in tqdm(range(n_episodes), desc=f"Epoch {epoch+1}/{n_epochs}"):
            # Create episode
            episode = dataset.create_episode(n_way=n_way, n_shot=n_shot, n_query=n_query, device=device)

            # Zero gradients
            optimizer.zero_grad()

            # Forward pass
            logits = proto_model(
                episode['support_input_ids'],
                episode['support_attention_mask'],
                episode['support_engineered_features'],
                episode['support_labels'],
                episode['query_input_ids'],
                episode['query_attention_mask'],
                episode['query_engineered_features']
            )

            # Compute loss
            loss = proto_model.compute_loss(logits, episode['query_labels'])

            # Backward pass
            loss.backward()

            # Update weights
            optimizer.step()

            # Update scheduler if provided
            if scheduler is not None:
                scheduler.step()

            # Calculate accuracy
            _, preds = torch.max(logits, dim=1)
            acc = (preds == episode['query_labels']).float().mean().item()

            # Update metrics
            epoch_loss += loss.item()
            epoch_acc += acc

            # Evaluate every eval_interval episodes
            if (episode_idx + 1) % eval_interval == 0:
                eval_metrics = evaluate_prototypical_network(
                    proto_model, dataset, n_episodes=5, n_way=n_way,
                    n_shot=n_shot, n_query=n_query, device=device
                )

                # Store evaluation metrics
                metrics['eval_accuracy'].append(eval_metrics['accuracy'])
                metrics['eval_precision'].append(eval_metrics['precision'])
                metrics['eval_recall'].append(eval_metrics['recall'])
                metrics['eval_f1'].append(eval_metrics['f1'])

                # Log progress
                print(f"Episode {episode_idx+1}/{n_episodes}, "
                      f"Loss: {loss.item():.4f}, Acc: {acc:.4f}, "
                      f"Eval Acc: {eval_metrics['accuracy']:.4f}, "
                      f"Eval F1: {eval_metrics['f1']:.4f}")

        # Calculate epoch metrics
        epoch_loss /= n_episodes
        epoch_acc /= n_episodes

        # Store epoch metrics
        metrics['train_loss'].append(epoch_loss)
        metrics['train_accuracy'].append(epoch_acc)

        # Log epoch summary
        print(f"Epoch {epoch+1}/{n_epochs}, Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}")

    return metrics

def evaluate_prototypical_network(proto_model, dataset, n_episodes=100, n_way=2,
                                n_shot=5, n_query=15, device='cuda'):
    """
    Evaluate a prototypical network

    Args:
        proto_model: Prototypical network model
        dataset: FewShotPhishingDataset
        n_episodes: Number of episodes to evaluate on
        n_way: Number of classes per episode
        n_shot: Number of support examples per class
        n_query: Number of query examples per class
        device: Device to evaluate on

    Returns:
        metrics: Dictionary of evaluation metrics
    """
    proto_model.eval()

    # Initialize metrics
    all_preds = []
    all_labels = []

    # Evaluation loop
    with torch.no_grad():
        for _ in tqdm(range(n_episodes), desc="Evaluating"):
            # Create episode
            episode = dataset.create_episode(n_way=n_way, n_shot=n_shot, n_query=n_query, device=device)

            # Forward pass
            logits = proto_model(
                episode['support_input_ids'],
                episode['support_attention_mask'],
                episode['support_engineered_features'],
                episode['support_labels'],
                episode['query_input_ids'],
                episode['query_attention_mask'],
                episode['query_engineered_features']
            )

            # Get predictions
            _, preds = torch.max(logits, dim=1)

            # Store predictions and labels
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(episode['query_labels'].cpu().numpy())

    # Calculate metrics
    metrics = {
        'accuracy': accuracy_score(all_labels, all_preds),
        'precision': precision_score(all_labels, all_preds, zero_division=0),
        'recall': recall_score(all_labels, all_preds, zero_division=0),
        'f1': f1_score(all_labels, all_preds, zero_division=0),
        'confusion_matrix': confusion_matrix(all_labels, all_preds)
    }

    return metrics

# Function to visualize training progress
def visualize_training_progress(metrics):
    """
    Visualize training progress and metrics

    Args:
        metrics: Dictionary of training and evaluation metrics
    """
    # Create figure with multiple subplots
    plt.figure(figsize=(20, 12))

    # Plot training loss
    plt.subplot(2, 2, 1)
    plt.plot(metrics['train_loss'])
    plt.title('Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')

    # Plot training accuracy
    plt.subplot(2, 2, 2)
    plt.plot(metrics['train_accuracy'], label='Training')
    plt.plot(range(0, len(metrics['eval_accuracy']) * (len(metrics['train_accuracy']) // len(metrics['eval_accuracy'])),
                  len(metrics['train_accuracy']) // len(metrics['eval_accuracy'])),
             metrics['eval_accuracy'], label='Evaluation')
    plt.title('Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Plot precision and recall
    plt.subplot(2, 2, 3)
    plt.plot(metrics['eval_precision'], label='Precision')
    plt.plot(metrics['eval_recall'], label='Recall')
    plt.title('Precision and Recall')
    plt.xlabel('Evaluation Interval')
    plt.ylabel('Score')
    plt.legend()

    # Plot F1 score
    plt.subplot(2, 2, 4)
    plt.plot(metrics['eval_f1'])
    plt.title('F1 Score')
    plt.xlabel('Evaluation Interval')
    plt.ylabel('F1 Score')

    plt.tight_layout()
    plt.savefig('few_shot_training_progress.png')
    plt.close()

# Function to visualize model performance across different shot values
def visualize_shot_performance(proto_model, dataset, shot_values=[1, 5, 10, 15, 20], device='cuda'):
    """
    Visualize model performance across different numbers of support examples (shots)

    Args:
        proto_model: Prototypical network model
        dataset: FewShotPhishingDataset
        shot_values: List of shot values to evaluate
        device: Device to evaluate on
    """
    # Initialize metrics
    shot_metrics = {
        'shots': shot_values,
        'accuracy': [],
        'precision': [],
        'recall': [],
        'f1': []
    }

    # Evaluate for each shot value
    for n_shot in shot_values:
        print(f"Evaluating with {n_shot} shots...")
        metrics = evaluate_prototypical_network(
            proto_model, dataset, n_episodes=20,
            n_shot=n_shot, n_query=15, device=device
        )

        # Store metrics
        shot_metrics['accuracy'].append(metrics['accuracy'])
        shot_metrics['precision'].append(metrics['precision'])
        shot_metrics['recall'].append(metrics['recall'])
        shot_metrics['f1'].append(metrics['f1'])

    # Plot metrics
    plt.figure(figsize=(12, 8))

    plt.plot(shot_metrics['shots'], shot_metrics['accuracy'], 'o-', label='Accuracy')
    plt.plot(shot_metrics['shots'], shot_metrics['precision'], 'o-', label='Precision')
    plt.plot(shot_metrics['shots'], shot_metrics['recall'], 'o-', label='Recall')
    plt.plot(shot_metrics['shots'], shot_metrics['f1'], 'o-', label='F1 Score')

    plt.title('Model Performance vs. Number of Support Examples (Shots)')
    plt.xlabel('Number of Shots')
    plt.ylabel('Score')
    plt.xticks(shot_metrics['shots'])
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.7)

    plt.tight_layout()
    plt.savefig('shot_performance.png')
    plt.close()

    return shot_metrics

# ================================================================================
# TESTING AND INTEGRATION WITH HYBRID BERT MODEL
# ================================================================================

def test_few_shot_on_new_campaign(proto_model, campaign_manager, base_model, tokenizer,
                                 scaler, test_urls, device='cuda'):
    """
    Test few-shot learning on a new phishing campaign

    Args:
        proto_model: Trained prototypical network
        campaign_manager: Phishing campaign manager
        base_model: Original hybrid BERT model
        tokenizer: BERT tokenizer
        scaler: Feature scaler
        test_urls: List of URLs to test (representing a new campaign)
        device: Device to run inference on

    Returns:
        results: Dictionary of prediction results
    """
    from sklearn.preprocessing import StandardScaler

    proto_model.eval()
    base_model.eval()

    # Preprocess test URLs
    print("Preprocessing test URLs...")
    processed_urls = [preprocess_url(url) for url in test_urls]

    # Tokenize URLs
    encoded = [tokenizer.encode_plus(
        url,
        add_special_tokens=True,
        max_length=128,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='pt'
    ) for url in processed_urls]

    input_ids = torch.cat([e['input_ids'] for e in encoded], dim=0).to(device)
    attention_mask = torch.cat([e['attention_mask'] for e in encoded], dim=0).to(device)

    # Extract engineered features
    # In a real system, you would extract the features using your feature extraction pipeline
    # Here we'll simulate this with random features that are properly scaled

    # Get feature dimension from model
    feature_dim = proto_model.feature_extractor.feature_encoder[0].in_features

    # Simulate engineered features (replace with actual feature extraction)
    test_features = torch.randn(len(test_urls), feature_dim)
    test_features = torch.tensor(scaler.transform(test_features.numpy()), dtype=torch.float32).to(device)

    # Get predictions from base model
    base_results = []
    with torch.no_grad():
        for i in range(0, len(test_urls), 8):  # Process in batches of 8
            batch_ids = input_ids[i:i+8]
            batch_mask = attention_mask[i:i+8]
            batch_features = test_features[i:i+8]

            outputs = base_model(batch_ids, batch_mask, batch_features)
            probs = F.softmax(outputs, dim=1)
            _, preds = torch.max(outputs, dim=1)

            for j in range(len(batch_ids)):
                idx = i + j
                if idx < len(test_urls):
                    base_results.append({
                        'url': test_urls[idx],
                        'prediction': preds[j].item(),
                        'confidence': probs[j, preds[j]].item()
                    })

    # Get few-shot predictions
    fs_results = []

    # Test against each campaign
    for campaign_id, campaign_data in campaign_manager.campaigns.items():
        print(f"Testing against campaign {campaign_id}...")

        # Get support examples
        support_ids = campaign_data['input_ids'][:5].to(device)
        support_mask = campaign_data['attention_mask'][:5].to(device)
        support_features = campaign_data['engineered_features'][:5].to(device)
        support_labels = torch.ones(5, dtype=torch.long).to(device)  # All are phishing

        # Add negative examples (legitimate)
        legitimate_indices = torch.where(dataset.labels == 0)[0][:5]
        support_ids = torch.cat([support_ids, dataset.input_ids[legitimate_indices].to(device)])
        support_mask = torch.cat([support_mask, dataset.attention_masks[legitimate_indices].to(device)])
        support_features = torch.cat([support_features, dataset.engineered_features[legitimate_indices].to(device)])
        support_labels = torch.cat([support_labels, torch.zeros(5, dtype=torch.long).to(device)])

        # Get few-shot predictions
        with torch.no_grad():
            for i in range(0, len(test_urls), 8):  # Process in batches of 8
                batch_ids = input_ids[i:i+8]
                batch_mask = attention_mask[i:i+8]
                batch_features = test_features[i:i+8]

                if len(batch_ids) == 0:
                    continue

                logits = proto_model(
                    support_ids,
                    support_mask,
                    support_features,
                    support_labels,
                    batch_ids,
                    batch_mask,
                    batch_features
                )

                probs = F.softmax(logits, dim=1)
                _, preds = torch.max(logits, dim=1)

                for j in range(len(batch_ids)):
                    idx = i + j
                    if idx < len(test_urls) and idx < len(fs_results) + 8:
                        if idx >= len(fs_results):
                            fs_results.append({
                                'url': test_urls[idx],
                                'campaigns': {}
                            })

                        fs_results[idx]['campaigns'][campaign_id] = {
                            'prediction': preds[j].item(),
                            'confidence': probs[j, preds[j]].item()
                        }

    # Combine results and make final prediction
    for i, fs_result in enumerate(fs_results):
        campaign_preds = [details['prediction'] for campaign_id, details in fs_result['campaigns'].items()]
        campaign_confs = [details['confidence'] for campaign_id, details in fs_result['campaigns'].items()]

        # If any campaign identifies as phishing with high confidence
        if 1 in campaign_preds and max([campaign_confs[j] for j, pred in enumerate(campaign_preds) if pred == 1]) > 0.8:
            fs_final_pred = 1
            fs_final_conf = max([campaign_confs[j] for j, pred in enumerate(campaign_preds) if pred == 1])
        else:
            # Use the prediction with highest confidence
            max_conf_idx = campaign_confs.index(max(campaign_confs))
            fs_final_pred = campaign_preds[max_conf_idx]
            fs_final_conf = campaign_confs[max_conf_idx]

        fs_results[i]['final_prediction'] = fs_final_pred
        fs_results[i]['final_confidence'] = fs_final_conf

    # Combine both models for final result
    final_results = []
    for base_result, fs_result in zip(base_results, fs_results):
        url = base_result['url']

        # If both models agree
        if base_result['prediction'] == fs_result['final_prediction']:
            final_pred = base_result['prediction']
            # Take the higher confidence
            final_conf = max(base_result['confidence'], fs_result['final_confidence'])
            ensemble_method = 'agreement'
        else:
            # If base model is highly confident, trust it
            if base_result['confidence'] > 0.9:
                final_pred = base_result['prediction']
                final_conf = base_result['confidence']
                ensemble_method = 'base_high_conf'
            # If few-shot model identifies as phishing with good confidence, trust it
            elif fs_result['final_prediction'] == 1 and fs_result['final_confidence'] > 0.7:
                final_pred = 1
                final_conf = fs_result['final_confidence']
                ensemble_method = 'fs_phish_conf'
            # Otherwise use weighted ensemble
            else:
                base_weight = 0.7
                fs_weight = 0.3

                # Calculate weighted probabilities
                p_phishing = base_weight * (base_result['prediction'] * base_result['confidence']) + \
                             fs_weight * (fs_result['final_prediction'] * fs_result['final_confidence'])

                final_pred = 1 if p_phishing > 0.5 else 0
                final_conf = abs(p_phishing - 0.5) * 2  # Scale to [0, 1]
                ensemble_method = 'weighted'

        final_results.append({
            'url': url,
            'base_prediction': base_result['prediction'],
            'base_confidence': base_result['confidence'],
            'fs_prediction': fs_result['final_prediction'],
            'fs_confidence': fs_result['final_confidence'],
            'final_prediction': final_pred,
            'final_confidence': final_conf,
            'ensemble_method': ensemble_method
        })

    return final_results

# Helper function for URL preprocessing
def preprocess_url(url):
    """Preprocess URL to make it more suitable for BERT tokenization"""
    # Replace special characters with spaces around them to help tokenization
    for char in ['/', '.', '-', '=', '?', '&', '_', ':', '@']:
        url = url.replace(char, f' {char} ')
    # Additional preprocessing specific to URLs
    url = url.replace('http', 'http ')
    url = url.replace('https', 'https ')
    url = url.replace('www', 'www ')
    return url

# ================================================================================
# MAIN INTEGRATION WORKFLOW
# ================================================================================

def integrate_few_shot_learning(hybrid_bert_model, tokenizer, scaler, train_dataset, test_dataset,
                               embedding_dim=256, device='cuda'):
    """
    Integrate few-shot learning with the existing hybrid BERT model

    Args:
        hybrid_bert_model: Trained HybridBERTModel
        tokenizer: BERT tokenizer
        scaler: Feature scaler
        train_dataset: Training dataset
        test_dataset: Test dataset
        embedding_dim: Dimension of the embedding space
        device: Device to run on

    Returns:
        proto_model: Trained prototypical network
        campaign_manager: Initialized campaign manager
    """
    print("Integrating few-shot learning with hybrid BERT model...")

    # 1. Initialize prototypical network with hybrid BERT feature extractor
    proto_model = PrototypicalNetwork(
        feature_extractor=hybrid_bert_model,
        embedding_dim=embedding_dim
    ).to(device)

    # 2. Create few-shot dataset wrappers
    train_few_shot_dataset = FewShotPhishingDataset(
        input_ids=train_dataset['input_ids'],
        attention_masks=train_dataset['attention_masks'],
        engineered_features=train_dataset['engineered_features'],
        labels=train_dataset['labels'],
        urls=train_dataset.get('urls', None)
    )

    test_few_shot_dataset = FewShotPhishingDataset(
        input_ids=test_dataset['input_ids'],
        attention_masks=test_dataset['attention_masks'],
        engineered_features=test_dataset['engineered_features'],
        labels=test_dataset['labels'],
        urls=test_dataset.get('urls', None)
    )

    # 3. Initialize campaign manager
    campaign_manager = PhishingCampaignManager(
        embedding_model=proto_model,
        eps=0.5,  # DBSCAN clustering parameter
        min_samples=5  # Minimum samples for a cluster
    )

    # 4. Detect and register campaigns from training data
    campaign_labels = campaign_manager.detect_campaigns(
        train_few_shot_dataset.input_ids,
        train_few_shot_dataset.attention_masks,
        train_few_shot_dataset.engineered_features,
        train_few_shot_dataset.urls
    )

    campaign_ids = campaign_manager.register_campaigns(
        campaign_labels,
        train_few_shot_dataset.input_ids,
        train_few_shot_dataset.attention_masks,
        train_few_shot_dataset.engineered_features,
        train_few_shot_dataset.labels,
        train_few_shot_dataset.urls
    )

    # 5. Train prototypical network
    optimizer = torch.optim.Adam(proto_model.parameters(), lr=0.0001)

    # Learning rate scheduler
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)

    # Train the model
    training_metrics = train_prototypical_network(
        proto_model=proto_model,
        dataset=train_few_shot_dataset,
        optimizer=optimizer,
        scheduler=scheduler,
        n_epochs=5,
        n_episodes=50,
        n_shot=5,
        n_query=15,
        device=device,
        eval_interval=10
    )

    # 6. Visualize training progress
    visualize_training_progress(training_metrics)

    # 7. Evaluate on test set
    test_metrics = evaluate_prototypical_network(
        proto_model=proto_model,
        dataset=test_few_shot_dataset,
        n_episodes=50,
        n_shot=5,
        n_query=15,
        device=device
    )

    print("\nTest set metrics:")
    print(f"Accuracy: {test_metrics['accuracy']:.4f}")
    print(f"Precision: {test_metrics['precision']:.4f}")
    print(f"Recall: {test_metrics['recall']:.4f}")
    print(f"F1 Score: {test_metrics['f1']:.4f}")

    # 8. Visualize performance vs shots
    shot_metrics = visualize_shot_performance(
        proto_model=proto_model,
        dataset=test_few_shot_dataset,
        shot_values=[1, 3, 5, 10, 15],
        device=device
    )

    # 9. Evaluate campaign detection
    campaign_metrics = campaign_manager.evaluate_campaign_detection(
        proto_model=proto_model,
        dataset=test_few_shot_dataset,
        campaign_ids=list(campaign_manager.campaigns.keys()),
        n_shot=5,
        n_query=15,
        device=device
    )

    print("\nCampaign detection metrics:")
    for campaign_id, metrics in campaign_metrics.items():
        print(f"Campaign {campaign_id}:")
        print(f"  Accuracy: {metrics['accuracy']:.4f}")
        print(f"  F1 Score: {metrics['f1']:.4f}")

    # 10. Save trained model and campaign manager
    os.makedirs('few_shot_artifacts', exist_ok=True)

    # Save model
    torch.save(proto_model.state_dict(), 'few_shot_artifacts/proto_model.pt')

    # Save campaign manager (without embedding model to avoid circular references)
    temp_embedding_model = campaign_manager.embedding_model
    campaign_manager.embedding_model = None

    with open('few_shot_artifacts/campaign_manager.pkl', 'wb') as f:
        pickle.dump(campaign_manager, f)

    # Restore embedding model
    campaign_manager.embedding_model = temp_embedding_model

    # Save campaign metadata as JSON for easier inspection
    campaign_metadata = {}
    for campaign_id, campaign in campaign_manager.campaigns.items():
        campaign_metadata[campaign_id] = {
            'size': campaign['size'],
            'creation_time': campaign['creation_time'].isoformat(),
            'cluster_label': int(campaign['cluster_label'])
        }

        if 'urls' in campaign and campaign['urls'] is not None:
            # Include a few example URLs
            campaign_metadata[campaign_id]['example_urls'] = campaign['urls'][:5]

    with open('few_shot_artifacts/campaign_metadata.json', 'w') as f:
        json.dump(campaign_metadata, f, indent=2)

    # 11. Create prediction function
    def predict_with_few_shot(url, engineered_features):
        """
        Production-ready prediction function using both base model and few-shot learning

        Args:
            url: URL to classify
            engineered_features: Dictionary of engineered features

        Returns:
            dict: Prediction results
        """
        # Preprocess URL
        processed_url = preprocess_url(url)

        # Tokenize URL
        encoded = tokenizer.encode_plus(
            processed_url,
            add_special_tokens=True,
            max_length=128,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )

        input_ids = encoded['input_ids'].to(device)
        attention_mask = encoded['attention_mask'].to(device)

        # Preprocess engineered features
        feature_values = np.array([[engineered_features.get(feature, 0)
                                 for feature in hybrid_bert_model.feature_encoder[0].in_features]])
        scaled_features = scaler.transform(feature_values).astype(np.float32)
        features_tensor = torch.tensor(scaled_features, dtype=torch.float32).to(device)

        # Get base model prediction
        hybrid_bert_model.eval()
        with torch.no_grad():
            base_outputs = hybrid_bert_model(input_ids, attention_mask, features_tensor)
            base_probs = F.softmax(base_outputs, dim=1)
            _, base_pred = torch.max(base_outputs, dim=1)

        # Initialize few-shot results
        fs_results = {
            'campaigns': {},
            'final_prediction': None,
            'final_confidence': 0.0
        }

        # Test against each campaign
        proto_model.eval()
        for campaign_id, campaign in campaign_manager.campaigns.items():
            # Get support examples (5 phishing from campaign, 5 legitimate)
            support_ids = campaign['input_ids'][:5].to(device)
            support_mask = campaign['attention_mask'][:5].to(device)
            support_features = campaign['engineered_features'][:5].to(device)
            support_labels = torch.ones(5, dtype=torch.long).to(device)  # All are phishing

            # Add negative examples (legitimate)
            # In production, you would have a stored set of negative examples
            legitimate_indices = campaign_manager.legitimate_indices[:5]
            support_ids = torch.cat([
                support_ids,
                train_few_shot_dataset.input_ids[legitimate_indices].to(device)
            ])
            support_mask = torch.cat([
                support_mask,
                train_few_shot_dataset.attention_masks[legitimate_indices].to(device)
            ])
            support_features = torch.cat([
                support_features,
                train_few_shot_dataset.engineered_features[legitimate_indices].to(device)
            ])
            support_labels = torch.cat([
                support_labels,
                torch.zeros(5, dtype=torch.long).to(device)
            ])

            # Get few-shot prediction
            with torch.no_grad():
                logits = proto_model(
                    support_ids,
                    support_mask,
                    support_features,
                    support_labels,
                    input_ids,
                    attention_mask,
                    features_tensor
                )

                probs = F.softmax(logits, dim=1)
                _, pred = torch.max(logits, dim=1)

            # Store campaign-specific result
            fs_results['campaigns'][campaign_id] = {
                'prediction': pred.item(),
                'confidence': probs[0, pred.item()].item()
            }

        # Combine campaign results
        if fs_results['campaigns']:
            campaign_preds = [details['prediction'] for _, details in fs_results['campaigns'].items()]
            campaign_confs = [details['confidence'] for _, details in fs_results['campaigns'].items()]

            # If any campaign identifies as phishing with high confidence
            if 1 in campaign_preds and max([campaign_confs[j] for j, pred in enumerate(campaign_preds) if pred == 1]) > 0.8:
                fs_results['final_prediction'] = 1
                fs_results['final_confidence'] = max([campaign_confs[j] for j, pred in enumerate(campaign_preds) if pred == 1])
            else:
                # Use the prediction with highest confidence
                max_conf_idx = campaign_confs.index(max(campaign_confs))
                fs_results['final_prediction'] = campaign_preds[max_conf_idx]
                fs_results['final_confidence'] = campaign_confs[max_conf_idx]

        # Ensemble the predictions
        if fs_results['final_prediction'] is not None:
            # If both models agree
            if base_pred.item() == fs_results['final_prediction']:
                final_pred = base_pred.item()
                # Take the higher confidence
                final_conf = max(base_probs[0, base_pred.item()].item(), fs_results['final_confidence'])
                ensemble_method = 'agreement'
            else:
                # If base model is highly confident, trust it
                if base_probs[0, base_pred.item()].item() > 0.9:
                    final_pred = base_pred.item()
                    final_conf = base_probs[0, base_pred.item()].item()
                    ensemble_method = 'base_high_conf'
                # If few-shot model identifies as phishing with good confidence, trust it
                elif fs_results['final_prediction'] == 1 and fs_results['final_confidence'] > 0.7:
                    final_pred = 1
                    final_conf = fs_results['final_confidence']
                    ensemble_method = 'fs_phish_conf'
                # Otherwise use weighted ensemble
                else:
                    base_weight = 0.7
                    fs_weight = 0.3

                    # Calculate weighted probabilities
                    p_phishing = base_weight * base_probs[0, 1].item() + \
                                fs_weight * (fs_results['final_prediction'] * fs_results['final_confidence'])

                    final_pred = 1 if p_phishing > 0.5 else 0
                    final_conf = abs(p_phishing - 0.5) * 2  # Scale to [0, 1]
                    ensemble_method = 'weighted'
        else:
            # If no campaign results, use base model
            final_pred = base_pred.item()
            final_conf = base_probs[0, final_pred].item()
            ensemble_method = 'base_only'

        return {
            'url': url,
            'is_phishing': bool(final_pred),
            'confidence': float(final_conf),
            'base_prediction': int(base_pred.item()),
            'base_confidence': float(base_probs[0, base_pred.item()].item()),
            'few_shot_prediction': fs_results['final_prediction'],
            'few_shot_confidence': float(fs_results['final_confidence']),
            'ensemble_method': ensemble_method,
            'campaign_matches': {
                campaign_id: details
                for campaign_id, details in fs_results['campaigns'].items()
                if details['confidence'] > 0.6  # Only include confident matches
            }
        }

    # Save example of integrated prediction function
    with open('few_shot_artifacts/prediction_example.py', 'w') as f:
        f.write("""
import torch
import torch.nn.functional as F
import numpy as np
import pickle
import json
from transformers import BertTokenizer
from model_definition import HybridBERTModel, PrototypicalNetwork

def load_models(model_dir='model_artifacts', fs_dir='few_shot_artifacts'):
    '''Load all necessary components for prediction'''
    # Device setup
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # Load tokenizer
    tokenizer = BertTokenizer.from_pretrained(f'{model_dir}/tokenizer/')

    # Load scaler
    with open(f'{model_dir}/scaler.pkl', 'rb') as f:
        scaler = pickle.load(f)

    # Load feature names
    with open(f'{model_dir}/feature_names.json', 'r') as f:
        feature_names = json.load(f)

    # Initialize and load base model
    base_model = HybridBERTModel(
        bert_model_name='bert-base-uncased',
        num_engineered_features=len(feature_names)
    ).to(device)

    base_model.load_state_dict(torch.load(f'{model_dir}/best_phishing_model.pt',
                                         map_location=device))
    base_model.eval()

    # Initialize and load prototypical network
    proto_model = PrototypicalNetwork(
        feature_extractor=base_model,
        embedding_dim=256
    ).to(device)

    proto_model.load_state_dict(torch.load(f'{fs_dir}/proto_model.pt',
                                         map_location=device))
    proto_model.eval()

    # Load campaign manager
    with open(f'{fs_dir}/campaign_manager.pkl', 'rb') as f:
        campaign_manager = pickle.load(f)

    # Reconnect the embedding model
    campaign_manager.embedding_model = proto_model

    return tokenizer, scaler, base_model, proto_model, campaign_manager, feature_names, device

def preprocess_url(url):
    '''Preprocess URL for tokenization'''
    # Replace special characters with spaces around them
    for char in ['/', '.', '-', '=', '?', '&', '_', ':', '@']:
        url = url.replace(char, f' {char} ')
    # Additional preprocessing
    url = url.replace('http', 'http ')
    url = url.replace('https', 'https ')
    url = url.replace('www', 'www ')
    return url

def predict_with_few_shot(url, engineered_features, tokenizer, scaler, base_model,
                         proto_model, campaign_manager, feature_names, device):
    '''
    Production-ready prediction function using both base model and few-shot learning

    Args:
        url: URL to classify
        engineered_features: Dictionary of engineered features
        tokenizer: BERT tokenizer
        scaler: Feature scaler
        base_model: Base HybridBERTModel
        proto_model: Prototypical network
        campaign_manager: Campaign manager with registered campaigns
        feature_names: List of feature names
        device: Device to run on

    Returns:
        dict: Prediction results with base model and few-shot ensembled
    '''
    # Preprocess URL
    processed_url = preprocess_url(url)

    # Tokenize URL
    encoded = tokenizer.encode_plus(
        processed_url,
        add_special_tokens=True,
        max_length=128,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='pt'
    )

    input_ids = encoded['input_ids'].to(device)
    attention_mask = encoded['attention_mask'].to(device)

    # Preprocess engineered features
    # Convert dict to array using feature names
    feature_values = np.array([[
        engineered_features.get(feature, 0) for feature in feature_names
    ]])
    scaled_features = scaler.transform(feature_values).astype(np.float32)
    features_tensor = torch.tensor(scaled_features, dtype=torch.float32).to(device)

    # Get base model prediction
    with torch.no_grad():
        base_outputs = base_model(input_ids, attention_mask, features_tensor)
        base_probs = F.softmax(base_outputs, dim=1)
        _, base_pred = torch.max(base_outputs, dim=1)

    # Initialize few-shot results
    fs_results = {
        'campaigns': {},
        'final_prediction': None,
        'final_confidence': 0.0
    }

    # Get few-shot predictions for each campaign
    for campaign_id, campaign in campaign_manager.campaigns.items():
        # Skip campaigns with no support examples
        if 'input_ids' not in campaign or len(campaign['input_ids']) < 5:
            continue

        # Get support examples
        support_ids = campaign['input_ids'][:5].to(device)
        support_mask = campaign['attention_mask'][:5].to(device)
        support_features = campaign['engineered_features'][:5].to(device)
        support_labels = torch.ones(5, dtype=torch.long).to(device)  # All phishing

        # For legitimate examples, use stored legitimate examples
        # In a real system, you would have these pre-stored
        if 'legitimate_ids' in campaign_manager.__dict__:
            leg_ids = campaign_manager.legitimate_ids[:5].to(device)
            leg_mask = campaign_manager.legitimate_masks[:5].to(device)
            leg_features = campaign_manager.legitimate_features[:5].to(device)
        else:
            # Fallback - in production you would pre-store these
            leg_ids = support_ids.clone()  # Placeholder
            leg_mask = support_mask.clone()  # Placeholder
            leg_features = support_features.clone()  # Placeholder

        leg_labels = torch.zeros(5, dtype=torch.long).to(device)  # All legitimate

        # Combine support examples
        c_support_ids = torch.cat([leg_ids, support_ids])
        c_support_mask = torch.cat([leg_mask, support_mask])
        c_support_features = torch.cat([leg_features, support_features])
        c_support_labels = torch.cat([leg_labels, support_labels])

        # Get prediction for this campaign
        with torch.no_grad():
            logits = proto_model(
                c_support_ids,
                c_support_mask,
                c_support_features,
                c_support_labels,
                input_ids,
                attention_mask,
                features_tensor
            )

            probs = F.softmax(logits, dim=1)
            _, pred = torch.max(logits, dim=1)

        # Store result for this campaign
        fs_results['campaigns'][campaign_id] = {
            'prediction': int(pred.item()),
            'confidence': float(probs[0, pred.item()].item())
        }

    # Determine final few-shot prediction from all campaigns
    if fs_results['campaigns']:
        campaign_preds = [details['prediction'] for _, details in fs_results['campaigns'].items()]
        campaign_confs = [details['confidence'] for _, details in fs_results['campaigns'].items()]

        # If any campaign confidently identifies as phishing
        phish_indices = [i for i, pred in enumerate(campaign_preds) if pred == 1]
        if phish_indices and max([campaign_confs[i] for i in phish_indices]) > 0.8:
            fs_results['final_prediction'] = 1
            fs_results['final_confidence'] = max([campaign_confs[i] for i in phish_indices])
        else:
            # Go with highest confidence prediction
            max_idx = campaign_confs.index(max(campaign_confs))
            fs_results['final_prediction'] = campaign_preds[max_idx]
            fs_results['final_confidence'] = campaign_confs[max_idx]

    # Ensemble the predictions
    if fs_results['final_prediction'] is not None:
        # If both models agree
        if base_pred.item() == fs_results['final_prediction']:
            final_pred = base_pred.item()
            final_conf = max(base_probs[0, base_pred.item()].item(), fs_results['final_confidence'])
            ensemble_method = 'agreement'
        else:
            # If base model highly confident, trust it
            if base_probs[0, base_pred.item()].item() > 0.9:
                final_pred = base_pred.item()
                final_conf = base_probs[0, base_pred.item()].item()
                ensemble_method = 'base_high_conf'
            # If few-shot identifies as phishing with good confidence
            elif fs_results['final_prediction'] == 1 and fs_results['final_confidence'] > 0.7:
                final_pred = 1
                final_conf = fs_results['final_confidence']
                ensemble_method = 'fs_phish_conf'
            # Otherwise use weighted ensemble
            else:
                base_weight = 0.7
                fs_weight = 0.3

                # Base model phishing probability
                base_p_phish = base_probs[0, 1].item()
                # Few-shot phishing probability
                fs_p_phish = fs_results['final_prediction'] * fs_results['final_confidence']

                # Weighted ensemble
                final_p_phish = (base_weight * base_p_phish) + (fs_weight * fs_p_phish)
                final_pred = 1 if final_p_phish > 0.5 else 0
                final_conf = abs(final_p_phish - 0.5) * 2  # Scale to [0, 1]
                ensemble_method = 'weighted'
    else:
        # If no campaign results, use only base model
        final_pred = base_pred.item()
        final_conf = base_probs[0, base_pred.item()].item()
        ensemble_method = 'base_only'

    # Return final prediction and detailed information
    return {
        'url': url,
        'is_phishing': bool(final_pred),
        'confidence': float(final_conf),
        'base_prediction': int(base_pred.item()),
        'base_confidence': float(base_probs[0, base_pred.item()].item()),
        'few_shot_prediction': fs_results['final_prediction'],
        'few_shot_confidence': float(fs_results['final_confidence']) if fs_results['final_confidence'] else 0.0,
        'ensemble_method': ensemble_method,
        'campaign_matches': {
            campaign_id: details
            for campaign_id, details in fs_results['campaigns'].items()
            if details['confidence'] > 0.6  # Only include confident matches
        }
    }

# Example usage
if __name__ == '__main__':
    # Load all models and components
    tokenizer, scaler, base_model, proto_model, campaign_manager, feature_names, device = load_models()

    # Example URL and features
    test_url = "http://suspicious-bank-secure.co.uk/login.php"

    # These would come from your feature extraction pipeline
    test_features = {
        'length_url': 47,
        'length_hostname': 27,
        'ip': 0,
        'nb_dots': 2,
        'nb_hyphens': 2,
        'nb_at': 0,
        'nb_qm': 0,
        'nb_and': 0,
        'nb_or': 0,
        'nb_eq': 0,
        'nb_underscore': 0,
        'nb_tilde': 0,
        'nb_percent': 0,
        'nb_slash': 1,
        'nb_star': 0,
        'nb_colon': 1,
        'nb_comma': 0,
        'nb_semicolumn': 0,
        'nb_dollar': 0,
        'nb_space': 0,
        'nb_www': 0,
        'nb_com': 0,
        'nb_dslash': 1,
        'http_in_path': 0,
        'https_token': 0,
        'ratio_digits_url': 0.0,
        'ratio_digits_host': 0.0,
        'punycode': 0,
        'port': 0,
        'tld_in_path': 0,
        'tld_in_subdomain': 0,
        'abnormal_subdomain': 0,
        'prefix_suffix': 1,
        'random_domain': 0,
        'shortening_service': 0,
        'path_extension': 1,
        'domain_in_brand': 1,
        'brand_in_subdomain': 0,
        'brand_in_path': 0,
        'suspicious_tld': 0,
        'statistical_report': 0,
        'length_words_raw': 5,
        'char_repeat': 0,
        'shortest_words_raw': 2,
        'shortest_word_host': 2,
        'shortest_word_path': 5,
        'longest_words_raw': 10,
        'longest_word_host': 10,
        'longest_word_path': 5,
        'avg_words_raw': 6.2,
        'avg_word_host': 7.0,
        'avg_word_path': 5.0,
        'phish_hints': 1,
        'domain_age_days': 7,
        'path_length': 10,
        'url_length': 47,
        'domain_length': 27
    }

    # Make prediction
    result = predict_with_few_shot(
        test_url,
        test_features,
        tokenizer,
        scaler,
        base_model,
        proto_model,
        campaign_manager,
        feature_names,
        device
    )

    print("Prediction results:")
    print(json.dumps(result, indent=2))
""")

    # Return the trained models
    return proto_model, campaign_manager

# ================================================================================
# EXAMPLE USAGE
# ================================================================================

def main():
    """Example of how to use the few-shot learning extension"""
    # Set device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    # 1. Load your existing HybridBERTModel
    # This would be your trained model from the original implementation
    model_path = 'model_artifacts/best_phishing_model.pt'

    # Load model configuration
    with open('model_artifacts/model_config.json', 'r') as f:
        model_config = json.load(f)

    # Initialize model
    hybrid_bert_model = HybridBERTModel(
        bert_model_name='bert-base-uncased',
        num_engineered_features=model_config['selected_features_count']
    ).to(device)

    # Load weights
    hybrid_bert_model.load_state_dict(torch.load(model_path, map_location=device))
    hybrid_bert_model.eval()

    # 2. Load tokenizer and scaler
    tokenizer = BertTokenizer.from_pretrained('model_artifacts/tokenizer/')

    with open('model_artifacts/scaler.pkl', 'rb') as f:
        scaler = pickle.load(f)

    # 3. Load your train and test datasets
    # For this example, we'll create dummy datasets
    # In practice, you would load your actual datasets

    # Generate dummy data
    num_train = 1000
    num_test = 200
    feature_dim = model_config['selected_features_count']

    # Create dummy tensors for demonstration
    train_input_ids = torch.randint(0, 30000, (num_train, 128))
    train_attention_masks = torch.ones(num_train, 128)
    train_engineered_features = torch.randn(num_train, feature_dim)
    train_labels = torch.randint(0, 2, (num_train,))

    test_input_ids = torch.randint(0, 30000, (num_test, 128))
    test_attention_masks = torch.ones(num_test, 128)
    test_engineered_features = torch.randn(num_test, feature_dim)
    test_labels = torch.randint(0, 2, (num_test,))

    # Create dataset dictionaries
    train_dataset = {
        'input_ids': train_input_ids,
        'attention_masks': train_attention_masks,
        'engineered_features': train_engineered_features,
        'labels': train_labels,
        'urls': [f"https://example-{i}.com/page" for i in range(num_train)]
    }

    test_dataset = {
        'input_ids': test_input_ids,
        'attention_masks': test_attention_masks,
        'engineered_features': test_engineered_features,
        'labels': test_labels,
        'urls': [f"https://test-{i}.com/page" for i in range(num_test)]
    }

    # 4. Integrate few-shot learning
    proto_model, campaign_manager = integrate_few_shot_learning(
        hybrid_bert_model=hybrid_bert_model,
        tokenizer=tokenizer,
        scaler=scaler,
        train_dataset=train_dataset,
        test_dataset=test_dataset,
        embedding_dim=256,
        device=device
    )

    # 5. Test on some example URLs
    test_urls = [
        "http://paypal-secure-login.com/account/signin",
        "http://bank0famerica.com/login.php",
        "http://www.google.com",
        "http://amazon.com/product/12345",
        "http://faceb00k-security.net/verify"
    ]

    # Generate random features for testing
    test_features = torch.randn(len(test_urls), feature_dim).numpy()

    # Test the few-shot model on these URLs
    results = test_few_shot_on_new_campaign(
        proto_model=proto_model,
        campaign_manager=campaign_manager,
        base_model=hybrid_bert_model,
        tokenizer=tokenizer,
        scaler=scaler,
        test_urls=test_urls,
        device=device
    )

    # Print results
    print("\nFew-shot detection results:")
    for result in results:
        print(f"URL: {result['url']}")
        print(f"  Base model: {'Phishing' if result['base_prediction'] == 1 else 'Legitimate'} "
              f"(Confidence: {result['base_confidence']:.4f})")
        print(f"  Few-shot: {'Phishing' if result['fs_prediction'] == 1 else 'Legitimate'} "
              f"(Confidence: {result['fs_confidence']:.4f})")
        print(f"  Final verdict: {'Phishing' if result['final_prediction'] == 1 else 'Legitimate'} "
              f"(Confidence: {result['final_confidence']:.4f}, Method: {result['ensemble_method']})")
        print()

if __name__ == "__main__":
    main()

pip install captum

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_curve, auc, confusion_matrix, matthews_corrcoef,
    classification_report, precision_recall_curve
)
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectFromModel, RFE, RFECV, mutual_info_classif, SelectKBest
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
import eli5
from eli5.sklearn import PermutationImportance
import shap
import lime
import lime.lime_tabular
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertModel, BertConfig, get_linear_schedule_with_warmup
from tqdm import tqdm
from captum.attr import IntegratedGradients, LayerGradCam, Occlusion, NoiseTunnel
import captum.attr as attr
import warnings
warnings.filterwarnings('ignore')

# Set random seeds for reproducibility
seed = 42
np.random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

# Check for GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# 1. Data Loading and Preprocessing
print("Loading and preprocessing data...")

# Load the dataset
df = pd.read_csv('/content/drive/MyDrive/phishing/dataset_full.csv')

print(f"Dataset shape: {df.shape}")

# Check for missing values
print("\nChecking for missing values:")
print(df.isnull().sum().sum())

# Check class distribution
print("\nClass distribution:")
print(df['phishing'].value_counts(normalize=True))

# Extract features and target
features = df.drop(['phishing', 'url'] if 'url' in df.columns else ['phishing'], axis=1)
urls = df['url'].values if 'url' in df.columns else None
labels = df['phishing'].values

# If URL column doesn't exist, create a placeholder
if 'url' not in df.columns:
    print("Warning: URL column not found in the dataset. Adding a placeholder column.")
    # Generate placeholder URLs based on existing features
    urls = []
    for i, row in df.iterrows():
        if row['phishing'] == 1:  # Phishing
            urls.append(f"http://phishing-example-{i}.com/page")
        else:  # Legitimate
            urls.append(f"https://legitimate-example-{i}.com/page")
    df['url'] = urls
    urls = df['url'].values

# 2. Feature Engineering and Selection
print("\n2. Performing Feature Selection...")

# Basic statistical analysis
print("\nBasic feature statistics:")
print(features.describe().T.sort_values(by='mean', ascending=False).head(10))

# 2.1 Feature Selection using Random Forest Feature Importance
print("\n2.1 Random Forest Feature Importance")
rf_selector = RandomForestClassifier(n_estimators=100, random_state=seed)
rf_selector.fit(features, labels)

# Get feature importance
feature_importances = pd.DataFrame(
    {'feature': features.columns, 'importance': rf_selector.feature_importances_}
)
feature_importances = feature_importances.sort_values('importance', ascending=False)

# Visualize top features
plt.figure(figsize=(12, 8))
sns.barplot(x='importance', y='feature', data=feature_importances.head(15))
plt.title('Feature Importance from Random Forest')
plt.tight_layout()
plt.savefig('rf_feature_importance.png')
plt.close()

# Select top features based on importance
rf_selector = SelectFromModel(
    RandomForestClassifier(n_estimators=100, random_state=seed),
    threshold='median'  # Adjust threshold as needed
)
rf_selector.fit(features, labels)
selected_features_rf = features.columns[rf_selector.get_support()]
print(f"Selected {len(selected_features_rf)} features using RF importance")
print(f"Top 10 features: {list(selected_features_rf)[:10]}")

# 2.2 Feature Selection using Mutual Information
print("\n2.2 Mutual Information Feature Selection")
mi_selector = SelectKBest(mutual_info_classif, k=min(50, features.shape[1]))
mi_selector.fit(features, labels)

# Get feature scores
mi_scores = pd.DataFrame(
    {'feature': features.columns, 'mi_score': mi_selector.scores_}
)
mi_scores = mi_scores.sort_values('mi_score', ascending=False)

# Visualize top features by mutual information
plt.figure(figsize=(12, 8))
sns.barplot(x='mi_score', y='feature', data=mi_scores.head(15))
plt.title('Feature Importance from Mutual Information')
plt.tight_layout()
plt.savefig('mi_feature_importance.png')
plt.close()

selected_features_mi = features.columns[mi_selector.get_support()]
print(f"Selected {len(selected_features_mi)} features using Mutual Information")
print(f"Top 10 features: {list(selected_features_mi)[:10]}")

# 2.3 Recursive Feature Elimination with Cross-Validation
print("\n2.3 Recursive Feature Elimination with Cross-Validation")

# Use a lighter version for demonstration, can be more comprehensive in production
min_features = max(5, min(30, features.shape[1] // 3))
rfecv = RFECV(
    estimator=GradientBoostingClassifier(random_state=seed),
    step=max(1, features.shape[1] // 20),  # Step size for feature elimination
    cv=5,  # 5-fold cross-validation
    scoring='f1',  # Optimize for F1 score
    min_features_to_select=min_features,
    n_jobs=-1
)

# If the dataset is too large, use a subset for RFECV
if features.shape[0] > 10000:
    print("  Using a subset of data for RFECV due to large dataset size")
    X_subset, _, y_subset, _ = train_test_split(
        features, labels,
        train_size=10000,
        random_state=seed,
        stratify=labels
    )
    rfecv.fit(X_subset, y_subset)
else:
    rfecv.fit(features, labels)

# Plot number of features vs. CV scores
plt.figure(figsize=(10, 6))
# In newer scikit-learn versions, grid_scores_ was renamed to cv_results_['mean_test_score']
if hasattr(rfecv, 'grid_scores_'):
    # For older versions of scikit-learn
    plt.plot(range(min_features, len(rfecv.grid_scores_) + min_features), rfecv.grid_scores_)
elif hasattr(rfecv, 'cv_results_'):
    # For newer versions of scikit-learn
    cv_scores = rfecv.cv_results_['mean_test_score']
    plt.plot(range(min_features, len(cv_scores) + min_features), cv_scores)
else:
    # Alternative approach if neither attribute exists
    feature_counts = np.sum(rfecv.get_support(), axis=0)
    plt.axvline(x=feature_counts, color='r', linestyle='--',
                label=f'Selected features: {feature_counts}')
    plt.legend()
    print(f"Warning: RFECV visualization modified - could not find scores attribute.")

plt.xlabel('Number of features')
plt.ylabel('Cross-validation score (F1)')
plt.title('RFECV: Number of Features vs. Performance')
plt.tight_layout()
plt.savefig('rfecv_feature_selection.png')
plt.close()

selected_features_rfecv = features.columns[rfecv.support_]
print(f"Selected {len(selected_features_rfecv)} features using RFECV")
print(f"Top 10 features: {list(selected_features_rfecv)[:10]}")

# 2.4 Combine feature selection methods to get final feature set
# Strategy: Take the union of features that appear in at least 2 selection methods
print("\n2.4 Combining Feature Selection Methods")

# Count occurrences of each feature across methods
feature_occurrences = {}
for feature in features.columns:
    count = 0
    if feature in selected_features_rf:
        count += 1
    if feature in selected_features_mi:
        count += 1
    if feature in selected_features_rfecv:
        count += 1
    feature_occurrences[feature] = count

# Select features that appear in at least 2 selection methods
final_selected_features = [
    feature for feature, count in feature_occurrences.items() if count >= 2
]

print(f"Final selected features: {len(final_selected_features)} out of {features.shape[1]}")
print(f"Top 15 features: {final_selected_features[:15]}")

# Use only the selected features for further processing
features = features[final_selected_features]
print(f"Reduced feature set shape: {features.shape}")

# Feature correlation analysis
correlation_matrix = features.corr().abs()
plt.figure(figsize=(14, 12))
sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', linewidths=0.5)
plt.title('Feature Correlation Matrix')
plt.tight_layout()
plt.savefig('feature_correlation.png')
plt.close()

# Identify highly correlated features (r > 0.9)
high_corr_features = []
for i in range(len(correlation_matrix.columns)):
    for j in range(i):
        if correlation_matrix.iloc[i, j] > 0.9:
            colname = correlation_matrix.columns[i]
            high_corr_features.append(colname)

if high_corr_features:
    print(f"\nIdentified {len(set(high_corr_features))} highly correlated features (r > 0.9)")
    # In practice, you might want to remove some of these features
    # For this example, we'll keep them to avoid changing the feature set too much

# Normalize the engineered features
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features).astype(np.float32)

# Split the data
# In the train_test_split:
X_train, X_test, urls_train, urls_test, y_train, y_test = train_test_split(
    features_scaled, urls, labels, test_size=0.2, random_state=seed, stratify=labels
)


print(f"Training set size: {len(X_train)}, Test set size: {len(X_test)}")

# 3. BERT Tokenization for URLs
print("\n3. Tokenizing URLs for BERT...")

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

def preprocess_url(url):
    """Preprocess URL to make it more suitable for BERT tokenization"""
    # Replace special characters with spaces around them to help tokenization
    for char in ['/', '.', '-', '=', '?', '&', '_', ':', '@']:
        url = url.replace(char, f' {char} ')
    # Additional preprocessing specific to URLs
    url = url.replace('http', 'http ')
    url = url.replace('https', 'https ')
    url = url.replace('www', 'www ')
    return url

# Tokenize URLs
def tokenize_urls(urls, max_length=128):
    input_ids = []
    attention_masks = []

    for url in tqdm(urls, desc="Tokenizing URLs"):
        processed_url = preprocess_url(url)
        encoded = tokenizer.encode_plus(
            processed_url,
            add_special_tokens=True,
            max_length=max_length,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )

        input_ids.append(encoded['input_ids'])
        attention_masks.append(encoded['attention_mask'])

    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)

# Tokenize training and testing URLs
train_input_ids, train_attention_masks = tokenize_urls(urls_train)
test_input_ids, test_attention_masks = tokenize_urls(urls_test)

# 4. Create a PyTorch Dataset
class PhishingDataset(Dataset):
    def __init__(self, input_ids, attention_masks, engineered_features, labels):
        self.input_ids = input_ids
        self.attention_masks = attention_masks
        # Convert engineered_features to float32 explicitly
        self.engineered_features = torch.tensor(engineered_features, dtype=torch.float32)
        self.labels = torch.tensor(labels, dtype=torch.long)

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
            'input_ids': self.input_ids[idx],
            'attention_mask': self.attention_masks[idx],
            'engineered_features': self.engineered_features[idx],
            'labels': self.labels[idx]
        }

# Create datasets
train_dataset = PhishingDataset(
    train_input_ids,
    train_attention_masks,
    X_train,
    y_train
)

test_dataset = PhishingDataset(
    test_input_ids,
    test_attention_masks,
    X_test,
    y_test
)

# Create data loaders
batch_size = 32  # Adjust based on memory constraints

train_dataloader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True
)

test_dataloader = DataLoader(
    test_dataset,
    batch_size=batch_size
)

print(f"Created dataloaders with batch size: {batch_size}")

# 5. Define the Enhanced Hybrid BERT Model with Interpretability
class HybridBERTModel(nn.Module):
    def __init__(self, bert_model_name='bert-base-uncased', num_engineered_features=None):
        super(HybridBERTModel, self).__init__()

        # Initialize BERT with a configuration that allows for gradient checkpointing
        self.bert_config = BertConfig.from_pretrained(bert_model_name)
        self.bert_config.gradient_checkpointing = True  # Memory optimization
        self.bert = BertModel.from_pretrained(bert_model_name, config=self.bert_config)

        # Optional: Freeze some BERT layers to reduce training time and prevent overfitting
        # Freeze the first 6 layers of BERT
        modules = [self.bert.embeddings, *self.bert.encoder.layer[:6]]
        for module in modules:
            for param in module.parameters():
                param.requires_grad = False

        # Get actual feature dimension
        if num_engineered_features is None:
            num_engineered_features = X_train.shape[1]

        # Dropout layers with different rates
        self.dropout1 = nn.Dropout(0.3)
        self.dropout2 = nn.Dropout(0.5)  # Higher dropout for later layers

        # BERT output size
        bert_hidden_size = self.bert.config.hidden_size  # Usually 768

        # Attention mechanism for engineered features
        self.feature_attention = nn.Sequential(
            nn.Linear(num_engineered_features, num_engineered_features),
            nn.Tanh(),
            nn.Linear(num_engineered_features, 1, bias=False),
            nn.Softmax(dim=1)
        )

        # Process engineered features separately before combining
        self.feature_encoder = nn.Sequential(
            nn.Linear(num_engineered_features, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.3)
        )

        # Process BERT embeddings separately
        self.bert_encoder = nn.Sequential(
            nn.Linear(bert_hidden_size, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.3)
        )

        # Combined size after separate processing
        combined_size = 256 + 128  # BERT encoder output + feature encoder output

        # Fully connected layers with residual connections
        self.fc1 = nn.Linear(combined_size, 256)
        self.bn1 = nn.BatchNorm1d(256)

        self.fc2 = nn.Linear(256, 128)
        self.bn2 = nn.BatchNorm1d(128)

        self.fc3 = nn.Linear(128, 64)
        self.bn3 = nn.BatchNorm1d(64)

        # Output layer
        self.classifier = nn.Linear(64, 2)  # 2 output classes

        # Activation functions
        self.relu = nn.ReLU()
        self.gelu = nn.GELU()  # Sometimes performs better than ReLU

        # Store intermediate outputs for interpretability
        self.bert_output = None
        self.feature_output = None
        self.combined_output = None
        self.feature_attention_weights = None

    def forward(self, input_ids, attention_mask, engineered_features):
        # Get BERT embeddings
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_attentions=True,  # Get attention weights for interpretability
            output_hidden_states=True  # Get all hidden states
        )

        pooled_output = outputs.pooler_output  # [CLS] token embedding
        self.bert_attentions = outputs.attentions  # Store for interpretability
        self.bert_hidden_states = outputs.hidden_states  # Store for interpretability

        # Apply dropout to BERT output
        pooled_output = self.dropout1(pooled_output)

        # Process BERT output
        bert_encoded = self.bert_encoder(pooled_output)
        self.bert_output = bert_encoded  # Store for interpretability

        # Apply attention to engineered features
        feature_attention = self.feature_attention(engineered_features)
        self.feature_attention_weights = feature_attention  # Store for interpretability

        # Apply attention weights (optional enhancement)
        # weighted_features = engineered_features * feature_attention

        # Process engineered features
        feature_encoded = self.feature_encoder(engineered_features)
        self.feature_output = feature_encoded  # Store for interpretability

        # Concatenate BERT output with engineered features
        combined = torch.cat((bert_encoded, feature_encoded), dim=1)
        self.combined_output = combined  # Store for interpretability

        # First fully connected block with residual connection
        residual = combined
        x = self.fc1(combined)
        x = self.bn1(x)
        x = self.gelu(x)
        x = self.dropout2(x)

        # Second fully connected block
        x = self.fc2(x)
        x = self.bn2(x)
        x = self.gelu(x)
        x = self.dropout2(x)

        # Third fully connected block
        x = self.fc3(x)
        x = self.bn3(x)
        x = self.gelu(x)
        x = self.dropout2(x)

        # Output layer
        logits = self.classifier(x)

        return logits

    # Method to get attention weights for explainability
    def get_attention_weights(self):
        return {
            'bert_attentions': self.bert_attentions,
            'feature_attention': self.feature_attention_weights
        }

    # Method to get intermediate outputs for explainability
    def get_intermediate_outputs(self):
        return {
            'bert_output': self.bert_output,
            'feature_output': self.feature_output,
            'combined_output': self.combined_output
        }

# Initialize the model
# Initialize the model
model = HybridBERTModel(num_engineered_features=X_train.shape[1]).to(device)
model = model.float()  # Explicitly set model to float32 precision
print(f"Model initialized with {sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable parameters")
# 6. Advanced Training Setup
# Hyperparameters with more optimization
epochs = 2
learning_rate = 2e-5
weight_decay = 0.01
lr_scheduler_factor = 0.75
lr_scheduler_patience = 2

# Check class imbalance and use weighted loss if needed
# Check class imbalance and use weighted loss if needed
class_weights = None
class_counts = np.bincount(y_train)
if class_counts[0] / class_counts[1] > 1.5 or class_counts[1] / class_counts[0] > 1.5:
    print("\nDetected class imbalance. Using weighted loss function.")
    weight = torch.tensor([1.0, class_counts[0] / class_counts[1]], dtype=torch.float32).to(device)  # Ensure float32
    criterion = nn.CrossEntropyLoss(weight=weight)
else:
    criterion = nn.CrossEntropyLoss()

# Optimizer with weight decay
optimizer = torch.optim.AdamW(
    [
        {"params": model.bert.parameters(), "lr": learning_rate / 10},  # Lower LR for BERT
        {"params": [p for n, p in model.named_parameters() if "bert" not in n], "lr": learning_rate}
    ],
    lr=learning_rate,
    weight_decay=weight_decay
)

# Learning rate scheduler
total_steps = len(train_dataloader) * epochs
warmup_steps = int(0.1 * total_steps)  # 10% of total steps for warmup
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=warmup_steps,
    num_training_steps=total_steps
)

# 7. Enhanced Training Loop with Early Stopping
print("\n7. Starting training with early stopping...")

# For tracking metrics
train_losses = []
train_accs = []
val_losses = []
val_accs = []

def train_epoch(model, dataloader, optimizer, scheduler, criterion, device):
    model.train()
    total_loss = 0
    correct_predictions = 0
    total_predictions = 0

    progress_bar = tqdm(dataloader, desc="Training")

    for batch in progress_bar:
        # Move batch to device and ensure float32
        input_ids = batch['input_ids'].to(device, dtype=torch.long)  # Input IDs should be long
        attention_mask = batch['attention_mask'].to(device, dtype=torch.long)  # Attention mask should be long
        engineered_features = batch['engineered_features'].to(device, dtype=torch.float32)  # Ensure float32
        labels = batch['labels'].to(device, dtype=torch.long)  # Labels should be long

        # Clear gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = model(input_ids, attention_mask, engineered_features)

        # Calculate loss
        loss = criterion(outputs, labels)

        # Backward pass
        loss.backward()

        # Clip gradients to avoid exploding gradients
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        # Update weights
        optimizer.step()

        # Update learning rate
        scheduler.step()

        # Update metrics
        total_loss += loss.item()

        _, preds = torch.max(outputs, dim=1)
        correct_predictions += (preds == labels).sum().item()
        total_predictions += labels.size(0)

        # Update progress bar
        progress_bar.set_postfix({
            'loss': f"{loss.item():.4f}",
            'acc': f"{correct_predictions/total_predictions:.4f}"
        })

    epoch_loss = total_loss / len(dataloader)
    epoch_acc = correct_predictions / total_predictions

    return epoch_loss, epoch_acc

def evaluate(model, dataloader, criterion, device):
    model.eval()
    total_loss = 0
    correct_predictions = 0
    total_predictions = 0

    all_preds = []
    all_labels = []
    all_probs = []

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            # Move batch to device and ensure correct dtypes
            input_ids = batch['input_ids'].to(device, dtype=torch.long)
            attention_mask = batch['attention_mask'].to(device, dtype=torch.long)
            engineered_features = batch['engineered_features'].to(device, dtype=torch.float32)
            labels = batch['labels'].to(device, dtype=torch.long)

            # Forward pass
            outputs = model(input_ids, attention_mask, engineered_features)

            # Calculate loss
            loss = criterion(outputs, labels)

            # Get predictions and probabilities
            probs = F.softmax(outputs, dim=1)
            _, preds = torch.max(outputs, dim=1)

            # Update metrics
            total_loss += loss.item()
            correct_predictions += (preds == labels).sum().item()
            total_predictions += labels.size(0)

            # Store predictions and labels for metrics calculation
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs[:, 1].cpu().numpy())  # Probability of class 1 (phishing)

    epoch_loss = total_loss / len(dataloader)
    epoch_acc = correct_predictions / total_predictions

    return epoch_loss, epoch_acc, all_preds, all_labels, all_probs

# Early stopping parameters
patience = 3
best_val_loss = float('inf')
early_stopping_counter = 0
best_val_acc = 0

# Training loop with early stopping
for epoch in range(epochs):
    print(f"\nEpoch {epoch+1}/{epochs}")

    # Train
    train_loss, train_acc = train_epoch(
        model, train_dataloader, optimizer, scheduler, criterion, device
    )
    train_losses.append(train_loss)
    train_accs.append(train_acc)

    print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")

    # Evaluate
    val_loss, val_acc, val_preds, val_labels, val_probs = evaluate(
        model, test_dataloader, criterion, device
    )
    val_losses.append(val_loss)
    val_accs.append(val_acc)

    print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")

    # Save best model
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save(model.state_dict(), 'best_phishing_model.pt')
        print(f"New best model saved with validation accuracy: {val_acc:.4f}")
        early_stopping_counter = 0
    else:
        early_stopping_counter += 1
        print(f"No improvement in validation accuracy for {early_stopping_counter} epochs")

        if early_stopping_counter >= patience:
            print(f"Early stopping triggered after {epoch+1} epochs")
            break

print("\nTraining complete!")

# 8. Final Evaluation and Metrics
print("\n8. Performing final evaluation...")

# Load the best model
model.load_state_dict(torch.load('best_phishing_model.pt'))
model.eval()

# Get predictions on test set
_, _, y_pred, y_true, y_probs = evaluate(model, test_dataloader, criterion, device)

# Calculate metrics
acc = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)
mcc = matthews_corrcoef(y_true, y_pred)

print("\nTest Metrics:")
print(f"Accuracy: {acc:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"Matthews Correlation Coefficient: {mcc:.4f}")

# Print classification report
print("\nClassification Report:")
print(classification_report(y_true, y_pred, target_names=['Legitimate', 'Phishing']))

# 9. Advanced Visualizations and Evaluation
print("\n9. Creating advanced visualizations...")

# 9.1 Confusion Matrix with Annotations
plt.figure(figsize=(10, 8))
cm = confusion_matrix(y_true, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Legitimate', 'Phishing'],
            yticklabels=['Legitimate', 'Phishing'])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.savefig('confusion_matrix.png')
plt.close()

# 9.2 ROC Curve with Confidence Interval
plt.figure(figsize=(10, 8))
fpr, tpr, _ = roc_curve(y_true, y_probs)
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.savefig('roc_curve.png')
plt.close()

# 9.3 Precision-Recall Curve
plt.figure(figsize=(10, 8))
precision_curve, recall_curve, _ = precision_recall_curve(y_true, y_probs)
pr_auc = auc(recall_curve, precision_curve)

plt.plot(recall_curve, precision_curve, color='green', lw=2, label=f'PR curve (area = {pr_auc:.2f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc="lower left")
plt.savefig('pr_curve.png')
plt.close()

# 9.4 Training History with Early Stopping Indication
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(range(1, len(train_losses)+1), train_losses, 'b-', label='Training Loss')
plt.plot(range(1, len(val_losses)+1), val_losses, 'r-', label='Validation Loss')
plt.axvline(x=len(val_losses)-patience, color='g', linestyle='--',
            label='Early Stopping Point' if len(val_losses) < epochs else None)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(range(1, len(train_accs)+1), train_accs, 'b-', label='Training Accuracy')
plt.plot(range(1, len(val_accs)+1), val_accs, 'r-', label='Validation Accuracy')
plt.axvline(x=len(val_accs)-patience, color='g', linestyle='--',
            label='Early Stopping Point' if len(val_accs) < epochs else None)
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()

plt.tight_layout()
plt.savefig('training_history.png')
plt.close()

# 9.5 Error Analysis - where the model is making mistakes
error_indices = np.where(np.array(y_pred) != np.array(y_true))[0]
if len(error_indices) > 0:
    print("\nError Analysis:")
    print(f"Total errors: {len(error_indices)} out of {len(y_true)} ({len(error_indices)/len(y_true):.2%})")

    # Show top 5 false positives and false negatives
    fps = [i for i in error_indices if y_pred[i] == 1 and y_true[i] == 0]
    fns = [i for i in error_indices if y_pred[i] == 0 and y_true[i] == 1]

    print(f"False Positives: {len(fps)} ({len(fps)/len(y_true):.2%})")
    print(f"False Negatives: {len(fns)} ({len(fns)/len(y_true):.2%})")

# 10. Explainable AI (XAI) for Model Interpretation
print("\n10. Generating Model Explanations (XAI)...")

# 10.1 Feature Importance for Engineered Features using SHAP

# Create a background dataset for SHAP (use a subset for efficiency)
X_background = X_test[:50]  # Use 100 samples as background
background_dataset = PhishingDataset(
    test_input_ids[:100],
    test_attention_masks[:100],
    X_background,
    y_test[:100]
)
background_dataloader = DataLoader(background_dataset, batch_size=32, shuffle=False)

# Function to extract engineered features from the model
def get_engineered_features_importance(model, dataloader, feature_names):
    model.eval()
    shap_values = []

    # Define an explainer function for the model's use of engineered features
    def model_predict(features):
        # Need to convert to PyTorch tensor
        features_tensor = torch.tensor(features, dtype=torch.float32).to(device)
        model.eval()
        with torch.no_grad():
            # Use a random batch of text features (not ideal but functional for demonstration)
            batch = next(iter(dataloader))
            input_ids = batch['input_ids'][:features_tensor.shape[0]].to(device)
            attention_mask = batch['attention_mask'][:features_tensor.shape[0]].to(device)

            # If batch is smaller than features, replicate to match size
            if input_ids.shape[0] < features_tensor.shape[0]:
                repeats = (features_tensor.shape[0] // input_ids.shape[0]) + 1
                input_ids = input_ids.repeat(repeats, 1)[:features_tensor.shape[0]]
                attention_mask = attention_mask.repeat(repeats, 1)[:features_tensor.shape[0]]

            outputs = model(input_ids, attention_mask, features_tensor)
            probs = F.softmax(outputs, dim=1)
            return probs[:, 1].cpu().numpy()  # Return probability of phishing class

    # Use a subset of test data for SHAP analysis
    batch = next(iter(dataloader))
    sample_features = batch['engineered_features'].cpu().numpy()[:20]  # Just 20 samples


    # Create SHAP explainer
    try:
        explainer = shap.KernelExplainer(model_predict, X_background)
        shap_values = explainer.shap_values(sample_features)  # Analyze 20 samples for efficiency

        # Create summary plot
        plt.figure(figsize=(12, 10))
        shap.summary_plot(shap_values, sample_features[:20], feature_names=feature_names, show=False)
        plt.tight_layout()
        plt.savefig('shap_feature_importance.png')
        plt.close()

        # Create bar plot of feature importance
        plt.figure(figsize=(12, 10))
        shap.summary_plot(shap_values, sample_features[:20], feature_names=feature_names, plot_type='bar', show=False)
        plt.tight_layout()
        plt.savefig('shap_feature_importance_bar.png')
        plt.close()

        print("SHAP analysis complete. Visualizations saved.")
    except Exception as e:
        print(f"Error in SHAP analysis: {e}")
        print("Skipping SHAP analysis and proceeding with other explanations.")

# Run SHAP analysis if the dataset is not too large
if X_test.shape[0] <= 5000:
    get_engineered_features_importance(model, test_dataloader, features.columns)
else:
    print("Dataset too large for SHAP analysis on this hardware. Skipping SHAP.")

# 10.2 Integrated Gradients for URL text features (BERT part)
def get_text_feature_importance(model, dataloader):
    print("\nAnalyzing text feature importance with Integrated Gradients...")
    model.eval()

    # Define a wrapper function for the model to work with Captum
    def predict_wrapper(input_ids, attention_mask, features):
        outputs = model(input_ids, attention_mask, features)
        return outputs[:, 1]  # Return score for phishing class

    # Setup Integrated Gradients
    ig = IntegratedGradients(predict_wrapper)

    # Get a batch of data
    batch = next(iter(dataloader))
    input_ids = batch['input_ids'][:5].to(device)  # Analyze 5 examples
    attention_mask = batch['attention_mask'][:5].to(device)
    features = batch['engineered_features'][:5].to(device)
    labels = batch['labels'][:5].to(device)

    try:
        # Generate attributions for input tokens
        attributions, delta = ig.attribute(
            inputs=input_ids.long(),
            additional_forward_args=(attention_mask, features),
            target=labels,
            return_convergence_delta=True
        )

        # Average attributions across all samples
        attributions_sum = attributions.sum(dim=0).abs()
        attributions_norm = attributions_sum / attributions_sum.max()

        # Convert attributions to token importance
        for i in range(min(3, input_ids.size(0))):  # Show 3 examples
            tokens = tokenizer.convert_ids_to_tokens(input_ids[i].cpu().numpy())
            url_attributions = attributions[i].sum(dim=0).abs().cpu().numpy()

            # Normalize attributions
            max_attr = url_attributions.max()
            if max_attr > 0:
                url_attributions = url_attributions / max_attr

            # Print top important tokens
            token_attr_pairs = list(zip(tokens, url_attributions))
            token_attr_pairs = [(t, a) for t, a in token_attr_pairs if t not in ['[PAD]', '[CLS]', '[SEP]']]
            token_attr_pairs.sort(key=lambda x: x[1], reverse=True)

            print(f"\nExample {i+1} - {'Phishing' if labels[i] == 1 else 'Legitimate'} URL")
            print("Top tokens by importance:")
            for token, attr in token_attr_pairs[:10]:
                print(f"{token}: {attr:.4f}")

        print("\nURL token importance analysis complete.")
    except Exception as e:
        print(f"Error in Integrated Gradients analysis: {e}")
        print("Skipping token importance analysis.")

# Run token importance analysis
get_text_feature_importance(model, test_dataloader)

# 10.3 Layer Gradient CAM for intermediate layers
def analyze_layer_importance(model, dataloader):
    print("\nAnalyzing layer importance with Gradient CAM...")
    model.eval()

    try:
        # Get a batch of data
        batch = next(iter(dataloader))
        input_ids = batch['input_ids'][:5].to(device)  # Analyze 5 examples
        attention_mask = batch['attention_mask'][:5].to(device)
        features = batch['engineered_features'][:5].to(device)
        labels = batch['labels'][:5].to(device)

        # Get model outputs and intermediate representations
        with torch.no_grad():
            outputs = model(input_ids, attention_mask, features)
            intermediate_outputs = model.get_intermediate_outputs()

        # Print importance of each component based on magnitude
        for name, output in intermediate_outputs.items():
            if output is not None:
                magnitude = output.abs().mean().item()
                print(f"{name} average magnitude: {magnitude:.4f}")

        print("\nLayer importance analysis complete.")
    except Exception as e:
        print(f"Error in layer importance analysis: {e}")
        print("Skipping layer importance analysis.")

# Run layer importance analysis
analyze_layer_importance(model, test_dataloader)

# 10.4 Attention Visualization
def visualize_attention(model, dataloader):
    print("\nVisualizing attention patterns...")
    model.eval()

    try:
        # Get a batch of data
        batch = next(iter(dataloader))
        input_ids = batch['input_ids'][:1].to(device)  # Analyze 1 example
        attention_mask = batch['attention_mask'][:1].to(device)
        features = batch['engineered_features'][:1].to(device)

        # Forward pass to get attention weights
        _ = model(input_ids, attention_mask, features)
        attention_data = model.get_attention_weights()

        if 'feature_attention' in attention_data and attention_data['feature_attention'] is not None:
            feature_attention = attention_data['feature_attention'].squeeze().cpu().numpy()

            # Plot feature attention weights
            plt.figure(figsize=(12, 6))
            plt.bar(range(len(feature_attention)), feature_attention)
            plt.title('Feature Attention Weights')
            plt.xlabel('Feature Index')
            plt.ylabel('Attention Weight')
            plt.tight_layout()
            plt.savefig('feature_attention.png')
            plt.close()

            print("Feature attention visualization saved.")

        # For BERT attention, we can visualize the first layer's attention pattern
        if 'bert_attentions' in attention_data and attention_data['bert_attentions'] is not None:
            # Get the first attention head from the last layer

            bert_attention = attention_data['bert_attentions'][-1][0, 0].detach().cpu().numpy()

            # Get tokens for visualization
            tokens = tokenizer.convert_ids_to_tokens(input_ids[0].cpu().numpy())
            tokens = [t for t in tokens if t not in ['[PAD]']]

            # Plot attention heatmap
            plt.figure(figsize=(12, 10))
            sns.heatmap(bert_attention[:len(tokens), :len(tokens)],
                        xticklabels=tokens,
                        yticklabels=tokens,
                        cmap='viridis')
            plt.title('BERT Self-Attention Pattern (Last Layer, First Head)')
            plt.tight_layout()
            plt.savefig('bert_attention.png')
            plt.close()

            print("BERT attention visualization saved.")
    except Exception as e:
        print(f"Error in attention visualization: {e}")
        print("Skipping attention visualization.")

# Run attention visualization
visualize_attention(model, test_dataloader)

# 11. Deployment and Production Readiness
print("\n11. Preparing for production deployment...")

# 11.1 Save model components
import pickle
import json
import os

# Create a directory for model artifacts
os.makedirs('model_artifacts', exist_ok=True)

# Save the StandardScaler
with open('model_artifacts/scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)

# Save the feature names
with open('model_artifacts/feature_names.json', 'w') as f:
    json.dump(list(features.columns), f)

# Save tokenizer configuration
tokenizer.save_pretrained('./model_artifacts/tokenizer/')

# Save model configuration
with open('model_artifacts/model_config.json', 'w') as f:
    model_config = {
        'bert_model_name': 'bert-base-uncased',
        'num_engineered_features': X_train.shape[1],
        'feature_selection_methods': {
            'random_forest': list(selected_features_rf),
            'mutual_info': list(selected_features_mi),
            'rfecv': list(selected_features_rfecv),
            'final_features': list(final_selected_features)
        },
        'selected_features_count': len(final_selected_features),
        'original_features_count': df.drop(['phishing', 'url'] if 'url' in df.columns else ['phishing'], axis=1).shape[1],
        'best_validation_accuracy': best_val_acc,
        'metrics': {
            'accuracy': acc,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'mcc': mcc
        }
    }
    json.dump(model_config, f, indent=2)

# Save the best model
torch.save(model.state_dict(), 'model_artifacts/best_phishing_model.pt')

# 11.2 Create a simple prediction function for production
def predict_url_phishing(url, engineered_features, model, tokenizer, scaler):
    """
    Production-ready prediction function for a single URL

    Args:
        url (str): The URL to classify
        engineered_features (dict): Dictionary of engineered features
        model: Trained model
        tokenizer: BERT tokenizer
        scaler: Feature scaler

    Returns:
        dict: Prediction results including class and probability
    """
    # Preprocess URL
    processed_url = preprocess_url(url)

    # Tokenize URL
    encoded = tokenizer.encode_plus(
        processed_url,
        add_special_tokens=True,
        max_length=128,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='pt'
    )

    input_ids = encoded['input_ids'].to(device)
    attention_mask = encoded['attention_mask'].to(device)

    # Preprocess engineered features
    feature_values = np.array([[engineered_features.get(feature, 0) for feature in final_selected_features]])
    scaled_features = scaler.transform(feature_values).astype(np.float32)
    features_tensor = torch.tensor(scaled_features, dtype=torch.float32).to(device)

    # Make prediction
    model.eval()
    with torch.no_grad():
        outputs = model(input_ids, attention_mask, features_tensor)
        probs = F.softmax(outputs, dim=1)
        _, pred = torch.max(outputs, dim=1)

    # Return results
    result = {
        'url': url,
        'is_phishing': bool(pred.item()),
        'phishing_probability': float(probs[0, 1].item()),
        'confidence': float(probs[0, pred.item()].item())
    }

    return result

# Save example of prediction function
with open('model_artifacts/prediction_example.py', 'w') as f:
    f.write("""
import torch
import torch.nn.functional as F
import numpy as np
import pickle
import json
from transformers import BertTokenizer
from model_definition import HybridBERTModel  # Import your model definition

def load_model_components(model_dir='model_artifacts'):
    '''Load all necessary components for prediction'''
    # Load tokenizer
    tokenizer = BertTokenizer.from_pretrained(f'{model_dir}/tokenizer/')

    # Load scaler
    with open(f'{model_dir}/scaler.pkl', 'rb') as f:
        scaler = pickle.load(f)

    # Load feature names
    with open(f'{model_dir}/feature_names.json', 'r') as f:
        feature_names = json.load(f)

    # Load model configuration
    with open(f'{model_dir}/model_config.json', 'r') as f:
        model_config = json.load(f)

    # Initialize model
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = HybridBERTModel(
        bert_model_name='bert-base-uncased',
        num_engineered_features=len(feature_names)
    ).to(device)

    # Load model weights
    model.load_state_dict(torch.load(f'{model_dir}/best_phishing_model.pt',
                                     map_location=device))
    model.eval()

    return model, tokenizer, scaler, feature_names, device

def preprocess_url(url):
    '''Preprocess URL for tokenization'''
    # Replace special characters with spaces around them
    for char in ['/', '.', '-', '=', '?', '&', '_', ':', '@']:
        url = url.replace(char, f' {char} ')
    # Additional preprocessing
    url = url.replace('http', 'http ')
    url = url.replace('https', 'https ')
    url = url.replace('www', 'www ')
    return url

def predict_url(url, engineered_features, model, tokenizer, scaler, feature_names, device):
    '''Predict whether a URL is phishing or legitimate'''
    # Preprocess URL
    processed_url = preprocess_url(url)

    # Tokenize URL
    encoded = tokenizer.encode_plus(
        processed_url,
        add_special_tokens=True,
        max_length=128,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='pt'
    )

    input_ids = encoded['input_ids'].to(device)
    attention_mask = encoded['attention_mask'].to(device)

    # Preprocess engineered features
    feature_values = np.array([[engineered_features.get(feature, 0) for feature in feature_names]])
    scaled_features = scaler.transform(feature_values)
    features_tensor = torch.tensor(scaled_features, dtype=torch.float32).to(device)

    # Make prediction
    with torch.no_grad():
        outputs = model(input_ids, attention_mask, features_tensor)
        probs = F.softmax(outputs, dim=1)
        _, pred = torch.max(outputs, dim=1)

    # Return results
    result = {
        'url': url,
        'is_phishing': bool(pred.item()),
        'phishing_probability': float(probs[0, 1].item()),
        'confidence': float(probs[0, pred.item()].item())
    }

    return result

# Example usage
if __name__ == '__main__':
    # Load model components
    model, tokenizer, scaler, feature_names, device = load_model_components()

    # Example URL and features
    test_url = "http://suspicious-looking-bank.com/login.php"

    # These would come from your feature extraction logic
    test_features = {
        'length_url': 45,
        'length_hostname': 26,
        'ip': 0,
        'nb_dots': 1,
        'nb_hyphens': 2,
        'nb_at': 0,
        'nb_qm': 0,
        'nb_and': 0,
        'nb_or': 0,
        'nb_eq': 0,
        'nb_underscore': 0,
        'nb_tilde': 0,
        'nb_percent': 0,
        'nb_slash': 1,
        'nb_star': 0,
        'nb_colon': 1,
        'nb_comma': 0,
        'nb_semicolumn': 0,
        'nb_dollar': 0,
        'nb_space': 0,
        'nb_www': 0,
        'nb_com': 1,
        'nb_dslash': 1,
        # Add other features as needed
    }

    # Make prediction
    result = predict_url(test_url, test_features, model, tokenizer, scaler, feature_names, device)
    print(result)
""")

print("\nProduction-ready model artifacts saved in 'model_artifacts' directory.")

# 12. Final Summary
print("\n12. Final Summary")
print("=" * 50)
print(f"Dataset size: {df.shape[0]} samples with {df.shape[1]-2} original features")
print(f"Feature selection reduced feature count from {len(features)+2} to {len(final_selected_features)}")
print(f"Best model accuracy: {best_val_acc:.4f}")
print(f"Final test accuracy: {acc:.4f}")
print(f"F1 score: {f1:.4f}")
print(f"MCC: {mcc:.4f}")
print("=" * 50)
print("\nInnovations implemented:")
print("1. Comprehensive feature selection using multiple methods")
print("2. Advanced hybrid architecture combining BERT with engineered features")
print("3. Feature attention mechanism for better feature importance understanding")
print("4. Explainable AI (XAI) techniques for model interpretation")
print("5. Detailed error analysis and visualization")
print("6. Production-ready deployment artifacts")
print("=" * 50)

print("\nPhishing detection model training and evaluation completed successfully.")