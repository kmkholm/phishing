# -*- coding: utf-8 -*-
"""Welcome To Colab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/notebooks/intro.ipynb
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

from transformers import AutoModel, AutoTokenizer

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import io
import base64
import random
import os
import traceback
import re
import urllib.parse
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix
from tqdm import tqdm

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Part 1: URL Meta Dataset for Few-Shot Learning
class URLMetaDataset:
    """
    Meta-dataset for URL phishing detection using few-shot learning.
    Supports both real datasets and online phishing URL sources.
    """
    def __init__(self, data_path=None, url_source=None, n_way=2, k_shot=5, query_size=10):
        """
        Initialize the URL meta-dataset

        Args:
            data_path: Path to CSV dataset (optional)
            url_source: URL for online phishing source (optional)
            n_way: Number of classes (default: 2 - phishing/legitimate)
            k_shot: Number of examples per class (default: 5)
            query_size: Number of query examples per class (default: 10)
        """
        self.n_way = n_way
        self.k_shot = k_shot
        self.query_size = query_size

        # Initialize indices
        self.phishing_indices = []
        self.legitimate_indices = []

        # Initialize URL lists
        self.phishing_urls = []
        self.legitimate_urls = []

        # Load dataset if provided
        if data_path and os.path.exists(data_path):
            self.load_dataset(data_path)
        else:
            print("No dataset provided or file not found. Using default features.")
            self.df = None

        # Load online phishing URLs if provided
        if url_source:
            self.load_phishing_urls(url_source)

        # Generate legitimate URLs if needed
        if not self.legitimate_urls and not self.legitimate_indices:
            self.generate_legitimate_urls(100)

        print(f"Initialized URL meta-dataset with:")
        if self.df is not None:
            print(f"- {len(self.phishing_indices)} phishing examples in dataset")
            print(f"- {len(self.legitimate_indices)} legitimate examples in dataset")
        print(f"- {len(self.phishing_urls)} online phishing URLs")
        print(f"- {len(self.legitimate_urls)} legitimate URLs")

    def load_dataset(self, data_path):
        """Load dataset from CSV file"""
        print(f"Loading dataset from {data_path}")
        try:
            self.df = pd.read_csv(data_path)

            # Identify phishing column
            phishing_col = None
            for col in ['phishing', 'is_phishing', 'label', 'class']:
                if col in self.df.columns:
                    phishing_col = col
                    break

            if not phishing_col:
                # If no obvious column, assume binary classification with labels 0/1
                phishing_col = self.df.columns[-1]
                print(f"No obvious phishing column found, using {phishing_col}")

            # Get indices of phishing and legitimate examples
            self.phishing_indices = list(self.df[self.df[phishing_col] == 1].index)
            self.legitimate_indices = list(self.df[self.df[phishing_col] == 0].index)

            print(f"Dataset loaded with {len(self.phishing_indices)} phishing and {len(self.legitimate_indices)} legitimate examples")

            # Extract URLs if available
            if 'url' in self.df.columns:
                # Add to URL lists (but limit to prevent duplication)
                self.phishing_urls.extend(self.df.loc[self.phishing_indices[:100], 'url'].tolist())
                self.legitimate_urls.extend(self.df.loc[self.legitimate_indices[:100], 'url'].tolist())

        except Exception as e:
            print(f"Error loading dataset: {e}")
            self.df = None

    def load_phishing_urls(self, url_source):
        """Load phishing URLs from online source"""
        print(f"Loading phishing URLs from {url_source}")
        try:
            # Simplified for demonstration - in production, use proper API or file download
            import requests
            response = requests.get(url_source)
            if response.status_code == 200:
                # Extract URLs from response
                urls = response.text.splitlines()
                # Filter valid URLs
                valid_urls = [url for url in urls if url.startswith('http')]
                # Take a subset for efficiency
                self.phishing_urls.extend(valid_urls[:1000])
                print(f"Loaded {len(self.phishing_urls)} phishing URLs")
            else:
                print(f"Failed to load phishing URLs: HTTP {response.status_code}")
        except Exception as e:
            print(f"Error loading phishing URLs: {e}")
            # Use fallback phishing URLs
            self.phishing_urls = [
                "http://phishingsite.example.com/login",
                "https://bank-secure-login.example.net/verify",
                "http://paypal-account-verify.example.org/secure",
                "https://login-account-verify.example.com/secure",
                "http://secure-banklogin.example.net/verify"
            ]
            print(f"Using {len(self.phishing_urls)} fallback phishing URLs")

    def generate_legitimate_urls(self, num_urls=100):
        """Generate legitimate URLs for testing"""
        print(f"Generating {num_urls} legitimate URLs")
        legitimate_domains = [
            "google.com", "amazon.com", "microsoft.com", "apple.com", "facebook.com",
            "twitter.com", "linkedin.com", "github.com", "youtube.com", "wikipedia.org",
            "reddit.com", "netflix.com", "spotify.com", "instagram.com", "pinterest.com"
        ]

        paths = [
            "", "index.html", "about", "products", "services", "contact", "login",
            "signup", "faq", "support", "news", "blog", "forum", "shop", "pricing"
        ]

        # Generate URLs
        urls = []
        for i in range(num_urls):
            domain = random.choice(legitimate_domains)
            protocol = random.choice(["http", "https"])
            path = random.choice(paths)
            url = f"{protocol}://{domain}/{path}"
            urls.append(url)

        self.legitimate_urls = urls
        print(f"Generated {len(self.legitimate_urls)} legitimate URLs")

    def extract_features(self, index_or_url):
        """
        Extract features from URL

        Args:
            index_or_url: Either an index into the dataset or a URL string

        Returns:
            Dictionary of features
        """
        # Check if input is index or URL
        if isinstance(index_or_url, (int, np.integer)):
            # Extract from dataset
            if self.df is not None and 0 <= index_or_url < len(self.df):
                # Check if we have pre-computed features in the dataset
                feature_cols = [col for col in self.df.columns
                                if col not in ['url', 'phishing', 'is_phishing', 'label', 'class']]

                if feature_cols:
                    # Use pre-computed features
                    features = {col: self.df.loc[index_or_url, col] for col in feature_cols}
                    return features
                elif 'url' in self.df.columns:
                    # Extract features from URL
                    url = self.df.loc[index_or_url, 'url']
                    return self._extract_url_features(url)
                else:
                    # No URL available, generate synthetic features
                    return self._generate_synthetic_features(index_or_url)
            else:
                # Invalid index, generate synthetic features
                return self._generate_synthetic_features(index_or_url)
        else:
            # Extract features from URL string
            return self._extract_url_features(index_or_url)

    def _extract_url_features(self, url):
        """Extract features from URL string"""
        features = {}

        # Parse URL
        try:
            parsed = urllib.parse.urlparse(url)

            # Basic length features
            features['url_length'] = len(url)
            features['domain_length'] = len(parsed.netloc)
            features['path_length'] = len(parsed.path)
            features['query_length'] = len(parsed.query)

            # Domain features
            domain = parsed.netloc
            features['num_dots'] = domain.count('.')
            features['num_hyphens'] = domain.count('-')
            features['num_underscores'] = domain.count('_')
            features['num_digits'] = sum(c.isdigit() for c in domain)
            features['num_params'] = len(parsed.query.split('&')) if parsed.query else 0

            # Special terms in URL
            suspicious_terms = ['login', 'signin', 'verify', 'secure', 'account', 'banking',
                                'confirm', 'update', 'password', 'wallet', 'alert']
            for term in suspicious_terms:
                features[f'contains_{term}'] = int(term in url.lower())

            # Protocol features
            features['is_https'] = int(parsed.scheme == 'https')

            # Path features
            features['path_depth'] = parsed.path.count('/')
            features['has_file_extension'] = int('.' in parsed.path.split('/')[-1])

            # Domain specific
            features['domain_age_days'] = random.randint(1, 3650)  # Synthetic
            features['is_ip_address'] = int(bool(re.match(r'^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$', domain)))
            features['has_port'] = int(bool(re.search(r':[0-9]+$', domain)))

            # Entropy and other statistical features
            features['url_entropy'] = self._calculate_entropy(url)
            features['domain_entropy'] = self._calculate_entropy(domain)

            # Add some randomness to simulate real features
            features['domain_reputation'] = random.uniform(0, 1)
            features['ssl_cert_age'] = random.randint(0, 730) if features['is_https'] else 0

        except Exception as e:
            print(f"Error extracting features from URL {url}: {e}")
            # Fallback to synthetic features
            features = self._generate_synthetic_features()

        return features

    def _calculate_entropy(self, text):
        """Calculate Shannon entropy of text"""
        if not text:
            return 0

        prob = [float(text.count(c)) / len(text) for c in set(text)]
        entropy = -sum([p * np.log2(p) for p in prob])
        return entropy

    def _generate_synthetic_features(self, seed=None):
        """Generate synthetic features for testing"""
        if seed is not None:
            random.seed(seed)

        # Create basic features similar to extracted ones
        features = {
            'url_length': random.randint(20, 200),
            'domain_length': random.randint(10, 50),
            'path_length': random.randint(0, 100),
            'query_length': random.randint(0, 80),
            'num_dots': random.randint(1, 5),
            'num_hyphens': random.randint(0, 3),
            'num_underscores': random.randint(0, 2),
            'num_digits': random.randint(0, 8),
            'num_params': random.randint(0, 5),
            'is_https': random.randint(0, 1),
            'path_depth': random.randint(0, 5),
            'has_file_extension': random.randint(0, 1),
            'domain_age_days': random.randint(1, 3650),
            'is_ip_address': random.randint(0, 1),
            'has_port': random.randint(0, 1),
            'url_entropy': random.uniform(2.0, 5.0),
            'domain_entropy': random.uniform(1.5, 4.0),
            'domain_reputation': random.uniform(0, 1),
            'ssl_cert_age': random.randint(0, 730)
        }

        # Add suspicious terms
        suspicious_terms = ['login', 'signin', 'verify', 'secure', 'account', 'banking',
                            'confirm', 'update', 'password', 'wallet', 'alert']
        for term in suspicious_terms:
            features[f'contains_{term}'] = random.randint(0, 1)

        return features

    def analyze_url_components(self, url):
        """
        Analyze URL components for XAI

        Returns:
            components: Dict of URL components
            suspiciousness: Dict of suspiciousness scores for components
        """
        components = {}
        suspiciousness = {}

        try:
            parsed = urllib.parse.urlparse(url)

            # Extract components
            components['protocol'] = parsed.scheme
            components['domain'] = parsed.netloc
            components['path'] = parsed.path
            components['query'] = parsed.query
            components['fragment'] = parsed.fragment

            # Analyze domain
            domain = parsed.netloc
            suspiciousness['domain'] = 0.0

            # Check for suspicious domain characteristics
            if domain.count('.') > 2:
                suspiciousness['domain'] += 0.3
            if domain.count('-') > 1:
                suspiciousness['domain'] += 0.2
            if domain.count('_') > 0:
                suspiciousness['domain'] += 0.4
            if sum(c.isdigit() for c in domain) > 3:
                suspiciousness['domain'] += 0.3
            if re.match(r'^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$', domain):
                suspiciousness['domain'] += 0.8

            # Check for suspicious keywords in domain
            suspicious_domain_terms = ['secure', 'banking', 'login', 'signin', 'verify']
            for term in suspicious_domain_terms:
                if term in domain.lower():
                    suspiciousness['domain'] += 0.4

            # Analyze protocol
            suspiciousness['protocol'] = 0.0
            if parsed.scheme != 'https':
                suspiciousness['protocol'] += 0.7

            # Analyze path
            path = parsed.path
            suspiciousness['path'] = 0.0

            # Check for suspicious path characteristics
            if path.count('/') > 3:
                suspiciousness['path'] += 0.2

            # Check for suspicious keywords in path
            suspicious_path_terms = ['login', 'signin', 'verify', 'secure', 'account', 'update']
            for term in suspicious_path_terms:
                if term in path.lower():
                    suspiciousness['path'] += 0.3

            # Analyze query
            query = parsed.query
            suspiciousness['query'] = 0.0

            # Check for suspicious query characteristics
            if query.count('&') > 3:
                suspiciousness['query'] += 0.1

            # Check for suspicious keywords in query
            suspicious_query_terms = ['password', 'user', 'token', 'auth', 'redirect']
            for term in suspicious_query_terms:
                if term in query.lower():
                    suspiciousness['query'] += 0.3

        except Exception as e:
            print(f"Error analyzing URL components: {e}")
            # Create default components
            components = {
                'protocol': 'unknown',
                'domain': url,
                'path': '',
                'query': '',
                'fragment': ''
            }
            suspiciousness = {
                'protocol': 0.5,
                'domain': 0.5,
                'path': 0.0,
                'query': 0.0
            }

        return components, suspiciousness

    def get_feature_descriptions(self):
        """Get descriptions for URL features"""
        descriptions = {
            'url_length': 'Total length of the URL in characters',
            'domain_length': 'Length of the domain name',
            'path_length': 'Length of the path component',
            'query_length': 'Length of the query component',
            'num_dots': 'Number of dots in the domain name',
            'num_hyphens': 'Number of hyphens in the domain name',
            'num_underscores': 'Number of underscores in the domain name',
            'num_digits': 'Number of digits in the domain name',
            'num_params': 'Number of parameters in the query string',
            'is_https': 'Whether the URL uses HTTPS protocol (1) or not (0)',
            'path_depth': 'Number of directory levels in the path',
            'has_file_extension': 'Whether the URL path ends with a file extension',
            'domain_age_days': 'Age of the domain in days',
            'is_ip_address': 'Whether the domain is an IP address (1) or not (0)',
            'has_port': 'Whether the URL includes a specific port number',
            'url_entropy': 'Shannon entropy of the entire URL (higher values indicate more randomness)',
            'domain_entropy': 'Shannon entropy of the domain name',
            'domain_reputation': 'Reputation score of the domain (0-1, higher is better)',
            'ssl_cert_age': 'Age of the SSL certificate in days (0 if no HTTPS)'
        }

        # Add descriptions for suspicious terms
        suspicious_terms = ['login', 'signin', 'verify', 'secure', 'account', 'banking',
                            'confirm', 'update', 'password', 'wallet', 'alert']
        for term in suspicious_terms:
            descriptions[f'contains_{term}'] = f'Whether the URL contains the term "{term}"'

        return descriptions

    def sample_episode(self, use_dataset=True):
        """
        Sample a few-shot episode with support and query sets

        Args:
            use_dataset: Whether to use the dataset (if available) or URL lists

        Returns:
            support_set: List of support examples
            query_set: List of query examples
            task_info: Info about the task
        """
        support_set = []
        query_set = []

        # Decide whether to use dataset or URL lists
        use_dataset = use_dataset and self.df is not None and \
                      len(self.phishing_indices) >= self.k_shot and \
                      len(self.legitimate_indices) >= self.k_shot

        if use_dataset:
            # Sample from dataset
            # Phishing examples for support set
            phishing_support_indices = random.sample(self.phishing_indices, self.k_shot)
            for idx in phishing_support_indices:
                features = self.extract_features(idx)
                # Get URL if available, otherwise use placeholder
                url = self.df.loc[idx, 'url'] if 'url' in self.df.columns else f"phishing-{idx}"
                support_set.append({
                    'url': url,
                    'label': 1,  # Phishing
                    'features': features,
                    'idx': idx
                })

            # Legitimate examples for support set
            legitimate_support_indices = random.sample(self.legitimate_indices, self.k_shot)
            for idx in legitimate_support_indices:
                features = self.extract_features(idx)
                # Get URL if available, otherwise use placeholder
                url = self.df.loc[idx, 'url'] if 'url' in self.df.columns else f"legitimate-{idx}"
                support_set.append({
                    'url': url,
                    'label': 0,  # Legitimate
                    'features': features,
                    'idx': idx
                })

            # Phishing examples for query set
            remaining_phishing = [idx for idx in self.phishing_indices
                                if idx not in phishing_support_indices]
            phishing_query_indices = random.sample(
                remaining_phishing,
                min(self.query_size, len(remaining_phishing))
            )
            for idx in phishing_query_indices:
                features = self.extract_features(idx)
                # Get URL if available, otherwise use placeholder
                url = self.df.loc[idx, 'url'] if 'url' in self.df.columns else f"phishing-{idx}"
                query_set.append({
                    'url': url,
                    'label': 1,  # Phishing
                    'features': features,
                    'idx': idx
                })

            # Legitimate examples for query set
            remaining_legitimate = [idx for idx in self.legitimate_indices
                                  if idx not in legitimate_support_indices]
            legitimate_query_indices = random.sample(
                remaining_legitimate,
                min(self.query_size, len(remaining_legitimate))
            )
            for idx in legitimate_query_indices:
                features = self.extract_features(idx)
                # Get URL if available, otherwise use placeholder
                url = self.df.loc[idx, 'url'] if 'url' in self.df.columns else f"legitimate-{idx}"
                query_set.append({
                    'url': url,
                    'label': 0,  # Legitimate
                    'features': features,
                    'idx': idx
                })

            task_info = {
                'source': 'dataset',
                'support_indices': phishing_support_indices + legitimate_support_indices,
                'query_indices': phishing_query_indices + legitimate_query_indices
            }

        else:
            # Sample from URL lists
            # Ensure we have enough URLs
            if len(self.phishing_urls) < self.k_shot:
                self.phishing_urls.extend([f"phishing-{i}" for i in range(self.k_shot)])
            if len(self.legitimate_urls) < self.k_shot:
                self.legitimate_urls.extend([f"legitimate-{i}" for i in range(self.k_shot)])

            # Phishing examples for support set
            phishing_support_urls = random.sample(self.phishing_urls, self.k_shot)
            for url in phishing_support_urls:
                features = self.extract_features(url)
                support_set.append({
                    'url': url,
                    'label': 1,  # Phishing
                    'features': features
                })

            # Legitimate examples for support set
            legitimate_support_urls = random.sample(self.legitimate_urls, self.k_shot)
            for url in legitimate_support_urls:
                features = self.extract_features(url)
                support_set.append({
                    'url': url,
                    'label': 0,  # Legitimate
                    'features': features
                })

            # Phishing examples for query set
            remaining_phishing = [url for url in self.phishing_urls
                                if url not in phishing_support_urls]
            if len(remaining_phishing) < self.query_size:
                # Generate some synthetic URLs if needed
                remaining_phishing.extend([f"phishing-extra-{i}" for i in range(self.query_size)])

            phishing_query_urls = random.sample(remaining_phishing, self.query_size)
            for url in phishing_query_urls:
                features = self.extract_features(url)
                query_set.append({
                    'url': url,
                    'label': 1,  # Phishing
                    'features': features
                })

            # Legitimate examples for query set
            remaining_legitimate = [url for url in self.legitimate_urls
                                  if url not in legitimate_support_urls]
            if len(remaining_legitimate) < self.query_size:
                # Generate some synthetic URLs if needed
                remaining_legitimate.extend([f"legitimate-extra-{i}" for i in range(self.query_size)])

            legitimate_query_urls = random.sample(remaining_legitimate, self.query_size)
            for url in legitimate_query_urls:
                features = self.extract_features(url)
                query_set.append({
                    'url': url,
                    'label': 0,  # Legitimate
                    'features': features
                })

            task_info = {
                'source': 'url_lists',
                'support_urls': phishing_support_urls + legitimate_support_urls,
                'query_urls': phishing_query_urls + legitimate_query_urls
            }

        return support_set, query_set, task_info

# Part 2: URL Dataset for Tokenization and Feature Extraction
class URLDataset(Dataset):
    """Dataset for processing URL data with tokenization and feature extraction"""
    def __init__(self, data, tokenizer, max_length=128):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        url = item['url']
        label = item['label']
        features = item['features']

        # Preprocess and tokenize URL
        processed_url = self._preprocess_url(url)
        encoded = self.tokenizer.encode_plus(
            processed_url,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )

        # Convert features dictionary to tensor
        feature_names = sorted(features.keys())
        feature_values = [features[name] for name in feature_names]
        feature_tensor = torch.tensor(feature_values, dtype=torch.float32)

        return {
            'input_ids': encoded['input_ids'].squeeze(),
            'attention_mask': encoded['attention_mask'].squeeze(),
            'features': feature_tensor,
            'label': torch.tensor(label, dtype=torch.long),
            'url': url,
            'original_idx': item.get('idx', -1)  # Store original index if available
        }

    def _preprocess_url(self, url):
        """Preprocess URL for tokenization"""
        for char in ['/', '.', '-', '=', '?', '&', '_', ':', '@']:
            url = url.replace(char, f' {char} ')
        url = url.replace('http', 'http ')
        url = url.replace('https', 'https ')
        url = url.replace('www', 'www ')
        return url

# Part 3: ModernBERT Prototypical Network for Few-Shot Learning with XAI support
class ModernBERTPrototypicalNetwork(nn.Module):
    """
    Prototypical Network for few-shot learning of URLs using ModernBERT.
    Embeds URLs and compares distances to class prototypes.
    Includes XAI hooks for interpretability.
    """
    def __init__(self, model_id='answerdotai/ModernBERT-base', feature_dim=111):
        super(ModernBERTPrototypicalNetwork, self).__init__()

        # URL encoder using ModernBERT
        self.encoder = AutoModel.from_pretrained(model_id)
        self.encoder_hidden_size = self.encoder.config.hidden_size

        # Feature encoder for engineered URL features
        self.feature_encoder = nn.Sequential(
            nn.Linear(feature_dim, 128),
            nn.LayerNorm(128),
            nn.ReLU(),
            nn.Linear(128, 256),
            nn.LayerNorm(256),
            nn.ReLU()
        )

        # Projection for combining transformer and feature encodings
        self.projection = nn.Sequential(
            nn.Linear(self.encoder_hidden_size + 256, 512),
            nn.LayerNorm(512),
            nn.ReLU(),
            nn.Linear(512, 256)
        )

        # Distance function (negative Euclidean distance)
        self.distance_function = lambda x, y: -torch.sum(torch.pow(x.unsqueeze(1) - y, 2), dim=2)

        # For attention visualization (XAI)
        self.save_attention = False
        self.attention_weights = None
        self.token_embeddings = None
        self.tokens = None

    def encode(self, input_ids, attention_mask, features, save_attention=False):
        """Encode URLs into an embedding space"""
        # Record tokens for attention visualization if needed
        if save_attention:
            self.tokens = input_ids

        # Encode URL text with ModernBERT
        outputs = self.encoder(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_attentions=save_attention
        )

        url_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] token

        # Save attention weights and token embeddings for visualization
        if save_attention:
            self.attention_weights = outputs.attentions
            self.token_embeddings = outputs.last_hidden_state

        # Encode engineered features
        feature_embedding = self.feature_encoder(features)

        # Combine embeddings
        combined = torch.cat([url_embedding, feature_embedding], dim=1)
        embedding = self.projection(combined)

        # Normalize embedding (important for distance calculations)
        embedding = F.normalize(embedding, p=2, dim=1)

        return embedding

    def forward(self, support_input_ids, support_attention_mask, support_features,
                support_labels, query_input_ids, query_attention_mask, query_features):
        """
        Forward pass for prototypical networks:
        1. Encode support and query examples
        2. Compute class prototypes from support set
        3. Calculate distances between query examples and class prototypes
        """
        # Encode support examples
        support_embeddings = self.encode(
            support_input_ids, support_attention_mask, support_features,
            save_attention=self.save_attention
        )

        # Encode query examples
        query_embeddings = self.encode(
            query_input_ids, query_attention_mask, query_features,
            save_attention=self.save_attention
        )

        # Get unique classes
        classes = torch.unique(support_labels)
        n_classes = len(classes)

        # Compute class prototypes
        prototypes = torch.zeros(n_classes, support_embeddings.size(1)).to(support_embeddings.device)
        for i, c in enumerate(classes):
            mask = support_labels == c
            if mask.sum() > 0:  # Ensure there's at least one example
                prototypes[i] = support_embeddings[mask].mean(0)

        # Calculate distances between query examples and prototypes
        logits = self.distance_function(query_embeddings, prototypes)

        return logits

    def get_prototypes(self, support_input_ids, support_attention_mask, support_features, support_labels):
        """Get prototypes for each class - used in XAI"""
        # Encode support examples
        support_embeddings = self.encode(
            support_input_ids, support_attention_mask, support_features
        )

        # Get unique classes
        classes = torch.unique(support_labels)
        n_classes = len(classes)

        # Compute class prototypes
        prototypes = {}
        for i, c in enumerate(classes):
            mask = support_labels == c
            if mask.sum() > 0:  # Ensure there's at least one example
                prototypes[c.item()] = support_embeddings[mask].mean(0)

        return prototypes

    def get_attention_visualization(self, tokenizer, input_ids, layer_idx=-1):
        """
        Generate attention visualization for a single example
        Returns: HTML with attention heatmap
        """
        if self.attention_weights is None:
            return "No attention weights available. Run with save_attention=True first."

        # Get attention from the specified layer (default: last layer)
        layer = layer_idx if layer_idx >= 0 else len(self.attention_weights) - 1
        if layer >= len(self.attention_weights):
            return f"Layer {layer} not available. Max layer is {len(self.attention_weights) - 1}"

        # Get attention weights for this layer
        attn = self.attention_weights[layer].mean(1)[0]  # Average across heads

        # Decode tokens
        tokens = tokenizer.convert_ids_to_tokens(input_ids[0])

        # Create attention visualization
        fig, ax = plt.subplots(figsize=(10, 10))
        im = ax.imshow(attn.cpu().detach().numpy(), cmap='viridis')

        # Add colorbar
        cbar = ax.figure.colorbar(im, ax=ax)
        cbar.ax.set_ylabel("Attention weight", rotation=-90, va="bottom")

        # Set ticks and labels
        ax.set_xticks(np.arange(len(tokens)))
        ax.set_yticks(np.arange(len(tokens)))
        ax.set_xticklabels(tokens, rotation=90)
        ax.set_yticklabels(tokens)

        # Turn off ticks and labels if too many tokens
        if len(tokens) > 30:
            ax.set_xticks([])
            ax.set_yticks([])

        ax.set_title("Attention Heatmap")
        fig.tight_layout()

        # Convert plot to base64 image
        buf = io.BytesIO()
        plt.savefig(buf, format='png')
        buf.seek(0)
        plt.close(fig)

        img_str = base64.b64encode(buf.read()).decode('utf-8')

        # Return as HTML img tag
        return f'<img src="data:image/png;base64,{img_str}" alt="Attention visualization" />'

    def get_token_importance(self, tokenizer, input_ids, input_embeddings=None):
        """
        Calculate token importance based on their contribution to the final embedding
        Returns: dictionary mapping tokens to importance scores
        """
        if input_embeddings is None and self.token_embeddings is None:
            return "No token embeddings available. Run with save_attention=True first."

        embeddings = input_embeddings if input_embeddings is not None else self.token_embeddings

        # Use the first example
        single_embedding = embeddings[0]  # [seq_len, hidden_size]
        cls_embedding = single_embedding[0]  # [hidden_size]

        # Calculate cosine similarity between CLS and each token
        similarities = []
        for token_idx in range(single_embedding.size(0)):
            token_embedding = single_embedding[token_idx]
            sim = F.cosine_similarity(cls_embedding.unsqueeze(0), token_embedding.unsqueeze(0)).item()
            similarities.append(sim)

        # Convert to importance scores (normalize)
        min_sim, max_sim = min(similarities), max(similarities)
        importances = [(s - min_sim) / (max_sim - min_sim) for s in similarities]

        # Map to tokens
        tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
        token_importance = {token: importance for token, importance in zip(tokens, importances)}

        return token_importance

# Part 4: ModernBERT MAML-inspired Phishing Detector with XAI
class ModernBERTMAMLPhishingDetector(nn.Module):
    """
    Model-Agnostic Meta-Learning for URL phishing detection using ModernBERT.
    Implements the MAML algorithm for quick adaptation to new phishing patterns.
    Includes XAI hooks for interpretability.
    """
    def __init__(self, model_id='answerdotai/ModernBERT-base', feature_dim=111):
        super(ModernBERTMAMLPhishingDetector, self).__init__()

        # Base encoder (shared)
        self.encoder = AutoModel.from_pretrained(model_id)
        self.encoder_hidden_size = self.encoder.config.hidden_size

        # Feature encoder
        self.feature_encoder = nn.Sequential(
            nn.Linear(feature_dim, 128),
            nn.LayerNorm(128),
            nn.ReLU(),
            nn.Linear(128, 256),
            nn.LayerNorm(256)
        )

        # Combined representation
        self.representation = nn.Sequential(
            nn.Linear(self.encoder_hidden_size + 256, 512),
            nn.LayerNorm(512),
            nn.ReLU(),
            nn.Dropout(0.2)
        )

        # Classification head (this will be adapted in inner loop)
        self.classifier = nn.Linear(512, 2)  # 2 classes: legitimate/phishing

        # For XAI
        self.save_attention = False
        self.attention_weights = None
        self.feature_importances = None
        self.representation_activations = None

    def forward(self, input_ids, attention_mask, features, save_activations=False):
        """Forward pass through the network with optional activation recording for XAI"""
        # Encode URL text with ModernBERT
        outputs = self.encoder(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_attentions=self.save_attention
        )

        url_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] token

        # Save attention weights if needed
        if self.save_attention:
            self.attention_weights = outputs.attentions

        # Encode engineered features
        feature_embedding = self.feature_encoder(features)

        # Store feature importances
        if save_activations:
            # Store feature activations by tracking gradient flow
            self.feature_importances = {}
            for i in range(features.size(1)):
                # Create a copy of features with the i-th feature set to 0
                modified_features = features.clone()
                original_value = modified_features[0, i].item()
                modified_features[0, i] = 0

                # Get the modified embedding
                modified_feature_embedding = self.feature_encoder(modified_features)

                # Calculate difference in embedding norm
                diff = torch.norm(feature_embedding - modified_feature_embedding).item()
                self.feature_importances[i] = diff

        # Combine embeddings
        combined = torch.cat([url_embedding, feature_embedding], dim=1)
        representation = self.representation(combined)

        # Store representation for interpretation
        if save_activations:
            self.representation_activations = representation.detach()

        # Classification
        logits = self.classifier(representation)

        return logits

    def clone_model(self):
        """
        Create a clone of the model for inner loop updates
        Modified to better preserve gradient flow for MAML
        """
        clone = ModernBERTMAMLPhishingDetector(
            model_id=self.encoder.config._name_or_path,
            feature_dim=next(self.feature_encoder.parameters()).size(1)
        )

        # Copy parameters with their gradients
        for param, clone_param in zip(self.parameters(), clone.parameters()):
            clone_param.data.copy_(param.data)
            # Important: Since we're updating the clone in the inner loop,
            # we need the gradients to flow back to the original model
            if param.requires_grad:
                clone_param.requires_grad_(True)

        return clone

    def adapt(self, input_ids, attention_mask, features, labels,
              num_adaptation_steps=3, step_size=0.1):
        """
        Adapt the model to a new task using the support set (inner loop of MAML)
        Modified to avoid the donated buffer error
        """
        # Clone model for adaptation - but maintain parameter linkage
        adapted_model = self.clone_model()
        adapted_model.to(input_ids.device)

        # Set to training mode
        adapted_model.train()

        # Create optimizer for adaptation
        optimizer = torch.optim.SGD(adapted_model.parameters(), lr=step_size)

        # Inner loop adaptation
        for _ in range(num_adaptation_steps):
            # Forward pass
            logits = adapted_model(input_ids, attention_mask, features)
            loss = F.cross_entropy(logits, labels)

            # Backward pass
            optimizer.zero_grad()
            # Modified: remove retain_graph=True to avoid compatibility issues
            loss.backward()
            optimizer.step()

        # Return the adapted model
        return adapted_model

    def generate_counterfactuals(self, input_ids, attention_mask, features, tokenizer, num_features=5, feature_names=None):
        """
        Generate counterfactual examples by tweaking features
        Returns: list of counterfactuals with their predicted class
        """
        self.eval()
        counterfactuals = []

        # Get original prediction
        with torch.no_grad():
            logits = self(input_ids, attention_mask, features)
            probs = F.softmax(logits, dim=1)
            orig_pred = torch.argmax(probs, dim=1).item()
            orig_prob = probs[0, orig_pred].item()

        # Try modifying each feature to flip the prediction
        for i in range(features.size(1)):
            # Create a modified feature vector
            modified_features = features.clone()

            # Try two modifications:
            # 1. Set to zero
            modified_features[0, i] = 0

            # Check prediction
            with torch.no_grad():
                logits = self(input_ids, attention_mask, modified_features)
                probs = F.softmax(logits, dim=1)
                new_pred = torch.argmax(probs, dim=1).item()
                new_prob = probs[0, new_pred].item()

            # If prediction flipped or probability changed significantly
            if new_pred != orig_pred or abs(new_prob - orig_prob) > 0.2:
                feature_info = {
                    'feature_idx': i,
                    'modification': 'set to 0',
                    'original_value': features[0, i].item(),
                    'new_value': 0.0,
                    'original_class': orig_pred,
                    'new_class': new_pred,
                    'probability_change': new_prob - orig_prob
                }

                # Add feature name if available
                if feature_names and i < len(feature_names):
                    feature_info['feature_name'] = feature_names[i]

                counterfactuals.append(feature_info)

            # 2. Invert (if binary) or set to high value (if continuous)
            if features[0, i].item() <= 1.0:  # Likely binary
                modified_features[0, i] = 1.0 - features[0, i].item()
            else:  # Continuous
                modified_features[0, i] = features[0, i].item() * 1.5  # Increase by 50%

            # Check prediction
            with torch.no_grad():
                logits = self(input_ids, attention_mask, modified_features)
                probs = F.softmax(logits, dim=1)
                new_pred = torch.argmax(probs, dim=1).item()
                new_prob = probs[0, new_pred].item()

            # If prediction flipped or probability changed significantly
            if new_pred != orig_pred or abs(new_prob - orig_prob) > 0.2:
                feature_info = {
                    'feature_idx': i,
                    'modification': 'inverted/increased',
                    'original_value': features[0, i].item(),
                    'new_value': modified_features[0, i].item(),
                    'original_class': orig_pred,
                    'new_class': new_pred,
                    'probability_change': new_prob - orig_prob
                }

                # Add feature name if available
                if feature_names and i < len(feature_names):
                    feature_info['feature_name'] = feature_names[i]

                counterfactuals.append(feature_info)

        # Sort by absolute probability change and return top N
        counterfactuals.sort(key=lambda x: abs(x['probability_change']), reverse=True)
        return counterfactuals[:num_features]

    def get_layer_activations(self):
        """Get intermediate layer activations for interpretation"""
        if self.representation_activations is None:
            return None

        return self.representation_activations

# Part 5: Hybrid Few-Shot Model for Phishing Detection with ModernBERT and XAI
class HybridFewShotModernBERTPhishingDetector(nn.Module):
    """
    Hybrid model combining Prototypical Network and MAML approaches with
    ModernBERT for few-shot phishing detection.
    Includes comprehensive XAI support.
    """
    def __init__(self, model_id='answerdotai/ModernBERT-base', feature_dim=111):
        super(HybridFewShotModernBERTPhishingDetector, self).__init__()

        # Prototypical Network component
        self.proto_net = ModernBERTPrototypicalNetwork(
            model_id=model_id,
            feature_dim=feature_dim
        )

        # MAML component
        self.maml_net = ModernBERTMAMLPhishingDetector(
            model_id=model_id,
            feature_dim=feature_dim
        )

        # Combination layer (meta-learner)
        self.combination = nn.Sequential(
            nn.Linear(2, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.LayerNorm(32),
            nn.ReLU(),
            nn.Linear(32, 2)
        )

        # For XAI
        self.xai_mode = False
        self.model_contributions = None

        # For explainers (to be initialized later if needed)
        self.explainer = None

    def forward_proto(self, support_input_ids, support_attention_mask,
                      support_features, support_labels,
                      query_input_ids, query_attention_mask, query_features):
        """Forward pass through prototypical network"""
        return self.proto_net(
            support_input_ids, support_attention_mask, support_features,
            support_labels, query_input_ids, query_attention_mask, query_features
        )

    def forward_maml(self, support_input_ids, support_attention_mask,
                    support_features, support_labels,
                    query_input_ids, query_attention_mask, query_features,
                    num_adaptation_steps=3):
        """Forward pass through MAML network with adaptation - simplified to avoid gradient issues"""
        # In evaluation mode - use a simpler approach that avoids gradient tracking issues
        with torch.no_grad():  # Prevent gradient tracking for adaptation
            # Get initial prediction using base model
            logits = self.maml_net(query_input_ids, query_attention_mask, query_features)

        return logits

    def forward(self, support_input_ids, support_attention_mask, support_features,
                support_labels, query_input_ids, query_attention_mask, query_features):
        """
        Forward pass through hybrid model, combining prototypical networks and MAML
        """
        # Enable XAI mode if requested
        if self.xai_mode:
            self.proto_net.save_attention = True
            self.maml_net.save_attention = True

        # Get predictions from prototypical network
        proto_logits = self.forward_proto(
            support_input_ids, support_attention_mask, support_features,
            support_labels, query_input_ids, query_attention_mask, query_features
        )

        # Temporarily simplify training - use prototypical network output for MAML component
        # This avoids gradient flow issues during training
        if self.training:
            # During training, use proto_logits to avoid gradient issues
            maml_logits = proto_logits.clone()
        else:
            # During inference, use the full MAML approach
            maml_logits = self.forward_maml(
                support_input_ids, support_attention_mask, support_features,
                support_labels, query_input_ids, query_attention_mask, query_features
            )

        # Calculate model contributions for XAI
        if self.xai_mode:
            # Use softmax to get probabilities
            proto_probs = F.softmax(proto_logits, dim=1)
            maml_probs = F.softmax(maml_logits, dim=1)

            # Get differences in confidences
            proto_conf = proto_probs.max(dim=1)[0]
            maml_conf = maml_probs.max(dim=1)[0]

            # Store contributions
            self.model_contributions = {
                'proto': proto_conf.detach().cpu().numpy(),
                'maml': maml_conf.detach().cpu().numpy()
            }

            # Disable XAI mode after use
            self.proto_net.save_attention = False
            self.maml_net.save_attention = False
            self.xai_mode = False

        # Combine predictions (weighted average)
        combined_logits = (0.6 * proto_logits + 0.4 * maml_logits)

        return combined_logits, proto_logits, maml_logits

    def init_explainer(self, background_features, feature_names):
        """
        Initialize explainer for feature importance

        Args:
            background_features: Background dataset for explainer (tensor)
            feature_names: List of feature names
        """
        # Create a wrapper function for the model
        def model_predict(features):
            # Convert to tensor
            features_tensor = torch.tensor(features, dtype=torch.float32).to(device)
            batch_size = features.shape[0]

            # Use the MAML component for simplicity
            with torch.no_grad():
                # We need placeholder inputs for the text part
                dummy_input_ids = torch.zeros((batch_size, 128), dtype=torch.long).to(device)
                dummy_attention_mask = torch.ones((batch_size, 128), dtype=torch.long).to(device)

                # Get predictions
                logits = self.maml_net(dummy_input_ids, dummy_attention_mask, features_tensor)
                probs = F.softmax(logits, dim=1).cpu().numpy()

                return probs

        # Store the predict function and feature names
        self.feature_names = feature_names
        self.predict_fn = model_predict
        print("Explainer initialized")

    def generate_counterfactuals(self, tokenizer, meta_dataset, support_input_ids, support_attention_mask,
                                support_features, support_labels, query_input_ids, query_attention_mask,
                                query_features, query_idx=0):
        """
        Generate counterfactual examples by tweaking features

        Args:
            tokenizer: ModernBERT tokenizer
            meta_dataset: URLMetaDataset instance for feature descriptions
            support_*: Support set tensors
            query_*: Query set tensors
            query_idx: Index of query example to explain

        Returns:
            List of counterfactuals with their predicted class
        """
        feature_names = sorted(meta_dataset.get_feature_descriptions().keys())

        return self.maml_net.generate_counterfactuals(
            query_input_ids[query_idx:query_idx+1],
            query_attention_mask[query_idx:query_idx+1],
            query_features[query_idx:query_idx+1],
            tokenizer,
            feature_names=feature_names
        )

    def explain(self, tokenizer, meta_dataset, support_input_ids, support_attention_mask,
                support_features, support_labels, query_input_ids, query_attention_mask,
                query_features, query_urls, query_idx=0):
        """
        Generate comprehensive explanation for a specific query example

        Args:
            tokenizer: ModernBERT tokenizer
            meta_dataset: URLMetaDataset instance for feature descriptions
            support_*: Support set tensors
            query_*: Query set tensors
            query_urls: List of query URLs
            query_idx: Index of query example to explain

        Returns:
            Dictionary with comprehensive explanation
        """
        # Enable XAI mode
        self.xai_mode = True
        self.proto_net.save_attention = True
        self.maml_net.save_attention = True

        # Forward pass to collect data for explanation
        with torch.no_grad():  # Prevent gradient tracking during explanation
            combined_logits, proto_logits, maml_logits = self(
                support_input_ids, support_attention_mask, support_features, support_labels,
                query_input_ids[query_idx:query_idx+1],
                query_attention_mask[query_idx:query_idx+1],
                query_features[query_idx:query_idx+1]
            )

            # Get predicted class and confidence
            combined_probs = F.softmax(combined_logits, dim=1).detach()
            proto_probs = F.softmax(proto_logits, dim=1).detach()
            maml_probs = F.softmax(maml_logits, dim=1).detach()

            pred_class = torch.argmax(combined_probs, dim=1).item()
            pred_confidence = combined_probs[0, pred_class].item()

            # Get class names
            class_names = ["legitimate", "phishing"]

            # Initialize explanation
            explanation = {
                "prediction": {
                    "class": class_names[pred_class],
                    "confidence": pred_confidence,
                    "probabilities": {
                        "legitimate": combined_probs[0, 0].item(),
                        "phishing": combined_probs[0, 1].item()
                    }
                },
                "model_contributions": {
                    "prototypical": {
                        "class": class_names[torch.argmax(proto_probs, dim=1).item()],
                        "confidence": proto_probs.max().item(),
                        "weight": 0.6
                    },
                    "maml": {
                        "class": class_names[torch.argmax(maml_probs, dim=1).item()],
                        "confidence": maml_probs.max().item(),
                        "weight": 0.4
                    }
                },
                "url_analysis": {},
                "feature_importance": {},
                "counterfactuals": {},
                "similar_examples": {}
            }

            # 1. URL analysis
            url = query_urls[query_idx]
            components, suspiciousness = meta_dataset.analyze_url_components(url)

            explanation["url_analysis"] = {
                "components": components,
                "suspiciousness_scores": suspiciousness,
            }

            # Get features and their descriptions
            feature_descriptions = meta_dataset.get_feature_descriptions()
            feature_names = sorted(feature_descriptions.keys())

            # 2. Feature importance using perturbation analysis
            feature_importance = {}
            for i in range(min(query_features.size(1), len(feature_names))):
                # Create a modified feature tensor with this feature zeroed
                modified_features = query_features[query_idx:query_idx+1].clone()
                original_value = modified_features[0, i].item()
                modified_features[0, i] = 0

                # Get prediction with this feature zeroed
                mod_combined, _, _ = self(
                    support_input_ids, support_attention_mask, support_features, support_labels,
                    query_input_ids[query_idx:query_idx+1],
                    query_attention_mask[query_idx:query_idx+1],
                    modified_features
                )

                # Calculate probability change
                mod_probs = F.softmax(mod_combined, dim=1)
                orig_prob = combined_probs[0, pred_class].item()
                mod_prob = mod_probs[0, pred_class].item()

                # Importance is the change in probability
                importance = orig_prob - mod_prob

                # Get feature name
                feature_name = feature_names[i]

                feature_importance[feature_name] = {
                    "importance": importance,
                    "description": feature_descriptions.get(feature_name, "Feature " + str(i)),
                    "value": query_features[query_idx, i].item()
                }

            # Sort by absolute importance
            explanation["feature_importance"] = {
                k: v for k, v in sorted(
                    feature_importance.items(),
                    key=lambda item: abs(item[1]["importance"]),
                    reverse=True
                )
            }

            # 3. Generate counterfactuals
            counterfactuals = self.generate_counterfactuals(
                tokenizer,
                meta_dataset,
                support_input_ids,
                support_attention_mask,
                support_features,
                support_labels,
                query_input_ids,
                query_attention_mask,
                query_features,
                query_idx
            )

            # Add feature descriptions
            for cf in counterfactuals:
                if 'feature_name' in cf:
                    cf['feature_description'] = feature_descriptions.get(cf['feature_name'], "Unknown feature")
                else:
                    idx = cf['feature_idx']
                    if idx < len(feature_names):
                        cf['feature_name'] = feature_names[idx]
                        cf['feature_description'] = feature_descriptions.get(feature_names[idx], "Feature " + str(idx))
                    else:
                        cf['feature_name'] = f"Feature {idx}"
                        cf['feature_description'] = f"Unknown feature {idx}"

                cf['class_from'] = class_names[cf['original_class']]
                cf['class_to'] = class_names[cf['new_class']]

            explanation["counterfactuals"] = counterfactuals

            # 4. Generate feature importance visualization
            # Create feature importance plot
            plt.figure(figsize=(10, 6))

            # Get top features by importance
            top_features = sorted(
                feature_importance.items(),
                key=lambda x: abs(x[1]['importance']),
                reverse=True
            )[:15]  # Top 15 features

            feature_names = [f[0] for f in top_features]
            importance_values = [f[1]['importance'] for f in top_features]

            # Create horizontal bar chart
            bars = plt.barh(
                range(len(feature_names)),
                [abs(v) for v in importance_values],
                color=['red' if v < 0 else 'green' for v in importance_values]
            )

            # Add feature names as y labels
            plt.yticks(range(len(feature_names)), feature_names)
            plt.xlabel('Absolute Importance')
            plt.title('Feature Importance for Prediction')

            # Convert plot to base64 image
            buf = io.BytesIO()
            plt.savefig(buf, format='png', bbox_inches='tight')
            buf.seek(0)
            plt.close()

            importance_img_str = base64.b64encode(buf.read()).decode('utf-8')
            explanation["importance_plot"] = f'<img src="data:image/png;base64,{importance_img_str}" alt="Feature importance" />'

            # 5. Find similar examples from support set
            # Get embeddings
            query_embedding = self.proto_net.encode(
                query_input_ids[query_idx:query_idx+1],
                query_attention_mask[query_idx:query_idx+1],
                query_features[query_idx:query_idx+1]
            )

            support_embeddings = self.proto_net.encode(
                support_input_ids, support_attention_mask, support_features
            )

            # Calculate similarities (with detach to prevent gradient error)
            similarities = F.cosine_similarity(
                query_embedding, support_embeddings
            ).detach().cpu().numpy()

            # Get top 3 most similar examples
            top_indices = np.argsort(similarities)[-3:][::-1]

            similar_examples = []
            for idx in top_indices:
                # Get URL
                url = f"example-{idx}"

                # If items have original indices stored, try to get actual URL
                original_idx = getattr(support_input_ids, 'original_idx', -1)
                if original_idx != -1 and hasattr(meta_dataset, 'df') and 'url' in meta_dataset.df.columns:
                    url = meta_dataset.df.iloc[original_idx]['url']

                similar_examples.append({
                    "url": url,
                    "similarity": similarities[idx],
                    "class": class_names[support_labels[idx].item()]
                })

            explanation["similar_examples"] = similar_examples

            # 6. Token importance from attention
            token_importance = self.proto_net.get_token_importance(
                tokenizer,
                query_input_ids[query_idx:query_idx+1]
            )

            explanation["token_importance"] = token_importance

            # 7. Generate attention visualization
            attention_viz = self.proto_net.get_attention_visualization(
                tokenizer,
                query_input_ids[query_idx:query_idx+1]
            )

            explanation["attention_visualization"] = attention_viz

        return explanation

    def generate_explanation_html(self, explanation):
        """Generate a human-readable HTML explanation"""
        html = """
        <html>
        <head>
            <style>
                body { font-family: Arial, sans-serif; margin: 20px; }
                .container { max-width: 1000px; margin: 0 auto; }
                .section { margin-bottom: 30px; border: 1px solid #ddd; padding: 15px; border-radius: 5px; }
                .section-title { font-size: 18px; font-weight: bold; margin-bottom: 10px; }
                .prediction { font-size: 24px; font-weight: bold; margin-bottom: 20px; }
                .prediction.phishing { color: #d9534f; }
                .prediction.legitimate { color: #5cb85c; }
                .feature-bar { height: 15px; margin-bottom: 2px; border-radius: 3px; }
                .feature-row { margin-bottom: 10px; }
                .feature-name { font-weight: bold; }
                .feature-value { float: right; }
                .feature-description { color: #666; font-size: 12px; }
                .positive { background-color: #5cb85c; }
                .negative { background-color: #d9534f; }
                .url-component { margin-bottom: 10px; }
                .url-component-name { font-weight: bold; }
                .highlight-low { background-color: rgba(92, 184, 92, 0.3); }
                .highlight-med { background-color: rgba(240, 173, 78, 0.3); }
                .highlight-high { background-color: rgba(217, 83, 79, 0.3); }
                .model-contribution { margin-bottom: 10px; }
                .counterfactual { margin-bottom: 15px; padding: 10px; background-color: #f8f9fa; border-radius: 5px; }
                .similar-example { margin-bottom: 10px; }
                .token { display: inline-block; padding: 2px 4px; margin: 2px; border-radius: 3px; }
            </style>
        </head>
        <body>
            <div class="container">
        """

        # Prediction Section
        pred_class = explanation["prediction"]["class"]
        pred_confidence = explanation["prediction"]["confidence"]
        html += f"""
                <div class="section">
                    <div class="section-title">Prediction</div>
                    <div class="prediction {pred_class}">
                        {pred_class.upper()} ({pred_confidence:.2f})
                    </div>
                    <div>
                        <div>Legitimate: {explanation["prediction"]["probabilities"]["legitimate"]:.2f}</div>
                        <div>Phishing: {explanation["prediction"]["probabilities"]["phishing"]:.2f}</div>
                    </div>
                </div>
        """

        # URL Analysis Section
        html += """
                <div class="section">
                    <div class="section-title">URL Analysis</div>
        """

        components = explanation["url_analysis"]["components"]
        suspiciousness = explanation["url_analysis"]["suspiciousness_scores"]

        for component, value in components.items():
            if not value:  # Skip empty components
                continue

            score = suspiciousness.get(component, 0)
            highlight_class = "highlight-low"
            if score > 0.3:
                highlight_class = "highlight-med"
            if score > 0.7:
                highlight_class = "highlight-high"

            html += f"""
                    <div class="url-component">
                        <div class="url-component-name">{component}:</div>
                        <div class="{highlight_class}">{value}</div>
                    </div>
            """

        html += """
                </div>
        """

        # Feature Importance Section
        html += """
                <div class="section">
                    <div class="section-title">Feature Importance</div>
        """

        if "importance_plot" in explanation:
            html += explanation["importance_plot"]

        # Show top 10 most important features
        sorted_features = sorted(
            explanation["feature_importance"].items(),
            key=lambda item: abs(item[1]["importance"]),
            reverse=True
        )[:10]

        for feature, details in sorted_features:
            importance = details["importance"]
            description = details["description"]
            value = details["value"]

            # Normalize for bar width (0-100%)
            width = min(100, abs(importance * 200))  # Scale for visibility
            bar_class = "positive" if importance > 0 else "negative"

            html += f"""
                    <div class="feature-row">
                        <div class="feature-name">{feature} <span class="feature-value">{value:.2f}</span></div>
                        <div class="feature-bar {bar_class}" style="width: {width}%;"></div>
                        <div class="feature-description">{description}</div>
                    </div>
            """

        html += """
                </div>
        """

        # Model Contributions Section
        html += """
                <div class="section">
                    <div class="section-title">Model Contributions</div>
        """

        proto = explanation["model_contributions"]["prototypical"]
        maml = explanation["model_contributions"]["maml"]

        html += f"""
                    <div class="model-contribution">
                        <div>Prototypical Network (weight: {proto["weight"]}):</div>
                        <div>Prediction: {proto["class"]} (confidence: {proto["confidence"]:.2f})</div>
                    </div>
                    <div class="model-contribution">
                        <div>MAML Network (weight: {maml["weight"]}):</div>
                        <div>Prediction: {maml["class"]} (confidence: {maml["confidence"]:.2f})</div>
                    </div>
        """

        html += """
                </div>
        """

        # Counterfactuals Section
        html += """
                <div class="section">
                    <div class="section-title">What Would Change the Prediction?</div>
        """

        for cf in explanation["counterfactuals"]:
            html += f"""
                    <div class="counterfactual">
                        <div>If <strong>{cf.get('feature_name', f'Feature {cf["feature_idx"]}')}</strong> ({cf.get('feature_description', 'Unknown feature')}) was changed from {cf["original_value"]:.2f} to {cf["new_value"]:.2f}:</div>
                        <div>The prediction would change from <strong>{cf["class_from"]}</strong> to <strong>{cf["class_to"]}</strong></div>
                        <div>Confidence change: {cf["probability_change"]:.2f}</div>
                    </div>
            """

        html += """
                </div>
        """

        # Token Importance Section
        if "token_importance" in explanation and isinstance(explanation["token_importance"], dict):
            html += """
                    <div class="section">
                        <div class="section-title">Important URL Components</div>
                        <div>
            """

            for token, importance in explanation["token_importance"].items():
                # Skip special tokens and low importance tokens
                if token.startswith('[') or token.startswith('<') or importance < 0.1:
                    continue

                # Scale for color intensity (0.1-1.0)
                intensity = min(1.0, max(0.1, importance))
                red = int(255 * intensity)
                green = int(255 * (1 - intensity))

                html += f"""
                        <span class="token" style="background-color: rgba({red}, {green}, 0, 0.3);">{token}</span>
                """

            html += """
                        </div>
                    </div>
            """

        # Attention Visualization Section
        if "attention_visualization" in explanation and explanation["attention_visualization"].startswith('<img'):
            html += """
                    <div class="section">
                        <div class="section-title">Attention Visualization</div>
            """

            html += explanation["attention_visualization"]

            html += """
                    </div>
            """

        # Similar Examples Section
        if "similar_examples" in explanation and explanation["similar_examples"]:
            html += """
                    <div class="section">
                        <div class="section-title">Similar Examples</div>
            """

            for example in explanation["similar_examples"]:
                html += f"""
                        <div class="similar-example">
                            <div><strong>URL:</strong> {example["url"]}</div>
                            <div>Class: {example["class"]}</div>
                            <div>Similarity: {example["similarity"]:.4f}</div>
                        </div>
                """

            html += """
                    </div>
            """

        html += """
            </div>
        </body>
        </html>
        """

        return html

# Part 6: Training Functions
def train_epoch(model, meta_dataset, tokenizer, optimizer, device,
                num_episodes=100, inner_steps=3):
    """Train model for one epoch using episodic few-shot learning"""
    model.train()
    total_loss = 0
    total_acc = 0

    progress_bar = tqdm(range(num_episodes), desc="Training Episodes")

    for _ in progress_bar:
        # Sample a few-shot episode - try to use dataset first, fall back to URLs if needed
        try:
            support_set, query_set, task_info = meta_dataset.sample_episode(use_dataset=True)
        except Exception as e:
            print(f"Error sampling from dataset: {e}")
            support_set, query_set, task_info = meta_dataset.sample_episode(use_dataset=False)

        # Create datasets
        support_dataset = URLDataset(support_set, tokenizer)
        query_dataset = URLDataset(query_set, tokenizer)

        # Create dataloaders
        support_loader = DataLoader(support_dataset, batch_size=len(support_dataset), shuffle=True)
        query_loader = DataLoader(query_dataset, batch_size=len(query_dataset), shuffle=True)

        # Get all support data in one batch
        for support_batch in support_loader:
            support_input_ids = support_batch['input_ids'].to(device)
            support_attention_mask = support_batch['attention_mask'].to(device)
            support_features = support_batch['features'].to(device)
            support_labels = support_batch['label'].to(device)
            break

        # Get all query data in one batch
        for query_batch in query_loader:
            query_input_ids = query_batch['input_ids'].to(device)
            query_attention_mask = query_batch['attention_mask'].to(device)
            query_features = query_batch['features'].to(device)
            query_labels = query_batch['label'].to(device)
            break

        # Zero gradients
        optimizer.zero_grad()

        # Forward pass
        combined_logits, proto_logits, maml_logits = model(
            support_input_ids, support_attention_mask, support_features, support_labels,
            query_input_ids, query_attention_mask, query_features
        )

        # Calculate losses
        combined_loss = F.cross_entropy(combined_logits, query_labels)
        proto_loss = F.cross_entropy(proto_logits, query_labels)
        maml_loss = F.cross_entropy(maml_logits, query_labels)

        # Total loss (with emphasis on combined model)
        loss = 0.6 * combined_loss + 0.2 * proto_loss + 0.2 * maml_loss

        # Backward pass
        loss.backward()

        # Clip gradients
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        # Update weights
        optimizer.step()

        # Calculate accuracy
        _, preds = torch.max(combined_logits, dim=1)
        acc = (preds == query_labels).float().mean().item()

        # Update metrics
        total_loss += loss.item()
        total_acc += acc

        # Update progress bar
        progress_bar.set_postfix({
            'loss': f"{loss.item():.4f}",
            'acc': f"{acc:.4f}"
        })

    # Calculate epoch metrics
    avg_loss = total_loss / num_episodes
    avg_acc = total_acc / num_episodes

    return avg_loss, avg_acc

def evaluate(model, meta_dataset, tokenizer, device, num_episodes=30):
    """Evaluate model on few-shot episodes"""
    model.eval()
    total_loss = 0
    total_acc = 0
    all_preds = []
    all_labels = []

    with torch.no_grad():  # Ensure no gradients are computed during evaluation
        for _ in tqdm(range(num_episodes), desc="Evaluation Episodes"):
            # Sample a few-shot episode - try to use dataset first, fall back to URLs if needed
            try:
                support_set, query_set, _ = meta_dataset.sample_episode(use_dataset=True)
            except Exception as e:
                print(f"Error sampling from dataset: {e}")
                support_set, query_set, _ = meta_dataset.sample_episode(use_dataset=False)

            # Create datasets
            support_dataset = URLDataset(support_set, tokenizer)
            query_dataset = URLDataset(query_set, tokenizer)

            # Create dataloaders
            support_loader = DataLoader(support_dataset, batch_size=len(support_dataset))
            query_loader = DataLoader(query_dataset, batch_size=len(query_dataset))

            # Get all support data in one batch
            for support_batch in support_loader:
                support_input_ids = support_batch['input_ids'].to(device)
                support_attention_mask = support_batch['attention_mask'].to(device)
                support_features = support_batch['features'].to(device)
                support_labels = support_batch['label'].to(device)
                break

            # Get all query data in one batch
            for query_batch in query_loader:
                query_input_ids = query_batch['input_ids'].to(device)
                query_attention_mask = query_batch['attention_mask'].to(device)
                query_features = query_batch['features'].to(device)
                query_labels = query_batch['label'].to(device)
                break

            # Forward pass - use the combined logits directly to avoid MAML gradient issues
            # Use try-except to handle potential errors
            try:
                combined_logits, _, _ = model(
                    support_input_ids, support_attention_mask, support_features, support_labels,
                    query_input_ids, query_attention_mask, query_features
                )
            except Exception as e:
                print(f"Error during evaluation forward pass: {e}")
                # Fallback to just getting predictions from the prototypical network component
                combined_logits = model.forward_proto(
                    support_input_ids, support_attention_mask, support_features, support_labels,
                    query_input_ids, query_attention_mask, query_features
                )

            # Calculate loss
            loss = F.cross_entropy(combined_logits, query_labels)

            # Calculate predictions
            _, preds = torch.max(combined_logits, dim=1)

            # Update metrics
            total_loss += loss.item()
            acc = (preds == query_labels).float().mean().item()
            total_acc += acc

            # Store predictions and labels
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(query_labels.cpu().numpy())

    # Calculate metrics
    avg_loss = total_loss / num_episodes
    avg_acc = total_acc / num_episodes

    # Additional metrics
    precision = precision_score(all_labels, all_preds, zero_division=0)
    recall = recall_score(all_labels, all_preds, zero_division=0)
    f1 = f1_score(all_labels, all_preds, zero_division=0)

    return avg_loss, avg_acc, precision, recall, f1, all_preds, all_labels

# Part 7: Main Training Loop and Application
def main():
    print("Initializing Few-Shot Phishing Detection with ModernBERT and XAI...")

    # Parameters
    n_way = 2  # Binary classification: phishing vs legitimate
    k_shot = 5  # 5 examples per class in support set
    query_size = 10  # 10 examples per class in query set
    num_episodes = 50  # Number of episodes per epoch
    num_epochs = 3
    dataset_path = '/content/drive/MyDrive/phishing/dataset_full.csv'  # Path to your dataset
    url_source = "https://raw.githubusercontent.com/Phishing-Database/Phishing.Database/master/phishing-links-ACTIVE.txt"

    # Initialize meta-dataset with both real data and phishing URLs
    meta_dataset = URLMetaDataset(
        data_path=dataset_path,
        url_source=url_source,
        n_way=n_way,
        k_shot=k_shot,
        query_size=query_size
    )

    # Get feature dimension from dataset
    sample_support, sample_query, _ = meta_dataset.sample_episode()
    feature_dim = len(sample_support[0]['features'])
    print(f"Feature dimension from dataset: {feature_dim}")

    # Initialize tokenizer
    print("Loading ModernBERT tokenizer...")
    model_id = "answerdotai/ModernBERT-base"
    tokenizer = AutoTokenizer.from_pretrained(model_id)

    # Initialize model
    print("Initializing hybrid model with ModernBERT...")
    model = HybridFewShotModernBERTPhishingDetector(
        model_id=model_id,
        feature_dim=feature_dim
    )
    model = model.to(device)

    # Setup optimizer
    print("Setting up optimizer...")
    # Create separate parameter groups for different learning rates
    encoder_params = set()
    # First collect all encoder parameters
    for name, param in model.proto_net.encoder.named_parameters():
        encoder_params.add(param)
    for name, param in model.maml_net.encoder.named_parameters():
        encoder_params.add(param)

    # Create parameter groups
    other_params = [p for p in model.parameters() if p not in encoder_params]

    # Create optimizer with parameter groups
    optimizer = torch.optim.AdamW([
        {"params": list(encoder_params), "lr": 1e-5},  # Lower learning rate for ModernBERT parameters
        {"params": other_params, "lr": 3e-5}  # Higher learning rate for other parameters
    ], weight_decay=0.01)

    # Learning rate scheduler
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=num_epochs, eta_min=1e-6
    )

    # Training loop
    best_val_acc = 0
    train_losses = []
    train_accs = []
    val_losses = []
    val_accs = []

    print("Starting training...")
    for epoch in range(num_epochs):
        print(f"\nEpoch {epoch+1}/{num_epochs}")

        # Train
        train_loss, train_acc = train_epoch(
            model, meta_dataset, tokenizer, optimizer, device,
            num_episodes=num_episodes
        )
        train_losses.append(train_loss)
        train_accs.append(train_acc)

        # Evaluate
        val_loss, val_acc, precision, recall, f1, _, _ = evaluate(
            model, meta_dataset, tokenizer, device
        )
        val_losses.append(val_loss)
        val_accs.append(val_acc)

        # Update learning rate
        scheduler.step()

        print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")
        print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")
        print(f"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}")

        # Save best model
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), 'best_fewshot_modernbert_phishing_model.pt')
            print(f"New best model saved with validation accuracy: {val_acc:.4f}")

    print("Training complete!")

    # Final evaluation
    print("\nPerforming final evaluation...")

    final_loss, final_acc, final_precision, final_recall, final_f1, y_pred, y_true = evaluate(
        model, meta_dataset, tokenizer, device, num_episodes=20
    )

    print("\nFinal Test Metrics:")
    print(f"Accuracy: {final_acc:.4f}")
    print(f"Precision: {final_precision:.4f}")
    print(f"Recall: {final_recall:.4f}")
    print(f"F1 Score: {final_f1:.4f}")

    # Plot confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['Legitimate', 'Phishing'],
                yticklabels=['Legitimate', 'Phishing'])
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix for Few-Shot Phishing Detection with ModernBERT')
    plt.savefig('fewshot_modernbert_confusion_matrix.png')
    plt.close()

    # Plot training curves
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label='Train Loss')
    plt.plot(val_losses, label='Val Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(train_accs, label='Train Acc')
    plt.plot(val_accs, label='Val Acc')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.legend()

    plt.tight_layout()
    plt.savefig('fewshot_modernbert_training_curves.png')
    plt.close()

    print("\nEvaluation complete!")

    return model, tokenizer, meta_dataset

# Part 8: XAI Testing and Demo
def test_with_xai(model, tokenizer, meta_dataset, device, test_indices=None, output_dir="xai_reports"):
    """
    Test model on a set of URLs with detailed XAI explanations

    Args:
        model: The trained model
        tokenizer: ModernBERT tokenizer
        meta_dataset: URLMetaDataset instance
        device: Device to run on
        test_indices: List of indices to test (if None, samples randomly)
        output_dir: Directory to save XAI reports

    Returns:
        Dict with results and explanations
    """
    print(f"Testing model with XAI explanations...")

    # If no test indices provided, sample some
    if test_indices is None:
        # Get a mix of phishing and legitimate URLs
        num_test = 10
        phishing_indices = random.sample(meta_dataset.phishing_indices, min(num_test // 2, len(meta_dataset.phishing_indices)))
        legitimate_indices = random.sample(meta_dataset.legitimate_indices, min(num_test // 2, len(meta_dataset.legitimate_indices)))
        test_indices = phishing_indices + legitimate_indices

    # Prepare the few-shot examples
    support_set = []
    # Get support examples for phishing
    for idx in random.sample(meta_dataset.phishing_indices, min(5, len(meta_dataset.phishing_indices))):
        if idx not in test_indices:  # Avoid overlap
            features = meta_dataset.extract_features(idx)
            # Get URL (either from column or index-based synthetic URL)
            if hasattr(meta_dataset, 'df') and meta_dataset.df is not None and 'url' in meta_dataset.df.columns:
                url = meta_dataset.df.iloc[idx]['url']
            else:
                # Create a synthetic URL if not available
                url = f"https://example{idx}.com/page{idx}"

            support_set.append({
                'url': url,
                'label': 1,  # Phishing
                'features': features,
                'idx': idx
            })

    # Get support examples for legitimate
    for idx in random.sample(meta_dataset.legitimate_indices, min(5, len(meta_dataset.legitimate_indices))):
        if idx not in test_indices:  # Avoid overlap
            features = meta_dataset.extract_features(idx)
            # Get URL (either from column or index-based synthetic URL)
            if hasattr(meta_dataset, 'df') and meta_dataset.df is not None and 'url' in meta_dataset.df.columns:
                url = meta_dataset.df.iloc[idx]['url']
            else:
                # Create a synthetic URL if not available
                url = f"https://example{idx}.com/page{idx}"

            support_set.append({
                'url': url,
                'label': 0,  # Legitimate
                'features': features,
                'idx': idx
            })

    # Prepare query set (test indices)
    query_set = []
    for idx in test_indices[:5]:  # Limit to first 5 for demonstration
        features = meta_dataset.extract_features(idx)
        # Get URL and label
        if hasattr(meta_dataset, 'df') and meta_dataset.df is not None:
            if 'url' in meta_dataset.df.columns:
                url = meta_dataset.df.iloc[idx]['url']
            else:
                url = f"https://example{idx}.com/page{idx}"

            # Get label from dataset
            phishing_col = None
            for col in ['phishing', 'is_phishing', 'label', 'class']:
                if col in meta_dataset.df.columns:
                    phishing_col = col
                    break
            if not phishing_col:
                phishing_col = meta_dataset.df.columns[-1]

            label = meta_dataset.df.iloc[idx][phishing_col]
        else:
            # If no dataset, determine from indices lists
            url = f"https://example{idx}.com/page{idx}"
            label = 1 if idx in meta_dataset.phishing_indices else 0

        query_set.append({
            'url': url,
            'label': label,
            'features': features,
            'idx': idx
        })

    # Create datasets
    support_dataset = URLDataset(support_set, tokenizer)
    query_dataset = URLDataset(query_set, tokenizer)

    # Create dataloaders
    support_loader = DataLoader(support_dataset, batch_size=len(support_dataset))
    query_loader = DataLoader(query_dataset, batch_size=len(query_dataset))

    # Get all support data
    for support_batch in support_loader:
        support_input_ids = support_batch['input_ids'].to(device)
        support_attention_mask = support_batch['attention_mask'].to(device)
        support_features = support_batch['features'].to(device)
        support_labels = support_batch['label'].to(device)
        break

    # Get all query data
    for query_batch in query_loader:
        query_input_ids = query_batch['input_ids'].to(device)
        query_attention_mask = query_batch['attention_mask'].to(device)
        query_features = query_batch['features'].to(device)
        query_labels = query_batch['label'].to(device)
        # Store the actual URL strings separately
        query_urls = [url for url in query_batch['url']]
        break

    # Forward pass
    with torch.no_grad():
        combined_logits, proto_logits, maml_logits = model(
            support_input_ids, support_attention_mask, support_features, support_labels,
            query_input_ids, query_attention_mask, query_features
        )

    # Get predictions
    _, preds = torch.max(combined_logits, dim=1)

    # Calculate accuracy
    acc = (preds == query_labels).float().mean().item()

    # Detailed XAI analysis
    results = {
        "accuracy": acc,
        "predictions": [],
        "explanations": []
    }

    # Generate explanations for each query URL
    print("Generating XAI explanations for each URL...")
    for i in range(len(query_urls)):
        print(f"Processing URL {i+1}/{len(query_urls)}: {query_urls[i]}")

        # Generate explanation
        explanation = model.explain(
            tokenizer,
            meta_dataset,
            support_input_ids,
            support_attention_mask,
            support_features,
            support_labels,
            query_input_ids,
            query_attention_mask,
            query_features,
            query_urls,
            i
        )

        # Generate HTML explanation
        html_explanation = model.generate_explanation_html(explanation)

        # Add to results
        results["explanations"].append({
            "raw": explanation,
            "html": html_explanation
        })

        # Add prediction info
        pred_label = "phishing" if preds[i].item() == 1 else "legitimate"
        true_label = "phishing" if query_labels[i].item() == 1 else "legitimate"

        results["predictions"].append({
            "url": query_urls[i],
            "predicted": pred_label,
            "true": true_label,
            "correct": pred_label == true_label
        })

    # Create output directory if it doesn't exist
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)

        # Create index.html
        index_html = """
<!DOCTYPE html>
<html>
<head>
    <title>Few-Shot Phishing Detection XAI Reports</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .container {{ max-width: 800px; margin: 0 auto; }}
        h1 {{ color: #333; }}
        .url-list {{ margin-top: 20px; }}
        .url-item {{ margin-bottom: 10px; padding: 10px; border: 1px solid #ddd; border-radius: 5px; }}
        .url-item a {{ text-decoration: none; color: #0066cc; }}
        .correct {{ background-color: rgba(92, 184, 92, 0.2); }}
        .incorrect {{ background-color: rgba(217, 83, 79, 0.2); }}
        .summary {{ margin-bottom: 30px; padding: 15px; background-color: #f8f9fa; border-radius: 5px; }}
    </style>
</head>
<body>
    <div class="container">
        <h1>Few-Shot Phishing Detection XAI Reports</h1>

        <div class="summary">
            <h2>Summary</h2>
            <p>Accuracy: {accuracy:.2f}</p>
            <p>Total URLs analyzed: {total}</p>
        </div>

        <div class="url-list">
            <h2>URL Reports</h2>
""".format(
    accuracy=results["accuracy"],
    total=len(results["predictions"])
)

        # Process each URL
        for i, pred in enumerate(results["predictions"]):
            url = pred["url"]
            predicted = pred["predicted"]
            true = pred["true"]
            correct = pred["correct"]

            filename = f"report_{i}.html"

            # Write individual report
            with open(os.path.join(output_dir, filename), 'w') as f:
                f.write(results["explanations"][i]["html"])

            # Add to index
            status_class = "correct" if correct else "incorrect"
            index_html += f"""
                    <div class="url-item {status_class}">
                        <div><strong>URL:</strong> {url}</div>
                        <div><strong>Predicted:</strong> {predicted.upper()}</div>
                        <div><strong>True:</strong> {true.upper()}</div>
                        <div><a href="{filename}" target="_blank">View XAI Report</a></div>
                    </div>
            """

        index_html += """
                </div>
            </div>
        </body>
        </html>
        """

        # Write index.html
        index_path = os.path.join(output_dir, "index.html")
        with open(index_path, 'w') as f:
            f.write(index_html)

        print(f"XAI reports generated in {output_dir}/")
        print(f"View the main report at {index_path}")

    return results

def run_demo():
    """Run a demonstration of the phishing detection system"""
    print("Running ModernBERT Few-Shot Phishing Detection Demo with XAI")

    # Try to load a pre-trained model or train a new one
    model_path = 'best_fewshot_modernbert_phishing_model.pt'
    if os.path.exists(model_path):
        print("Loading pre-trained model...")

        # Initialize meta-dataset
        dataset_path = 'phishing_dataset.csv'
        url_source = "https://raw.githubusercontent.com/Phishing-Database/Phishing.Database/master/phishing-links-ACTIVE.txt"
        meta_dataset = URLMetaDataset(
            data_path=dataset_path,
            url_source=url_source,
            n_way=2,
            k_shot=5,
            query_size=10
        )

        # Get feature dimension
        sample_support, sample_query, _ = meta_dataset.sample_episode()
        feature_dim = len(sample_support[0]['features'])

        # Initialize tokenizer
        model_id = "answerdotai/ModernBERT-base"
        tokenizer = AutoTokenizer.from_pretrained(model_id)

        # Initialize model
        model = HybridFewShotModernBERTPhishingDetector(
            model_id=model_id,
            feature_dim=feature_dim
        )
        model.load_state_dict(torch.load(model_path, map_location=device))
        model = model.to(device)

    else:
        print("No pre-trained model found. Training a new model...")
        model, tokenizer, meta_dataset = main()

    # Set model to evaluation mode
    model.eval()

    # Run XAI testing
    print("\nTesting with XAI on sample URLs...")
    results = test_with_xai(model, tokenizer, meta_dataset, device)

    # Demo with custom URLs
    print("\nDemo with custom URLs...")
    custom_urls = [
        "https://secure-banking-login.example.com/verify",
        "https://paypal-account-secure.example.net/login",
        "https://google.com",
        "https://amazon.com",
        "https://192.168.1.1/admin/login.php"
    ]

    # Create query set with custom URLs
    query_set = []
    for i, url in enumerate(custom_urls):
        # Extract features
        features = meta_dataset.extract_features(url)

        # Assign synthetic labels (for demonstration only)
        # In a real application, these would be unknown
        is_phishing = 1 if "secure" in url.lower() or url.count(".") > 2 or "192.168" in url else 0

        query_set.append({
            'url': url,
            'label': is_phishing,  # Synthetic label
            'features': features
        })

    # Also create support set from meta-dataset
    support_set, _, _ = meta_dataset.sample_episode()

    # Create datasets
    support_dataset = URLDataset(support_set, tokenizer)
    query_dataset = URLDataset(query_set, tokenizer)

    # Create dataloaders
    support_loader = DataLoader(support_dataset, batch_size=len(support_dataset))
    query_loader = DataLoader(query_dataset, batch_size=len(query_dataset))

    # Get all support data
    for support_batch in support_loader:
        support_input_ids = support_batch['input_ids'].to(device)
        support_attention_mask = support_batch['attention_mask'].to(device)
        support_features = support_batch['features'].to(device)
        support_labels = support_batch['label'].to(device)
        break

    # Get all query data
    for query_batch in query_loader:
        query_input_ids = query_batch['input_ids'].to(device)
        query_attention_mask = query_batch['attention_mask'].to(device)
        query_features = query_batch['features'].to(device)
        query_labels = query_batch['label'].to(device)
        break

    # Make predictions
    with torch.no_grad():
        combined_logits, _, _ = model(
            support_input_ids, support_attention_mask, support_features, support_labels,
            query_input_ids, query_attention_mask, query_features
        )

    # Get predictions
    probs = F.softmax(combined_logits, dim=1)
    _, preds = torch.max(probs, dim=1)

    # Show results
    print("\nCustom URL Detection Results:")
    for i, url in enumerate(custom_urls):
        pred_class = "PHISHING" if preds[i].item() == 1 else "LEGITIMATE"
        confidence = probs[i, preds[i].item()].item()

        # Generate a simple explanation
        features = meta_dataset.extract_features(url)
        components, suspiciousness = meta_dataset.analyze_url_components(url)

        print(f"\nURL: {url}")
        print(f"Prediction: {pred_class} (confidence: {confidence:.4f})")

        # Show most suspicious components
        suspicious_components = {k: v for k, v in suspiciousness.items() if v > 0.3}
        if suspicious_components:
            print("Suspicious components:")
            for component, score in suspicious_components.items():
                print(f"- {component}: {score:.2f}")

        # Generate explanation if requested
        print("Generate full XAI explanation? (y/n)")
        choice = input()
        if choice.lower() == 'y':
            explanation = model.explain(
                tokenizer,
                meta_dataset,
                support_input_ids,
                support_attention_mask,
                support_features,
                support_labels,
                query_input_ids,
                query_attention_mask,
                query_features,
                custom_urls,
                i
            )

            # Generate HTML explanation
            html_explanation = model.generate_explanation_html(explanation)

            # Save HTML explanation
            os.makedirs("custom_url_reports", exist_ok=True)
            with open(f"custom_url_reports/explanation_{i}.html", "w") as f:
                f.write(html_explanation)

            print(f"Full explanation saved to custom_url_reports/explanation_{i}.html")

# Example usage
if __name__ == "__main__":
    # Run the demo
    run_demo()

from google.colab import drive
drive.mount('/content/drive')